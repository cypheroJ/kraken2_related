{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fbafa39-afa0-4ca2-95e0-ad3429e5aaa7",
   "metadata": {},
   "source": [
    "# Nanopore Microbiome Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a557013-9cea-4ad2-9d12-e5cdda490840",
   "metadata": {},
   "source": [
    "# 1. dorado Basecalling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59a205-de4b-4192-a7f6-0418191917c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Download Dorado basecaller\n",
    "- if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5ed0e-e88c-4b12-8d5b-bda1122a0952",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71673,
     "status": "ok",
     "timestamp": 1690673768489,
     "user": {
      "displayName": "Yin-Tse Huang",
      "userId": "00471228198785597773"
     },
     "user_tz": -480
    },
    "id": "tgWBvko6uEtD",
    "outputId": "19c89325-da8a-44a5-b60a-4c42411879ce"
   },
   "outputs": [],
   "source": [
    "!wget https://cdn.oxfordnanoportal.com/software/analysis/dorado-0.9.0-linux-x64.tar.gz -O /mnt/NanoporeRawData/dorado.tar.gz\n",
    "!tar -xvf /mnt/NanoporeRawData/dorado.tar.gz -C /mnt/NanoporeRawData/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa821116-0206-4cfb-b2e6-b84af2147012",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Fast5 Files Conversion\n",
    "- if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ed9b387-e401-43e1-8ee3-4002d304c240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAST5 directory detected, converting to POD5...\n",
      "Requirement already satisfied: pod5 in /opt/miniconda3/lib/python3.12/site-packages (0.3.15)\n",
      "Requirement already satisfied: lib-pod5==0.3.15 in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (0.3.15)\n",
      "Requirement already satisfied: iso8601 in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (2.1.0)\n",
      "Requirement already satisfied: more-itertools in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (10.5.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (1.26.4)\n",
      "Requirement already satisfied: pytz in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (2024.1)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (23.2)\n",
      "Requirement already satisfied: polars~=0.19 in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (0.20.31)\n",
      "Requirement already satisfied: vbz-h5py-plugin in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (4.66.2)\n",
      "Requirement already satisfied: pyarrow~=16.1.0 in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (16.1.0)\n",
      "Requirement already satisfied: h5py~=3.11.0 in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (3.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting 536 Fast5s: 100%|##########| 2142460/2142460 [4:05:15<00:00, 145.59Reads/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POD5 conversion completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Define paths\n",
    "raw_data_path = \"/mnt/NanoporeRawData/2023-000029/\"\n",
    "fast5_path = os.path.join(raw_data_path, \"fast5\")\n",
    "pod5_path = os.path.join(raw_data_path, \"pod5\")\n",
    "\n",
    "# Function to check if the number of files match and none are empty\n",
    "def check_pod5_files(fast5_path, pod5_path):\n",
    "    fast5_files = [f for f in os.listdir(fast5_path) if f.endswith('.fast5')]\n",
    "    pod5_files = [f for f in os.listdir(pod5_path) if f.endswith('.pod5')]\n",
    "\n",
    "    # Check if counts match and ensure no empty files\n",
    "    if len(fast5_files) != len(pod5_files):\n",
    "        return False\n",
    "\n",
    "    for pod5_file in pod5_files:\n",
    "        if os.path.getsize(os.path.join(pod5_path, pod5_file)) == 0:  # Check if the file is empty\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Check if only the fast5 directory exists, and convert FAST5 to POD5\n",
    "if os.path.exists(fast5_path) and not os.path.exists(pod5_path):\n",
    "    print(\"FAST5 directory detected, converting to POD5...\")\n",
    "\n",
    "    # Install pod5 package if not already installed\n",
    "    subprocess.run([\"pip\", \"install\", \"pod5\"], check=True)\n",
    "\n",
    "    # Ensure the pod5 directory exists\n",
    "    os.makedirs(pod5_path, exist_ok=True)\n",
    "\n",
    "    # Convert FAST5 to POD5, using -o for output and -O for one-to-one mapping\n",
    "    subprocess.run([\n",
    "        \"pod5\", \"convert\", \"fast5\", \n",
    "        \"-o\", pod5_path,  # Output directory for POD5 files\n",
    "        \"-O\", fast5_path,  # Parent directory for input files\n",
    "        fast5_path  # Input path\n",
    "    ], check=True)\n",
    "\n",
    "# Rerun conversion if checks fail\n",
    "if not check_pod5_files(fast5_path, pod5_path):\n",
    "    print(\"File count mismatch or empty POD5 files detected. Re-running conversion...\")\n",
    "    subprocess.run([\n",
    "        \"pod5\", \"convert\", \"fast5\", \n",
    "        \"-o\", pod5_path, \n",
    "        \"-O\", fast5_path,  \n",
    "        fast5_path  \n",
    "    ], check=True)\n",
    "\n",
    "print(\"POD5 conversion completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fa569d-bf90-4914-a0b7-4f3379e41f76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1-1. Pod5 Files Basecalling\n",
    "- basecall\n",
    "- convert BAM to fastq\n",
    "- generate NanoPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98118eeb-2260-476d-a727-c8d32973342e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dorado basecaller...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-23 18:05:53.122] [info] Running: \"basecaller\" \"sup\" \"/mnt/NanoporeRawData/2024-000025/pod5\"\n",
      "[2024-12-23 18:05:55.069] [info]  - downloading dna_r10.4.1_e8.2_400bps_sup@v5.0.0 with httplib\n",
      "[2024-12-23 18:06:57.923] [info] > Creating basecall pipeline\n",
      "[2024-12-23 18:07:12.602] [info] Calculating optimized batch size for GPU \"NVIDIA GeForce RTX 4090\" and model /mnt/Data/2024-000025/.temp_dorado_model-9c7a34f08328b121/dna_r10.4.1_e8.2_400bps_sup@v5.0.0. Full benchmarking will run for this device, which may take some time.\n",
      "[2024-12-23 18:07:42.852] [info] cuda:0 using chunk size 11520, batch size 128\n",
      "[2024-12-23 18:07:43.498] [info] cuda:0 using chunk size 5760, batch size 128\n",
      "[2024-12-23 20:13:45.007] [info] > Finished in (ms): 7560949\n",
      "[2024-12-23 20:13:45.007] [info] > Simplex reads basecalled: 467683\n",
      "[2024-12-23 20:13:45.007] [info] > Simplex reads filtered: 5\n",
      "[2024-12-23 20:13:45.007] [info] > Basecalled @ Samples/s: 3.403250e+06\n",
      "[2024-12-23 20:13:45.423] [info] > Finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting BAM to FASTQ...\n",
      "Generating NanoPlot...\n",
      "Dorado basecalling and analysis completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from packaging import version\n",
    "\n",
    "\n",
    "''' ===== Configuration ===== '''\n",
    "dorado_base_path = \"/mnt/NanoporeRawData\"\n",
    "raw_data_path = \"/mnt/NanoporeRawData/2024-000025/pod5\"\n",
    "basecall_data_path = \"/mnt/Data/2024-000025/1_dorado\"\n",
    "\n",
    "\n",
    "''' ===== Workflow ===== '''\n",
    "def get_latest_dorado_path(base_path):\n",
    "    dorado_versions = [\n",
    "        os.path.join(base_path, d) for d in os.listdir(base_path)\n",
    "        if d.startswith(\"dorado-\") and os.path.isdir(os.path.join(base_path, d))\n",
    "    ]\n",
    "    dorado_versions = sorted(\n",
    "        dorado_versions, \n",
    "        key=lambda d: version.parse(d.split('-')[1].split('-')[0]),\n",
    "        reverse=True\n",
    "    )\n",
    "    if dorado_versions:\n",
    "        return os.path.join(dorado_versions[0], \"bin\", \"dorado\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No Dorado versions found in the specified base path.\")\n",
    "\n",
    "try:\n",
    "    dorado_path = get_latest_dorado_path(dorado_base_path)\n",
    "    print(f\"Using Dorado binary: {dorado_path}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    exit(1)\n",
    "\n",
    "os.makedirs(basecall_data_path, exist_ok=True)\n",
    "\n",
    "print(\"Running dorado basecaller...\")\n",
    "bam_file_path = os.path.join(basecall_data_path, \"all.bam\")\n",
    "with open(bam_file_path, \"w\") as bam_file:\n",
    "    subprocess.run([\n",
    "        dorado_path, \"basecaller\", \"sup\", raw_data_path\n",
    "    ], stdout=bam_file, check=True)\n",
    "\n",
    "print(\"Converting BAM to FASTQ...\")\n",
    "subprocess.run([\n",
    "    \"bedtools\", \"bamtofastq\", \n",
    "    \"-i\", bam_file_path,\n",
    "    \"-fq\", os.path.join(basecall_data_path, \"all.fastq\")\n",
    "], check=True)\n",
    "\n",
    "print(\"Generating NanoPlot...\")\n",
    "subprocess.run([\n",
    "    \"NanoPlot\", \n",
    "    \"--fastq\", os.path.join(basecall_data_path, \"all.fastq\"),\n",
    "    \"-o\", basecall_data_path\n",
    "], check=True)\n",
    "\n",
    "print(\"Dorado basecalling and analysis completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1905c3-393d-4f58-b0d1-bd045c062de0",
   "metadata": {},
   "source": [
    "# 2. NanoACT Demultiplexing & Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0170c06-e45e-421d-ab67-89019b0b11ab",
   "metadata": {},
   "source": [
    "##### Rename SampleID\n",
    "- if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aff7411-98ad-4ad2-a6a1-7196d51fbc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded reference data for sample type: DNA samples.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(output_csv, fieldnames\u001b[38;5;241m=\u001b[39mbarcode_header)\n\u001b[1;32m     53\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriteheader()\n\u001b[0;32m---> 54\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterows\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile written to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/csv.py:157\u001b[0m, in \u001b[0;36mDictWriter.writerows\u001b[0;34m(self, rowdicts)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriterows\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdicts):\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterows\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrowdicts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/csv.py:147\u001b[0m, in \u001b[0;36mDictWriter._dict_to_list\u001b[0;34m(self, rowdict)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dict_to_list\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdict):\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextrasaction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m         wrong_fields \u001b[38;5;241m=\u001b[39m \u001b[43mrowdict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m wrong_fields:\n\u001b[1;32m    149\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict contains fields not in fieldnames: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m                              \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m wrong_fields]))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "''' ===== Configuration ===== '''\n",
    "project_id = \"2024-000012\"  # Set this to your project ID\n",
    "sample_type = \"DNA samples\"  # Specify the sample type for filtering\n",
    "replace_name = True  # Set True to replace SampleID with Reference ID from reference.csv\n",
    "\n",
    "''' File path settings '''\n",
    "base_dir = f\"/mnt/Data/{project_id}\"  # Base directory containing the csv files\n",
    "barcode_ID_file = os.path.join(base_dir, f\"{project_id}.csv\")\n",
    "reference_file = os.path.join(base_dir, \"reference.csv\")\n",
    "output_file = os.path.join(base_dir, f\"{project_id}_modf.csv\")\n",
    "\n",
    "\n",
    "''' ===== Workflow ===== '''\n",
    "# Initialize containers for data\n",
    "barcode_IDs = []\n",
    "reference_data = {}\n",
    "\n",
    "# Load reference data if the file exists\n",
    "if replace_name and os.path.exists(reference_file):\n",
    "    # Read the reference file and extract relevant columns\n",
    "    with open(reference_file, 'r') as ref_file:\n",
    "        reader = csv.DictReader(ref_file)  # Assuming CSV file\n",
    "        for row in reader:\n",
    "            # Filter by the specified sample type\n",
    "            if row['Sample type'] == sample_type:\n",
    "                reference_data[row['PCR ID']] = row['Name or Morph description']\n",
    "    print(f\"Loaded reference data for sample type: {sample_type}.\")\n",
    "else:\n",
    "    print(\"reference.csv not found. Skipping sample replacement step.\")\n",
    "\n",
    "# Read the barcode_ID file\n",
    "with open(barcode_ID_file, 'r') as barcode_file:\n",
    "    reader = csv.DictReader(barcode_file)\n",
    "    barcode_header = reader.fieldnames  # Save the header\n",
    "    for row in reader:\n",
    "        sample_id = row['SampleID']  # Original SampleID from the barcode file\n",
    "        # Replace SampleID if reference data is available and matches\n",
    "        if reference_data and sample_id in reference_data:\n",
    "            row['SampleID'] = reference_data[sample_id]  # Replace with Reference ID\n",
    "        barcode_IDs.append(row)  # Add modified row to the list\n",
    "\n",
    "# Prepare output data\n",
    "output_data = barcode_IDs  # Append modified barcode data\n",
    "\n",
    "# Write to the new CSV file\n",
    "with open(output_file, 'w', newline='') as output_csv:\n",
    "    writer = csv.DictWriter(output_csv, fieldnames=barcode_header)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_data)\n",
    "\n",
    "print(f\"File written to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75e38dd-786e-43c8-af93-bc783af8ffb5",
   "metadata": {},
   "source": [
    "## 2-1. Load NanoAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fecf34f-a969-4c30-8841-c347d6c205df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/Data/2024-000012\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "working_directory = os.getcwd()\n",
    "\n",
    "# Change to home directory\n",
    "os.chdir(os.path.expanduser(\"~\"))\n",
    "\n",
    "# Check if 'nanoACT' directory exists\n",
    "if not os.path.exists(\"nanoACT\"):\n",
    "    # If not, clone the repository\n",
    "    !git clone https://github.com/Raingel/nanoACT.git\n",
    "    os.chdir(os.path.expanduser(\"~/nanoACT/\"))\n",
    "else:\n",
    "    # If the directory exists, reset local changes and pull the latest updates\n",
    "    os.chdir(os.path.expanduser(\"~/nanoACT/\"))\n",
    "    !git fetch --all > /dev/null 2>&1\n",
    "    !git reset --hard origin/main > /dev/null 2>&1 # Force reset to the latest commit\n",
    "    !git pull > /dev/null 2>&1\n",
    "\n",
    "# Install requirements if necessary\n",
    "\"\"\"\n",
    "!pip install --upgrade pip\n",
    "!pip install -r requirements.txt\n",
    "\"\"\"\n",
    "\n",
    "# Import nanoAct and initialize\n",
    "from nanoact import nanoact\n",
    "dumb = nanoact.NanoAct(TEMP = \"/home/huanglabserver/nanoACT/temp/\")\n",
    "\n",
    "# Change back to the original working directory\n",
    "os.chdir(working_directory)\n",
    "\n",
    "# Verify the current working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64033c6-d12f-49c0-9286-be1fd6b8a068",
   "metadata": {},
   "source": [
    "## 2-2. Processing sequences\n",
    "- Quality filtering\n",
    "- Demultiplexing\n",
    "- Orientation \n",
    "- Trimming artificial reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2469e88-064d-42a9-aa1e-a969b05031bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:12] Start Qualityfilt...\n",
      "[13:45:12] QSCORE: 9, MIN_LEN: 700, MAX_LEN: 2000\n",
      "[13:46:16] 248060/570266 (43.50%) reads were passed quality filter\n",
      "[13:46:16] BARCODE_INDEX_FILE loaded\n",
      "[13:46:16] Parsed 10000\n",
      "[13:46:17] Parsed 20000\n",
      "[13:46:18] Parsed 30000\n",
      "[13:46:18] Parsed 40000\n",
      "[13:46:19] Parsed 50000\n",
      "[13:46:20] Parsed 60000\n",
      "[13:46:21] Parsed 70000\n",
      "[13:46:21] Parsed 80000\n",
      "[13:46:22] Parsed 90000\n",
      "[13:46:23] Parsed 100000\n",
      "[13:46:24] Parsed 110000\n",
      "[13:46:24] Parsed 120000\n",
      "[13:46:25] Parsed 130000\n",
      "[13:46:26] Parsed 140000\n",
      "[13:46:26] Parsed 150000\n",
      "[13:46:27] Parsed 160000\n",
      "[13:46:28] Parsed 170000\n",
      "[13:46:29] Parsed 180000\n",
      "[13:46:29] Parsed 190000\n",
      "[13:46:30] Parsed 200000\n",
      "[13:46:31] Parsed 210000\n",
      "[13:46:32] Parsed 220000\n",
      "[13:46:32] Parsed 230000\n",
      "[13:46:33] Parsed 240000\n",
      "[13:47:17] 140610/248060 (56.68%) reads were demultiplexed successfully\n",
      "[13:47:18] Processing D6NTU.fastq\n",
      "[13:47:21] Processing D6NTL.fastq\n",
      "[13:47:29] Processing D6WTL.fastq\n",
      "[13:47:32] Processing D6WTU.fastq\n",
      "[13:47:32] Processing DoLower.fastq\n",
      "[13:47:34] Processing D3NTU.fastq\n",
      "[13:47:36] Processing D3NTL.fastq\n",
      "[13:47:40] Processing D3WTL.fastq\n",
      "[13:47:41] Processing D3WTU.fastq\n",
      "[13:47:43] Processing 13118.fastq\n",
      "[13:47:44] Processing 13119.fastq\n",
      "[13:47:47] Processing 13120.fastq\n",
      "[13:47:47] 2_Singlebar_stat.csv is not in the accepted input format, skipping\n",
      "[13:47:47] Tirmming D6NTU.fastq\n",
      "[13:47:48] D6NTU Total reads: 17298, trimmed forward: 15394, trimmed reverse: 15440\n",
      "[13:47:48] Tirmming D6NTL.fastq\n",
      "[13:47:51] D6NTL Total reads: 40964, trimmed forward: 35936, trimmed reverse: 37050\n",
      "[13:47:51] Tirmming D6WTL.fastq\n",
      "[13:47:56] D6WTL Total reads: 11498, trimmed forward: 10450, trimmed reverse: 7640\n",
      "[13:47:56] Tirmming D6WTU.fastq\n",
      "[13:47:58] D6WTU Total reads: 1656, trimmed forward: 1460, trimmed reverse: 1082\n",
      "[13:47:58] Tirmming DoLower.fastq\n",
      "[13:47:59] DoLower Total reads: 8140, trimmed forward: 7252, trimmed reverse: 5644\n",
      "[13:47:59] Tirmming D3NTU.fastq\n",
      "[13:48:01] D3NTU Total reads: 8400, trimmed forward: 7480, trimmed reverse: 7484\n",
      "[13:48:01] Tirmming D3NTL.fastq\n",
      "[13:48:02] D3NTL Total reads: 20534, trimmed forward: 18224, trimmed reverse: 18168\n",
      "[13:48:02] Tirmming D3WTL.fastq\n",
      "[13:48:06] D3WTL Total reads: 5238, trimmed forward: 4674, trimmed reverse: 3322\n",
      "[13:48:06] Tirmming D3WTU.fastq\n",
      "[13:48:07] D3WTU Total reads: 7862, trimmed forward: 7092, trimmed reverse: 5206\n",
      "[13:48:07] Tirmming 13118.fastq\n",
      "[13:48:09] 13118 Total reads: 1970, trimmed forward: 1642, trimmed reverse: 1686\n",
      "[13:48:09] Tirmming 13119.fastq\n",
      "[13:48:09] 13119 Total reads: 15448, trimmed forward: 12508, trimmed reverse: 12340\n",
      "[13:48:09] Tirmming 13120.fastq\n",
      "[13:48:12] 13120 Total reads: 1602, trimmed forward: 1340, trimmed reverse: 1340\n",
      "Data processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "''' ===== Configuration ===== '''\n",
    "project_id = \"2024-000012\"  # Set this to your project ID\n",
    "\n",
    "\n",
    "''' Analysis settings '''\n",
    "input_format = \"fastq\"\n",
    "output_format = \"fastq\" #輸出檔案的格式，預設為 'both'。可以是 fastq 或 fasta。'both' 代表同時輸出 fastq 和 fasta\n",
    "mismatch_ratio_f = 0.1 #FwIndex容許的錯誤率，預設為0.15。例如barcode長度為20bp，則容許0.15*20=3bp的錯誤(edit distance)\n",
    "mismatch_ratio_r = 0.1 #RvAnchor容許的錯誤率，預設為0.15\n",
    "\n",
    "# Quality Filter Configuration\n",
    "QSCORE = 9 #recommended 7-9\n",
    "MIN_LEN = 700 #depends on the length of your reads\n",
    "MAX_LEN = 2000 #depends on the length of your reads\n",
    "\n",
    "# Demultiplexing Configuration\n",
    "expected_length_variation = 0.75 #預期的read長度變異，預設為0.3。例如預期的read長度為300bp，則容許0.3*300=90bp的變異\n",
    "search_range = 150 #搜尋barcode的範圍，預設為150bp。代表搜尋範圍為前150bp和後150bp\n",
    "rvc_rvanchor = False #預設為'False'。'True'則程式執行reverse-complement。\n",
    "\n",
    "# Orientation Correction Configuration\n",
    "orientation_search_range = 500 #搜尋FwPrimer和RvPrimer的範圍，預設為200bp。代表搜尋範圍為前200bp和後200bp。\n",
    "\n",
    "# Trim Reads Configuration\n",
    "fw_offset = 0 #從距離找到的切除位點開始往後切除幾個bp，預設為0，可以是負數。例如fw_offset=-10，則從距離找到的切除位點開始往前切除10個bp\n",
    "rv_offset = 0 #從距離找到的切除位點開始往前切除幾個bp，預設為0，可以是負數。例如rv_offset=-10，則從距離找到的切除位點開始往後切除10個bp\n",
    "discard_no_match = False\n",
    "check_both_directions = True\n",
    "reverse_complement_rv_col = True\n",
    "trimming_search_range = 200\n",
    "\n",
    "# Clustering Configuration (mmseqs_cluster)\n",
    "cluster_min_seq_id = 0.98\n",
    "cluster_mode = 0\n",
    "cov_mode = 0\n",
    "kmer_length = 15\n",
    "kmer_per_seq = 20\n",
    "sensitivity = 8.5\n",
    "min_read_num = 2\n",
    "suppress_output = False #suppress_output=False will output all details of the clustering process. Use it when unknown error occurs.\n",
    "\n",
    "# Consensus Configuration (mafft_consensus)\n",
    "minimal_reads = 2  # minimal_reads for consensus\n",
    "max_reads = -1 #max_reads: 設定最多的序列數量，-1 代表不限制。例如max_reads=100，則只會隨機取100個序列進行排比\n",
    "adjustdirection = False\n",
    "\n",
    "\n",
    "''' ===== Workflow ===== '''\n",
    "def process_data(project_id):\n",
    "    data_base_path = f\"/mnt/Data/{project_id}\"\n",
    "    src_path_dorado = os.path.join(data_base_path, \"1_dorado\")\n",
    "    des_path_nanofilt = os.path.join(data_base_path, \"2_nanofilt\")\n",
    "    des_path_demultiplex = os.path.join(data_base_path, \"3_demultiplex\")\n",
    "    des_path_orientation = os.path.join(data_base_path, \"4_orientation\")  # Orientation output folder\n",
    "    des_path_trimmed = os.path.join(data_base_path, \"5_trimmed\")  # Trimming output folder\n",
    "    barcode_index_file = os.path.join(data_base_path, f\"{project_id}.csv\")\n",
    "\n",
    "\n",
    "    # Step 1. Filter by Quality and Length\n",
    "    filtered_fastq = dumb.qualityfilt(\n",
    "        src = os.path.join(src_path_dorado, 'all.fastq'),\n",
    "        des = des_path_nanofilt,\n",
    "        name = 'all_qualityfilt.fastq',\n",
    "        QSCORE = QSCORE,\n",
    "        MIN_LEN = MIN_LEN,\n",
    "        MAX_LEN = MAX_LEN\n",
    "    )\n",
    "\n",
    "    # Step 2. Demultiplexing\n",
    "    demultiplexed = dumb.singlebar(\n",
    "        src = os.path.join(des_path_nanofilt, 'all_qualityfilt.fastq'),\n",
    "        des = des_path_demultiplex,\n",
    "        BARCODE_INDEX_FILE = barcode_index_file,\n",
    "        mismatch_ratio_f = mismatch_ratio_f,\n",
    "        mismatch_ratio_r = mismatch_ratio_r,\n",
    "        expected_length_variation = expected_length_variation,\n",
    "        search_range = search_range,\n",
    "        rvc_rvanchor = rvc_rvanchor,\n",
    "        input_format = input_format,\n",
    "        output_format = output_format\n",
    "    )\n",
    "\n",
    "    # Step 3. Orientation correction\n",
    "    orientation = dumb.orientation(\n",
    "        src = des_path_demultiplex,\n",
    "        des = des_path_orientation,\n",
    "        input_format = input_format,\n",
    "        output_format = output_format,\n",
    "        BARCODE_INDEX_FILE = barcode_index_file,\n",
    "        FwPrimer = \"FwPrimer\",\n",
    "        RvPrimer = \"RvPrimer\",\n",
    "        search_range = orientation_search_range\n",
    "    )\n",
    "    \n",
    "    # Step 4. Trim Reads\n",
    "    trimmed = dumb.trim_reads(\n",
    "        src = des_path_orientation,\n",
    "        des = des_path_trimmed,\n",
    "        BARCODE_INDEX_FILE = barcode_index_file,\n",
    "        fw_col = \"FwPrimer\",\n",
    "        rv_col = \"RvPrimer\",\n",
    "        input_format = input_format,\n",
    "        output_format = output_format,\n",
    "        mode = \"table\",\n",
    "        fw_offset = fw_offset,\n",
    "        rv_offset = rv_offset,\n",
    "        mismatch_ratio_f = mismatch_ratio_f,\n",
    "        mismatch_ratio_r = mismatch_ratio_r,\n",
    "        discard_no_match = discard_no_match,\n",
    "        check_both_directions = check_both_directions,\n",
    "        reverse_complement_rv_col = reverse_complement_rv_col,\n",
    "        search_range = trimming_search_range\n",
    "    )\n",
    "\n",
    "    return \"Data processing complete.\"\n",
    "\n",
    "# Call the function with the desired project ID:\n",
    "result = process_data(project_id)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf8ce9-9a6e-4146-abd3-20c8577f940e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Copy files for Kraken Analysis\n",
    "- if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b84f636d-2dbe-4b2e-a4fc-8332f454aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "\n",
    "''' ===== Configuration ===== '''\n",
    "project_id = \"Taiwan_soil_microbiome\"\n",
    "\n",
    "base_path = f\"/mnt/Data/{project_id}\"\n",
    "csv_file = os.path.join(base_path, \"Soil_Microbiome_Project.csv\")  # Path to your CSV file\n",
    "des_path = os.path.join(base_path, \"fastq\") # Destination directory for copied FASTQ files\n",
    "\n",
    "''' File path settings '''\n",
    "subfolder_col_base = \"Nanopore ID\"  # Base name for subfolder columns\n",
    "fastq_col_base = \"PCR ID\"  # Base name for fastq columns\n",
    "gene_name_lists = [\"16S\", \"ITS\"]  # Different gene name suffixes\n",
    "fastq_folder = \"5_trimmed\"\n",
    "\n",
    "''' ===== Workflow ===== '''\n",
    "def copy_fastq_files(csv_file, des_path):\n",
    "    df = pd.read_csv(csv_file, sep=',')\n",
    "    \n",
    "    # Loop through each gene name in the list\n",
    "    for gene_name in gene_name_lists:\n",
    "        # Create column names based on the base column name and gene suffix\n",
    "        subfolder_cols = [f'{subfolder_col_base} ({gene_name}_1)', f'{subfolder_col_base} ({gene_name}_2)']\n",
    "        fastq_cols = [f'{fastq_col_base} ({gene_name}_1)', f'{fastq_col_base} ({gene_name}_2)']\n",
    "\n",
    "        # Loop through each row in the DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "            for i, (subfolder_col, fastq_col) in enumerate(zip(subfolder_cols, fastq_cols), start=1):\n",
    "                subfolder = row.get(subfolder_col)\n",
    "                fastq_file = row.get(fastq_col)\n",
    "                \n",
    "                # If both subfolder and fastq file exist, construct the path\n",
    "                if pd.notna(subfolder) and pd.notna(fastq_file):\n",
    "                    # Construct the full path to the FASTQ file\n",
    "                    source_file = os.path.join(base_path, str(subfolder), fastq_folder, str(fastq_file) + \".fastq\")\n",
    "                    print(source_file)\n",
    "\n",
    "                    # Define the destination subfolder based on gene name and replicate number\n",
    "                    replicate_folder = f\"{gene_name}_replicate_{i}\"\n",
    "                    full_des_path = os.path.join(des_path, replicate_folder)\n",
    "\n",
    "                    # Ensure the destination directory exists\n",
    "                    os.makedirs(full_des_path, exist_ok=True)\n",
    "\n",
    "                    # Construct the destination file path\n",
    "                    dest_file = os.path.join(full_des_path, str(fastq_file) + \".fastq\")\n",
    "\n",
    "                    try:\n",
    "                        # Copy the FASTQ file to the destination directory\n",
    "                        shutil.copy2(source_file, dest_file)\n",
    "                        print(f\"Copied: {source_file} to {dest_file}\")\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"File not found: {source_file}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error copying file {source_file}: {e}\")\n",
    "\n",
    "# Call the function to copy the files\n",
    "copy_fastq_files(csv_file, des_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b37fe-cf50-47cd-ad11-0943984fc554",
   "metadata": {},
   "source": [
    "# 3. Microbiome Sequences Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a144d6-aebe-4af7-bdab-c84ab0b6fa62",
   "metadata": {},
   "source": [
    "## 3-1. Kraken2 execution\n",
    "- Kracken2 taxonomic assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c30e661-31b4-4328-8ceb-152cc3550a96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 FASTQ files. Processing Kraken2...\n",
      "Processing file for Kraken2: D3NTL.fastq (Base name: D3NTL)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading database information... done.\n",
      "20534 sequences (16.25 Mbp) processed in 0.822s (1498.6 Kseq/m, 1185.79 Mbp/m).\n",
      "  20532 sequences classified (99.99%)\n",
      "  2 sequences unclassified (0.01%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kraken2 analysis completed for D3NTL\n",
      "Processing file for Kraken2: D3NTU.fastq (Base name: D3NTU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading database information... done.\n",
      "8400 sequences (6.76 Mbp) processed in 0.111s (4526.4 Kseq/m, 3640.85 Mbp/m).\n",
      "  8400 sequences classified (100.00%)\n",
      "  0 sequences unclassified (0.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kraken2 analysis completed for D3NTU\n",
      "Processing file for Kraken2: D3WTL.fastq (Base name: D3WTL)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading database information... done.\n",
      "5238 sequences (5.77 Mbp) processed in 0.129s (2440.7 Kseq/m, 2687.60 Mbp/m).\n",
      "  5238 sequences classified (100.00%)\n",
      "  0 sequences unclassified (0.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kraken2 analysis completed for D3WTL\n",
      "Processing file for Kraken2: D3WTU.fastq (Base name: D3WTU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading database information... done.\n",
      "7862 sequences (8.72 Mbp) processed in 0.134s (3511.8 Kseq/m, 3896.80 Mbp/m).\n",
      "  7862 sequences classified (100.00%)\n",
      "  0 sequences unclassified (0.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kraken2 analysis completed for D3WTU\n",
      "Processing file for Kraken2: D6NTL.fastq (Base name: D6NTL)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading database information... done.\n",
      "40964 sequences (31.63 Mbp) processed in 0.253s (9700.0 Kseq/m, 7490.29 Mbp/m).\n",
      "  40962 sequences classified (100.00%)\n",
      "  2 sequences unclassified (0.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kraken2 analysis completed for D6NTL\n",
      "Processing file for Kraken2: D6NTU.fastq (Base name: D6NTU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading database information... done.\n",
      "17298 sequences (13.54 Mbp) processed in 0.167s (6231.1 Kseq/m, 4876.25 Mbp/m).\n",
      "  17296 sequences classified (99.99%)\n",
      "  2 sequences unclassified (0.01%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kraken2 analysis completed for D6NTU\n",
      "Processing file for Kraken2: D6WTL.fastq (Base name: D6WTL)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading database information... done.\n",
      "11498 sequences (12.49 Mbp) processed in 0.146s (4717.5 Kseq/m, 5125.62 Mbp/m).\n",
      "  11498 sequences classified (100.00%)\n",
      "  0 sequences unclassified (0.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kraken2 analysis completed for D6WTL\n",
      "Processing file for Kraken2: D6WTU.fastq (Base name: D6WTU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading database information... done.\n",
      "1656 sequences (1.87 Mbp) processed in 0.107s (931.8 Kseq/m, 1053.91 Mbp/m).\n",
      "  1656 sequences classified (100.00%)\n",
      "  0 sequences unclassified (0.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kraken2 analysis completed for D6WTU\n",
      "Processing file for Kraken2: DoLower.fastq (Base name: DoLower)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading database information... done.\n",
      "8140 sequences (8.99 Mbp) processed in 0.127s (3830.7 Kseq/m, 4230.06 Mbp/m).\n",
      "  8140 sequences classified (100.00%)\n",
      "  0 sequences unclassified (0.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kraken2 analysis completed for DoLower\n",
      "All Kraken2 analysis completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import subprocess\n",
    "\n",
    "\n",
    "''' ===== Configuration ===== '''\n",
    "project_id = \"2024-000012\"\n",
    "num_workers = 1  # Adjust this number based on your system's capabilities\n",
    "\n",
    "''' File path settings '''\n",
    "base_path =  f\"/mnt/Data/{project_id}\"  # Base directory containing the csv files\n",
    "fastq_dir = os.path.join(base_path, \"5_trimmed/Termites_soil\")\n",
    "output_dir = os.path.join(base_path, \"Termites_soil/kraken2_output\")\n",
    "kraken_db = \"/mnt/localdatabase/k2_unite-allEUK/\"\n",
    "\n",
    "\n",
    "''' ===== Workflow  ===== '''\n",
    "os.makedirs(output_dir, exist_ok=True) # Create the output directory if it doesn't exist\n",
    "\n",
    "# Function to check if Kraken 2 analysis is already done for a file\n",
    "def is_already_processed(base_name):\n",
    "    kraken_output = os.path.join(output_dir, f\"{base_name}.kraken2.out\")\n",
    "    kraken_report = os.path.join(output_dir, f\"{base_name}.kraken2.report\")\n",
    "    return os.path.exists(kraken_output) and os.path.exists(kraken_report)\n",
    "\n",
    "# Function to process each FASTQ file for Kraken 2 analysis\n",
    "def process_fastq_for_kraken(fastq_file):\n",
    "    base_name = os.path.splitext(fastq_file)[0]\n",
    "    print(f\"Processing file for Kraken2: {fastq_file} (Base name: {base_name})\")\n",
    "    if is_already_processed(base_name): # Skip processing if already processed\n",
    "        print(f\"Skipping {base_name}, already processed by Kraken2.\")\n",
    "        return\n",
    "\n",
    "    # Kraken 2 analysis\n",
    "    kraken_output = os.path.join(output_dir, f\"{base_name}.kraken2.out\")\n",
    "    kraken_report = os.path.join(output_dir, f\"{base_name}.kraken2.report\")\n",
    "    \n",
    "    kraken_cmd = [\n",
    "        \"kraken2\", \"--db\", kraken_db, \"--threads\", \"20\",\n",
    "        \"--confidence\", \"0.01\",  # Adjust confidence between 0 and 1 as needed\n",
    "        \"--memory-mapping\",\n",
    "        \"--output\", kraken_output,\n",
    "        \"--report\", kraken_report,\n",
    "        os.path.join(fastq_dir, fastq_file)\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(kraken_cmd, check=True)\n",
    "        print(f\"Kraken2 analysis completed for {base_name}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during Kraken2 analysis for {base_name}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if there are FASTQ files in the directory\n",
    "    fastq_files = [f for f in os.listdir(fastq_dir) if f.endswith(\".fastq\")]\n",
    "    \n",
    "    if not fastq_files:\n",
    "        print(f\"No FASTQ files found in {fastq_dir}.\")\n",
    "    else:\n",
    "        print(f\"Found {len(fastq_files)} FASTQ files. Processing Kraken2...\")\n",
    "\n",
    "    # Using ThreadPoolExecutor with a specified number of workers\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        executor.map(process_fastq_for_kraken, fastq_files)\n",
    "\n",
    "    print(\"All Kraken2 analysis completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8652d4-6bd5-4f84-b3a2-7d91bb647986",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Rarefaction Curve\n",
    "- To determine how much sequencing depth you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e304b994-af0c-40a3-a69b-5fae96ba4ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed... Creating species richness matrix...\n",
      "Executing R codes using Python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: In addition: \n",
      "R[write to console]: Warning messages:\n",
      "\n",
      "R[write to console]: 1: \n",
      "R[write to console]: In (function (package, help, pos = 2, lib.loc = NULL, character.only = FALSE,  :\n",
      "R[write to console]: \n",
      " \n",
      "R[write to console]:  libraries ‘/usr/local/lib/R/site-library’, ‘/usr/lib/R/site-library’ contain no packages\n",
      "\n",
      "R[write to console]: 2: \n",
      "R[write to console]: In (function (package, help, pos = 2, lib.loc = NULL, character.only = FALSE,  :\n",
      "R[write to console]: \n",
      " \n",
      "R[write to console]:  libraries ‘/usr/local/lib/R/site-library’, ‘/usr/lib/R/site-library’ contain no packages\n",
      "\n",
      "R[write to console]: 3: The shape palette can deal with a maximum of 6 discrete values because more\n",
      "than 6 becomes difficult to discriminate\n",
      "ℹ you have requested 218 values. Consider specifying shapes manually if you\n",
      "  need that many have them. \n",
      "\n",
      "R[write to console]: 4: Removed 212 rows containing missing values or values outside the scale range\n",
      "(`geom_point()`). \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species richness matrix and rarefaction analysis complete.\n",
      "The minimum sequencing depth is [19474.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from rpy2.robjects import pandas2ri, r\n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.vectors import FloatVector\n",
    "\n",
    "pandas2ri.activate() # Activate pandas2ri for DataFrame conversion\n",
    "\n",
    "''' ===== Configuration ===== '''\n",
    "project_id = \"Taiwan_soil_microbiome\"\n",
    "coverage_cutoff = 0.995\n",
    "\n",
    "''' File path settings '''\n",
    "base_path =  f\"/mnt/Data/{project_id}\"\n",
    "input_dir = Path(base_path) / \"thesis_data\"\n",
    "output_dir = Path(base_path) / \"thesis_data\"\n",
    "files = list(input_dir.glob(\"16S_r*_kraken_filtered/*.kraken2.report\"))\n",
    "output_csv = os.path.join(base_path, \"thesis_data/16S_species_richness_matrix.csv\")\n",
    "\n",
    "r.assign(\"output_dir\", str(output_dir))\n",
    "\n",
    "\n",
    "''' ===== Function ===== '''\n",
    "def parse_kraken_report(file_path, level=\"S\"):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            fields = line.strip().split(\"\\t\")\n",
    "            rank_code = fields[3]\n",
    "            if rank_code == level:  # Filter by taxonomic level\n",
    "                count = int(fields[1])\n",
    "                name = fields[5].strip()\n",
    "                data.append({\"Taxon\": name, \"Reads\": count})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "''' ===== Workflow ===== '''\n",
    "richness_matrix = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    file_name = file.name.replace(\".kraken2.report\", \"\")  \n",
    "    species_data = parse_kraken_report(file)\n",
    "    \n",
    "    if not species_data.empty:\n",
    "        species_data.set_index(\"Taxon\", inplace=True)\n",
    "        species_data = species_data[\"Reads\"].transpose()\n",
    "        \n",
    "        richness_matrix = pd.concat([richness_matrix, species_data], axis=1).fillna(0)\n",
    "\n",
    "richness_matrix.columns = [f.name.replace(\".kraken2.report\", \"\") for f in files]\n",
    "#richness_matrix = richness_matrix.transpose()\n",
    "richness_matrix.to_csv(output_csv)\n",
    "print(\"Analysis completed... Creating species richness matrix...\")\n",
    "\n",
    "print(\"Executing R codes using Python\")\n",
    "utils = rpackages.importr(\"utils\")\n",
    "utils.chooseCRANmirror(ind=1)  # Select the first CRAN mirror\n",
    "iNEXT = rpackages.importr(\"iNEXT\")\n",
    "\n",
    "richness_matrix_r = pandas2ri.py2rpy(richness_matrix)\n",
    "#print(richness_matrix_r)\n",
    "#print(richness_matrix.shape)\n",
    "#print(richness_matrix.head())\n",
    "#print(richness_matrix.isnull().sum())\n",
    "rf = iNEXT.iNEXT(richness_matrix_r, q=FloatVector([0]), datatype=\"abundance\", endpoint = 50000)\n",
    "r.assign(\"rf\", rf)\n",
    "r.assign(\"coverage_cutoff\", coverage_cutoff)\n",
    "\n",
    "r('''\n",
    "library(iNEXT)\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "\n",
    "rarefaction_curve <- ggiNEXT(rf, type = 1) + theme(legend.position=\"none\")\n",
    "ggsave(file.path(output_dir, \"rarefaction_curve.png\"), plot = rarefaction_curve, device = \"png\", width = 10, height = 8)\n",
    "\n",
    "rf_cov <- rf[[\"iNextEst\"]][[\"coverage_based\"]]\n",
    "uniq_cov <- sort(unique(rf_cov$Assemblage))\n",
    "cov <- rf_cov[rf_cov$SC >= coverage_cutoff, ]\n",
    "cov$Assemblage <- as.numeric(cov$Assemblage)\n",
    "\n",
    "low_cov <- cov %>% \n",
    "  group_by(Assemblage) %>%\n",
    "  slice_min(order_by = SC, with_ties = FALSE) %>%\n",
    "  arrange(Assemblage)\n",
    "\n",
    "low_cov <- low_cov[!is.infinite(low_cov$m), ]\n",
    "seq_depth <- max(low_cov$m)\n",
    "''')\n",
    "\n",
    "seq_depth = r(\"seq_depth\")\n",
    "print(\"Species richness matrix and rarefaction analysis complete.\")\n",
    "print(f\"The minimum sequencing depth is {seq_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1e9af-16f3-410a-8ec6-63e84e1d5f71",
   "metadata": {},
   "source": [
    "## 3-2. Kraken2 reads report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ad82455-047d-4aa6-838a-776903bcf543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Rank=D, Taxon Name=  Fungi, Tax ID=9, Reads=19990\n",
      "Debug: Rank=D, Taxon Name=  Eukaryota kgd Incertae sedis, Tax ID=8, Reads=338\n",
      "Debug: Rank=D, Taxon Name=  Metazoa, Tax ID=14, Reads=88\n",
      "Debug: Rank=D, Taxon Name=  Viridiplantae, Tax ID=23, Reads=44\n",
      "Debug: Rank=D, Taxon Name=  Amoebozoa, Tax ID=3, Reads=4\n",
      "{'sample_id': 'D3NTL', 'total_reads_root': 20532, 'taxon_name_root': 'root', 'taxon_reads': 19990, 'taxon_name': 'Fungi'}\n",
      "Debug: Rank=D, Taxon Name=  Fungi, Tax ID=9, Reads=8096\n",
      "Debug: Rank=D, Taxon Name=  Eukaryota kgd Incertae sedis, Tax ID=8, Reads=240\n",
      "Debug: Rank=D, Taxon Name=  Viridiplantae, Tax ID=23, Reads=24\n",
      "{'sample_id': 'D3NTU', 'total_reads_root': 8400, 'taxon_name_root': 'root', 'taxon_reads': 8096, 'taxon_name': 'Fungi'}\n",
      "Debug: Rank=D, Taxon Name=  Fungi, Tax ID=9, Reads=5232\n",
      "Debug: Rank=D, Taxon Name=  Eukaryota kgd Incertae sedis, Tax ID=8, Reads=4\n",
      "{'sample_id': 'D3WTL', 'total_reads_root': 5238, 'taxon_name_root': 'root', 'taxon_reads': 5232, 'taxon_name': 'Fungi'}\n",
      "Debug: Rank=D, Taxon Name=  Fungi, Tax ID=9, Reads=7860\n",
      "Debug: Rank=D, Taxon Name=  Viridiplantae, Tax ID=23, Reads=2\n",
      "{'sample_id': 'D3WTU', 'total_reads_root': 7862, 'taxon_name_root': 'root', 'taxon_reads': 7860, 'taxon_name': 'Fungi'}\n",
      "Debug: Rank=D, Taxon Name=  Fungi, Tax ID=9, Reads=40294\n",
      "Debug: Rank=D, Taxon Name=  Eukaryota kgd Incertae sedis, Tax ID=8, Reads=460\n",
      "Debug: Rank=D, Taxon Name=  Viridiplantae, Tax ID=23, Reads=68\n",
      "Debug: Rank=D, Taxon Name=  Metazoa, Tax ID=14, Reads=22\n",
      "Debug: Rank=D, Taxon Name=  Amoebozoa, Tax ID=3, Reads=4\n",
      "Debug: Rank=D, Taxon Name=  Stramenopila, Tax ID=22, Reads=4\n",
      "{'sample_id': 'D6NTL', 'total_reads_root': 40962, 'taxon_name_root': 'root', 'taxon_reads': 40294, 'taxon_name': 'Fungi'}\n",
      "Debug: Rank=D, Taxon Name=  Fungi, Tax ID=9, Reads=16692\n",
      "Debug: Rank=D, Taxon Name=  Eukaryota kgd Incertae sedis, Tax ID=8, Reads=448\n",
      "Debug: Rank=D, Taxon Name=  Viridiplantae, Tax ID=23, Reads=50\n",
      "Debug: Rank=D, Taxon Name=  Stramenopila, Tax ID=22, Reads=4\n",
      "Debug: Rank=D, Taxon Name=  Metazoa, Tax ID=14, Reads=4\n",
      "Debug: Rank=D, Taxon Name=  Amoebozoa, Tax ID=3, Reads=2\n",
      "Debug: Rank=D, Taxon Name=  Alveolata, Tax ID=2, Reads=2\n",
      "{'sample_id': 'D6NTU', 'total_reads_root': 17296, 'taxon_name_root': 'root', 'taxon_reads': 16692, 'taxon_name': 'Fungi'}\n",
      "Debug: Rank=D, Taxon Name=  Fungi, Tax ID=9, Reads=11488\n",
      "Debug: Rank=D, Taxon Name=  Metazoa, Tax ID=14, Reads=8\n",
      "Debug: Rank=D, Taxon Name=  Viridiplantae, Tax ID=23, Reads=2\n",
      "{'sample_id': 'D6WTL', 'total_reads_root': 11498, 'taxon_name_root': 'root', 'taxon_reads': 11488, 'taxon_name': 'Fungi'}\n",
      "Debug: Rank=D, Taxon Name=  Fungi, Tax ID=9, Reads=1654\n",
      "Debug: Rank=D, Taxon Name=  Metazoa, Tax ID=14, Reads=2\n",
      "{'sample_id': 'D6WTU', 'total_reads_root': 1656, 'taxon_name_root': 'root', 'taxon_reads': 1654, 'taxon_name': 'Fungi'}\n",
      "Debug: Rank=D, Taxon Name=  Fungi, Tax ID=9, Reads=8134\n",
      "{'sample_id': 'DoLower', 'total_reads_root': 8140, 'taxon_name_root': 'root', 'taxon_reads': 8134, 'taxon_name': 'Fungi'}\n",
      "Processing file 9/9 ...\n",
      "Reads report generated\n"
     ]
    }
   ],
   "source": [
    "''' latest'''\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "''' ===== Configuration ===== ''' \n",
    "project_id = \"2024-000012\"\n",
    "\n",
    "''' File path settings '''\n",
    "base_path = f\"/mnt/Data/{project_id}\"  # Base directory containing the CSV files\n",
    "input_dir = os.path.join(base_path, \"Termites_soil/kraken2_output\")\n",
    "output_csv = os.path.join(base_path, \"2024-000012_reads.csv\")\n",
    "taxon_rank = 'D'                            # Taxon rank input by user \n",
    "taxid = 9                                # Taxid input by user\n",
    "taxn = \"Fungi\"\n",
    "\n",
    "\n",
    "''' ===== Functions ===== '''\n",
    "def parse_kraken2_report(file_path, taxid, taxon_rank):\n",
    "    match = re.match(r\"(.+?)\\.kraken2(?:\\.report)?$\", Path(file_path).name)\n",
    "    sample_id = match.group(1) if match else Path(file_path).stem\n",
    "    total_reads_root = None\n",
    "    taxid_reads = None\n",
    "    taxn_reads = None\n",
    "    taxon_name_root = \"\"\n",
    "    taxon_name = \"\"\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            fields = line.strip().split('\\t')\n",
    "            perc_reads, total_reads, reads_assigned, rank, tax_id, taxon_name_temp = fields\n",
    "\n",
    "            if tax_id == '1':\n",
    "                total_reads_root = int(total_reads)\n",
    "                taxon_name_root = taxon_name_temp.strip()\n",
    "            \n",
    "            if taxn in taxon_name_temp and rank == taxon_rank:\n",
    "                taxn_reads = int(total_reads)\n",
    "                taxon_name = taxon_name_temp.strip()\n",
    "      \n",
    "    return {\n",
    "        'sample_id': sample_id,\n",
    "        'total_reads_root': total_reads_root,\n",
    "        'taxon_name_root': taxon_name_root,\n",
    "        'taxon_reads': taxn_reads,\n",
    "        'taxon_name': taxon_name\n",
    "    }\n",
    "\n",
    "def process_reports(input_dir, taxn, taxon_rank, output_csv):\n",
    "    report_files = list(Path(input_dir).rglob(\"*.kraken2.report\"))\n",
    "    total_files = len(report_files)\n",
    "    results = []\n",
    "    \n",
    "    for count, report_path in enumerate(report_files, start=1):\n",
    "        result = parse_kraken2_report(report_path, taxn, taxon_rank)\n",
    "        print(result)\n",
    "        if result['total_reads_root'] is not None and result['taxon_reads'] is not None:\n",
    "            results.append([\n",
    "                result['sample_id'], \n",
    "                result['total_reads_root'], \n",
    "                result['taxon_name_root'], \n",
    "                result['taxon_reads'], \n",
    "                result['taxon_name']\n",
    "            ])\n",
    "        \n",
    "        print(f\"Processing file {count}/{total_files} ...\", end='\\r')\n",
    "\n",
    "    results.sort(key=lambda x: x[0])  # Sort by the first column (Sample ID)\n",
    "\n",
    "    # Write sorted data to CSV\n",
    "    with open(output_csv, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\n",
    "            'Sample ID', \n",
    "            'Root Reads (taxid=1)', \n",
    "            'Root Name (rank=R)', \n",
    "            f'Taxon Reads (taxon={taxn}, rank={taxon_rank})', \n",
    "            f'Taxon Name (taxon={taxn}, rank={taxon_rank})'\n",
    "        ])\n",
    "        writer.writerows(results)\n",
    "    \n",
    "    print(\"\\nReads report generated\")\n",
    "\n",
    "\n",
    "''' ===== Workflow ===== '''\n",
    "process_reports(input_dir, taxn, taxon_rank, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48fa0b3-abfe-4b64-8bfc-dafd1fbabb89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Reads Comparison between Database\n",
    "- if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b161ee15-b84f-473d-a864-3a6f140c9e31",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Reads (taxid=1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m''' ===== Main Workflow ===== '''\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Read both QC files\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m data_1 \u001b[38;5;241m=\u001b[39m \u001b[43mread_qc_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m data_2 \u001b[38;5;241m=\u001b[39m read_qc_file(file_2)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Compare data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mread_qc_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m sample_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSample ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Extract root and taxon reads as integers\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m root_reads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReads (taxid=1)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     20\u001b[0m taxon_reads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReads (taxid=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtaxid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, rank=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtaxon_rank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     21\u001b[0m data[sample_id] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRoot Reads\u001b[39m\u001b[38;5;124m'\u001b[39m: root_reads, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTaxon Reads\u001b[39m\u001b[38;5;124m'\u001b[39m: taxon_reads}\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Reads (taxid=1)'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "''' ===== Configuration ===== '''\n",
    "project_id = \"2024-000012\"\n",
    "\n",
    "''' File path settings '''\n",
    "base_path = f\"/mnt/Data/{project_id}\"  # Base directory containing the CSV files\n",
    "file_1 = os.path.join(base_path, \"kraken_refseq_qc.csv\")  # First QC CSV file\n",
    "file_2 = os.path.join(base_path, \"kraken_D3_qc.csv\")  # Second QC CSV file\n",
    "output_csv = os.path.join(base_path, \"kraken_qc_comparison.csv\")  # Output comparison CSV\n",
    "\n",
    "\n",
    "''' ===== Functions ===== '''\n",
    "def read_qc_file(file_path):\n",
    "    data = {}\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            sample_id = row['Sample ID']\n",
    "            # Extract root and taxon reads as integers\n",
    "            root_reads = int(row['Reads (taxid=1)'])\n",
    "            taxon_reads = int(row[f'Reads (taxid={taxid}, rank={taxon_rank})'])\n",
    "            data[sample_id] = {'Root Reads': root_reads, 'Taxon Reads': taxon_reads}\n",
    "    return data\n",
    "\n",
    "def compare_qc_data(data_1, data_2):\n",
    "    comparison = []\n",
    "    all_sample_ids = set(data_1.keys()).union(data_2.keys())\n",
    "    \n",
    "    for sample_id in sorted(all_sample_ids):\n",
    "        root_reads_1 = data_1.get(sample_id, {}).get('Root Reads', 'N/A')\n",
    "        taxon_reads_1 = data_1.get(sample_id, {}).get('Taxon Reads', 'N/A')\n",
    "        root_reads_2 = data_2.get(sample_id, {}).get('Root Reads', 'N/A')\n",
    "        taxon_reads_2 = data_2.get(sample_id, {}).get('Taxon Reads', 'N/A')\n",
    "\n",
    "        comparison.append({\n",
    "            'Sample ID': sample_id,\n",
    "            'Root Reads (File 1)': root_reads_1,\n",
    "            'Taxon Reads (File 1)': taxon_reads_1,\n",
    "            'Root Reads (File 2)': root_reads_2,\n",
    "            'Taxon Reads (File 2)': taxon_reads_2,\n",
    "            'Root Reads Difference': (root_reads_2 - root_reads_1) if root_reads_1 != 'N/A' and root_reads_2 != 'N/A' else 'N/A',\n",
    "            'Taxon Reads Difference': (taxon_reads_2 - taxon_reads_1) if taxon_reads_1 != 'N/A' and taxon_reads_2 != 'N/A' else 'N/A'\n",
    "        })\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "def write_comparison_to_csv(comparison_data, output_csv):\n",
    "    with open(output_csv, 'w', newline='') as csvfile:\n",
    "        fieldnames = [\n",
    "            'Sample ID', \n",
    "            'Root Reads (File 1)', \n",
    "            'Taxon Reads (File 1)', \n",
    "            'Root Reads (File 2)', \n",
    "            'Taxon Reads (File 2)',\n",
    "            'Root Reads Difference', \n",
    "            'Taxon Reads Difference'\n",
    "        ]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(comparison_data)\n",
    "\n",
    "    print(\"\\nComparison report generated\")\n",
    "\n",
    "\n",
    "''' ===== Workflow ===== '''\n",
    "# Read both QC files\n",
    "data_1 = read_qc_file(file_1)\n",
    "data_2 = read_qc_file(file_2)\n",
    "\n",
    "# Compare data\n",
    "comparison_data = compare_qc_data(data_1, data_2)\n",
    "\n",
    "# Write comparison results to CSV\n",
    "write_comparison_to_csv(comparison_data, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efecdc52-e86b-4124-9a8d-c0415f8597df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Kraken Checkpoint Filtering\n",
    "- if necessary\n",
    "- List reports with reads < 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ce00e7e-52eb-483a-8988-849a0338cb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /mnt/Data/Taiwan_soil_microbiome/kraken2_failed.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "''' ===== Configuration ===== '''      \n",
    "project_id = \"Taiwan_soil_microbiome\"\n",
    "\n",
    "''' File path settings '''\n",
    "base_path = f\"/mnt/Data/{project_id}\" \n",
    "input_dir = os.path.join(base_path, \"\")  \n",
    "output_csv = os.path.join(base_path, \"kraken2_failed.csv\")\n",
    "target_dir = \"chemical_analysis\"\n",
    "\n",
    "\n",
    "''' ===== Functions  ===== '''\n",
    "# Function to extract sample ID from .kraken2.report files\n",
    "def extract_sample_id_from_kraken_reports(input_dir, output_csv):\n",
    "    results = []\n",
    "\n",
    "    # Iterate through all subdirectories in the given folder\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        # Only process subdirectories containing \"chemical_analysis\"\n",
    "        if target_dir in root:\n",
    "            for filename in files:\n",
    "                if filename.endswith(\".kraken2.report\"):\n",
    "                    file_path = os.path.join(root, filename)\n",
    "\n",
    "                    # Open and read the file line by line to find rows with tax rank 'R'\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        for line in file:\n",
    "                            columns = line.strip().split()\n",
    "\n",
    "                            # Ensure the row has enough columns and check if taxonomic rank is 'R'\n",
    "                            if len(columns) >= 6 and columns[3] == 'R':\n",
    "                                # Extract the second column (reads number) and convert to integer\n",
    "                                second_column_value = int(columns[1])\n",
    "\n",
    "                                # Filter: only include sample if reads number (second column) <= 2000\n",
    "                                if second_column_value <= 2000:\n",
    "                                    # Extract sample ID (assuming the sample ID is the number in the filename)\n",
    "                                    sample_id = filename.split('.')[0]\n",
    "                                    # Store the sample ID and second column value\n",
    "                                    results.append((sample_id, second_column_value))\n",
    "                                break  # Exit loop after processing the first 'R' rank\n",
    "\n",
    "    # Sort the results by sample ID in ascending order\n",
    "    results.sort(key=lambda x: int(x[0]))\n",
    "\n",
    "    # Write the sorted results to the CSV file\n",
    "    with open(output_csv, 'w', newline='') as csvfile:  # 'w' mode overwrites the file\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Sample ID', 'Reads'])\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(f\"Results saved to {output_csv}\")\n",
    "\n",
    "\n",
    "''' ===== Workflow ===== '''\n",
    "extract_sample_id_from_kraken_reports(input_dir, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ff844-a00c-432c-824a-ece92501e591",
   "metadata": {},
   "source": [
    "# 3-2. Kraken Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7b78cb-a7d4-4654-a575-c3628d9dc7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ...\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10957.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10957.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10957.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10958.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10958.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10958.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10959.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10959.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10959.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10960.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10960.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10960.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10961.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10961.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10961.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10962.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10962.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10962.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10963.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10963.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10963.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10964.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10964.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10964.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10965.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10965.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10965.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10969.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10969.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10969.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10970.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10970.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10970.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10971.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10971.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10971.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10972.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10972.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10972.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10973.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10973.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10973.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10974.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10974.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10974.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10976.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10976.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10976.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10977.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10977.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10977.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10978.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10978.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10978.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10979.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10979.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10979.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10981.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10981.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10981.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10982.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10982.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10982.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10983.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10983.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10983.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10984.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10984.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10984.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10985.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10985.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10985.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10986.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10986.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10986.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10987.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10987.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10987.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10988.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10988.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10988.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10989.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10989.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10989.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10991.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10991.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10991.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10992.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10992.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10992.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10993.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10993.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10993.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10994.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10994.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10994.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10995.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10995.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10995.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10996.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10996.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10996.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10997.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10997.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10997.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10998.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10998.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10998.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/10999.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10999.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/10999.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11001.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11001.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11001.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11002.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11002.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11002.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11003.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11003.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11003.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11004.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11004.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11004.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11005.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11005.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11005.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11006.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11006.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11006.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11007.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11007.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11007.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11008.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11008.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11008.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11009.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11009.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11009.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11011.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11011.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11011.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11012.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11012.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11012.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11013.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11013.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11013.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11014.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11014.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11014.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11015.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11015.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11015.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11016.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11016.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11016.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11017.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11017.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11017.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11018.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11018.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11018.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11019.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11019.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11019.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11020.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11020.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11020.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11021.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11021.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11021.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11022.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11022.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11022.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11024.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11024.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11024.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11025.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11025.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11025.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11026.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11026.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11026.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11027.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11027.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11027.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11028.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11028.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11028.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11030.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11030.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11030.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11031.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11031.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11031.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11033.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11033.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11033.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11034.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11034.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11034.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11035.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11035.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11035.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11037.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11037.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11037.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11038.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11038.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11038.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11039.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11039.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11039.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11040.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11040.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11040.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11041.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11041.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11041.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11042.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11042.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11042.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11043.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11043.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11043.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11044.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11044.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11044.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11046.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11046.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11046.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11047.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11047.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11047.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11050.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11050.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11050.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11051.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11051.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11051.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11052.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11052.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11052.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11064.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11064.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11064.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11066.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11066.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11066.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11082.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11082.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11082.kraken2.report\n",
      "Processing /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken/11083.kraken2.report -> /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11083.kraken2.report\n",
      "Output saved to /mnt/Data/Taiwan_soil_microbiome/thesis_data/16S_r2_kraken_filtered/11083.kraken2.report\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "''' ===== Configuration ===== '''          \n",
    "project_id = \"Taiwan_soil_microbiome\"\n",
    "\n",
    "''' File path settings '''\n",
    "base_path = f\"/mnt/Data/{project_id}\" \n",
    "input_dir = os.path.join(base_path, \"thesis_data/16S_r2_kraken\")\n",
    "output_dir = os.path.join(base_path, \"thesis_data/16S_r2_kraken_filtered\")\n",
    "include_taxid = 2  # Taxid to filter\n",
    "taxon_rank = 'D'  # Rank code to filter (e.g., D for domain)\n",
    "\n",
    "\n",
    "''' ===== Functions ===== ''' \n",
    "def create_taxon_rank_hierarchy():\n",
    "    # Define base ranks\n",
    "    ranks = ['R', 'D', 'K', 'P', 'C', 'O', 'F', 'G', 'S']\n",
    "    taxon_rank_hierarchy = {} # Initialize the hierarchy dictionary\n",
    "    rank_counter = 1\n",
    "    # Loop over the base ranks and their numbered versions\n",
    "    for rank in ranks:\n",
    "        taxon_rank_hierarchy[rank] = rank_counter # Add the base rank to the hierarchy\n",
    "        rank_counter += 1\n",
    "        for i in range(1, 10):\n",
    "            rank_code = f\"{rank}{i}\"\n",
    "            taxon_rank_hierarchy[rank_code] = rank_counter\n",
    "            rank_counter += 1\n",
    "    return taxon_rank_hierarchy\n",
    "\n",
    "def get_primary_rank(rank_code):\n",
    "    return ''.join(filter(str.isalpha, rank_code))\n",
    "\n",
    "def parse_kraken_report(file_path, include_taxid, taxon_rank, output_file):\n",
    "    # Load Kraken report\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = []\n",
    "        for line in file:\n",
    "            fields = line.strip().split('\\t')\n",
    "            percentage, num_reads, direct_reads, rank_code, taxid, name = fields\n",
    "            # Create a dictionary for each entry\n",
    "            data.append({\n",
    "                'percentage': float(percentage),\n",
    "                'num_reads': int(num_reads),\n",
    "                'direct_reads': int(direct_reads),\n",
    "                'rank_code': rank_code,\n",
    "                'primary_rank': get_primary_rank(rank_code),  # Determine the primary rank\n",
    "                'taxid': int(taxid),\n",
    "                'name': name,  # Keep unstripped name with indentation\n",
    "                'indentation': len(name) - len(name.strip())  # Indentation level (spaces before name)\n",
    "            })\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Step 1: Initialize variables for filtering\n",
    "    taxon_rank_hierarchy = create_taxon_rank_hierarchy()\n",
    "    start_index = df[df['taxid'] != 0].index[0]\n",
    "    filtered_data = []\n",
    "    upstream_data = []\n",
    "    target_index = None\n",
    "    target_indentation = None\n",
    "    found_target = False\n",
    "    downstream_end_index = None\n",
    "\n",
    "    # Step 2: Process the rows\n",
    "    for i in range(start_index, len(df)):\n",
    "        row = df.iloc[i]\n",
    "\n",
    "        # Check if we found the target row with the given taxon_rank and taxid\n",
    "        if (row['rank_code'] == taxon_rank and row['taxid'] == include_taxid):\n",
    "            target_index = i\n",
    "            target_indentation = row['indentation']\n",
    "            found_target = True\n",
    "            continue\n",
    "\n",
    "        # Append until a higher rank is found or until the end of the dataframe\n",
    "        if found_target:\n",
    "            if (row['rank_code'] == taxon_rank and row['taxid'] != include_taxid):\n",
    "                filtered_data.append(row)\n",
    "                if downstream_end_index is None:\n",
    "                    downstream_end_index = i\n",
    "\n",
    "            # Flexible filtering logic based on ranks and indentation after the target\n",
    "            if (taxon_rank_hierarchy.get(row['rank_code'], float('inf')) < taxon_rank_hierarchy[taxon_rank]\n",
    "                and row['indentation'] < target_indentation):\n",
    "                filtered_data.append(row)\n",
    "\n",
    "    if downstream_end_index is None:\n",
    "        downstream_end_index = len(df)\n",
    "\n",
    "    # Convert collected data into DataFrames\n",
    "    df_filtered_data = pd.DataFrame(filtered_data)\n",
    "    df_upstream_data = pd.DataFrame(df.iloc[start_index : target_index])\n",
    "    df_downstream_data = pd.DataFrame(df.iloc[target_index : downstream_end_index])\n",
    "\n",
    "    # Deduct the filtered data reads\n",
    "    if df_filtered_data.empty:\n",
    "        filtered_reads = 0\n",
    "    else:\n",
    "        filtered_reads = df_filtered_data['num_reads'].sum()\n",
    "    df_upstream_data['num_reads'] = df_upstream_data['num_reads'] - filtered_reads\n",
    "\n",
    "    # Final data\n",
    "    filtered_data = pd.concat([df_upstream_data, df_downstream_data]) # Combine the data before and after the input taxon_rank and taxid\n",
    "\n",
    "    # Set the first row percentage to 100% and round the percentages to 3 decimal places\n",
    "    first_row_reads = filtered_data['num_reads'].iloc[0]\n",
    "    filtered_data['percentage'] = round((filtered_data['num_reads'] / first_row_reads) * 100, 3)\n",
    "\n",
    "    # Rearrange the DataFrame to match the original Kraken2 report format\n",
    "    filtered_data = filtered_data[['percentage', 'num_reads', 'direct_reads', 'rank_code', 'taxid', 'name']]\n",
    "\n",
    "    # Save the filtered data to a new .report file\n",
    "    filtered_data.to_csv(output_file, sep='\\t', index=False, header=False)\n",
    "\n",
    "    print(f\"Output saved to {output_file}\")\n",
    "\n",
    "def process_all_kraken_reports(input_dir, output_dir, include_taxid, taxon_rank):\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)  # Delete the output directory if it exists\n",
    "    os.makedirs(output_dir)  # Create the output directory\n",
    "    \n",
    "    print(\"Processing ...\")\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.kraken2.report'):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            base_name = os.path.splitext(filename)[0]  # Get the base filename without extension\n",
    "            output_file = os.path.join(output_dir, f'{base_name}.report')  # Output as .kraken2.report\n",
    "\n",
    "            print(f\"Processing {file_path} -> {output_file}\")\n",
    "            parse_kraken_report(file_path, include_taxid, taxon_rank, output_file)\n",
    "\n",
    "\n",
    "''' ===== Workflow ===== '''\n",
    "process_all_kraken_reports(input_dir, output_dir, include_taxid, taxon_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aded3f-8bd2-48eb-abe8-d2d1fdf4a25f",
   "metadata": {},
   "source": [
    "# 3-3. Bracken analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5868e3d-0dcc-49bc-94f8-832628a6b814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 Kraken2 reports. Processing Bracken...\n",
      "Processing Kraken report for Bracken: D3NTL.kraken2.report (Base name: D3NTL.kraken2)\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTL.kraken2.bracken_P.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l P -t 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM START TIME: 01-03-2025 05:42:58\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of phylums in sample: 13 \n",
      "\t  >> Number of phylums with reads > threshold: 8 \n",
      "\t  >> Number of phylums with reads < threshold: 5 \n",
      "    >>> Total reads in sample: 20534\n",
      "\t  >> Total reads kept at phylums level (reads > threshold): 20404\n",
      "\t  >> Total reads discarded (phylums reads < threshold): 18\n",
      "\t  >> Reads distributed: 110\n",
      "\t  >> Reads not distributed (eg. no phylums above threshold): 0\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTL.kraken2.bracken_P.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:58\n",
      "  Bracken complete.\n",
      "Bracken P level analysis completed for D3NTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTL.kraken2.bracken_C.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l C -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:58\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of classes in sample: 22 \n",
      "\t  >> Number of classes with reads > threshold: 11 \n",
      "\t  >> Number of classes with reads < threshold: 11 \n",
      "    >>> Total reads in sample: 20534\n",
      "\t  >> Total reads kept at classes level (reads > threshold): 20340\n",
      "\t  >> Total reads discarded (classes reads < threshold): 40\n",
      "\t  >> Reads distributed: 152\n",
      "\t  >> Reads not distributed (eg. no classes above threshold): 0\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTL.kraken2.bracken_C.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:58\n",
      "  Bracken complete.\n",
      "Bracken C level analysis completed for D3NTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTL.kraken2.bracken_O.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l O -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:58\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of orders in sample: 34 \n",
      "\t  >> Number of orders with reads > threshold: 17 \n",
      "\t  >> Number of orders with reads < threshold: 17 \n",
      "    >>> Total reads in sample: 20534\n",
      "\t  >> Total reads kept at orders level (reads > threshold): 20144\n",
      "\t  >> Total reads discarded (orders reads < threshold): 62\n",
      "\t  >> Reads distributed: 324\n",
      "\t  >> Reads not distributed (eg. no orders above threshold): 2\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTL.kraken2.bracken_O.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:58\n",
      "  Bracken complete.\n",
      "Bracken O level analysis completed for D3NTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTL.kraken2.bracken_F.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l F -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:58\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of families in sample: 52 \n",
      "\t  >> Number of families with reads > threshold: 22 \n",
      "\t  >> Number of families with reads < threshold: 30 \n",
      "    >>> Total reads in sample: 20534\n",
      "\t  >> Total reads kept at families level (reads > threshold): 19814\n",
      "\t  >> Total reads discarded (families reads < threshold): 98\n",
      "\t  >> Reads distributed: 606\n",
      "\t  >> Reads not distributed (eg. no families above threshold): 14\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTL.kraken2.bracken_F.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:58\n",
      "  Bracken complete.\n",
      "Bracken F level analysis completed for D3NTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTL.kraken2.bracken_G.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l G -t 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM START TIME: 01-03-2025 05:42:58\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of genuses in sample: 74 \n",
      "\t  >> Number of genuses with reads > threshold: 17 \n",
      "\t  >> Number of genuses with reads < threshold: 57 \n",
      "    >>> Total reads in sample: 20534\n",
      "\t  >> Total reads kept at genuses level (reads > threshold): 19402\n",
      "\t  >> Total reads discarded (genuses reads < threshold): 172\n",
      "\t  >> Reads distributed: 880\n",
      "\t  >> Reads not distributed (eg. no genuses above threshold): 78\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTL.kraken2.bracken_G.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:58\n",
      "  Bracken complete.\n",
      "Bracken G level analysis completed for D3NTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTL.kraken2.bracken_S.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l S -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:58\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of species in sample: 82 \n",
      "\t  >> Number of species with reads > threshold: 18 \n",
      "\t  >> Number of species with reads < threshold: 64 \n",
      "    >>> Total reads in sample: 20534\n",
      "\t  >> Total reads kept at species level (reads > threshold): 18772\n",
      "\t  >> Total reads discarded (species reads < threshold): 194\n",
      "\t  >> Reads distributed: 1338\n",
      "\t  >> Reads not distributed (eg. no species above threshold): 228\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTL.kraken2.bracken_S.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:58\n",
      "  Bracken complete.\n",
      "Bracken S level analysis completed for D3NTL.kraken2\n",
      "Processing Kraken report for Bracken: D3NTU.kraken2.report (Base name: D3NTU.kraken2)\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTU.kraken2.bracken_P.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l P -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of phylums in sample: 8 \n",
      "\t  >> Number of phylums with reads > threshold: 5 \n",
      "\t  >> Number of phylums with reads < threshold: 3 \n",
      "    >>> Total reads in sample: 8400\n",
      "\t  >> Total reads kept at phylums level (reads > threshold): 8324\n",
      "\t  >> Total reads discarded (phylums reads < threshold): 14\n",
      "\t  >> Reads distributed: 62\n",
      "\t  >> Reads not distributed (eg. no phylums above threshold): 0\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTU.kraken2.bracken_P.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken P level analysis completed for D3NTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTU.kraken2.bracken_C.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l C -t 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of classes in sample: 15 \n",
      "\t  >> Number of classes with reads > threshold: 8 \n",
      "\t  >> Number of classes with reads < threshold: 7 \n",
      "    >>> Total reads in sample: 8400\n",
      "\t  >> Total reads kept at classes level (reads > threshold): 8296\n",
      "\t  >> Total reads discarded (classes reads < threshold): 26\n",
      "\t  >> Reads distributed: 78\n",
      "\t  >> Reads not distributed (eg. no classes above threshold): 0\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTU.kraken2.bracken_C.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken C level analysis completed for D3NTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTU.kraken2.bracken_O.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l O -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of orders in sample: 28 \n",
      "\t  >> Number of orders with reads > threshold: 7 \n",
      "\t  >> Number of orders with reads < threshold: 21 \n",
      "    >>> Total reads in sample: 8400\n",
      "\t  >> Total reads kept at orders level (reads > threshold): 8192\n",
      "\t  >> Total reads discarded (orders reads < threshold): 84\n",
      "\t  >> Reads distributed: 116\n",
      "\t  >> Reads not distributed (eg. no orders above threshold): 8\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTU.kraken2.bracken_O.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken O level analysis completed for D3NTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTU.kraken2.bracken_F.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l F -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of families in sample: 41 \n",
      "\t  >> Number of families with reads > threshold: 10 \n",
      "\t  >> Number of families with reads < threshold: 31 \n",
      "    >>> Total reads in sample: 8400\n",
      "\t  >> Total reads kept at families level (reads > threshold): 8064\n",
      "\t  >> Total reads discarded (families reads < threshold): 96\n",
      "\t  >> Reads distributed: 228\n",
      "\t  >> Reads not distributed (eg. no families above threshold): 12\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTU.kraken2.bracken_F.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken F level analysis completed for D3NTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTU.kraken2.bracken_G.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l G -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of genuses in sample: 52 \n",
      "\t  >> Number of genuses with reads > threshold: 11 \n",
      "\t  >> Number of genuses with reads < threshold: 41 \n",
      "    >>> Total reads in sample: 8400\n",
      "\t  >> Total reads kept at genuses level (reads > threshold): 7858\n",
      "\t  >> Total reads discarded (genuses reads < threshold): 124\n",
      "\t  >> Reads distributed: 392\n",
      "\t  >> Reads not distributed (eg. no genuses above threshold): 26\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTU.kraken2.bracken_G.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken G level analysis completed for D3NTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTU.kraken2.bracken_S.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l S -t 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3NTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of species in sample: 51 \n",
      "\t  >> Number of species with reads > threshold: 8 \n",
      "\t  >> Number of species with reads < threshold: 43 \n",
      "    >>> Total reads in sample: 8400\n",
      "\t  >> Total reads kept at species level (reads > threshold): 7612\n",
      "\t  >> Total reads discarded (species reads < threshold): 146\n",
      "\t  >> Reads distributed: 436\n",
      "\t  >> Reads not distributed (eg. no species above threshold): 206\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3NTU.kraken2.bracken_S.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken S level analysis completed for D3NTU.kraken2\n",
      "Processing Kraken report for Bracken: D3WTL.kraken2.report (Base name: D3WTL.kraken2)\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTL.kraken2.bracken_P.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l P -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of phylums in sample: 4 \n",
      "\t  >> Number of phylums with reads > threshold: 2 \n",
      "\t  >> Number of phylums with reads < threshold: 2 \n",
      "    >>> Total reads in sample: 5238\n",
      "\t  >> Total reads kept at phylums level (reads > threshold): 5222\n",
      "\t  >> Total reads discarded (phylums reads < threshold): 6\n",
      "\t  >> Reads distributed: 10\n",
      "\t  >> Reads not distributed (eg. no phylums above threshold): 0\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTL.kraken2.bracken_P.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken P level analysis completed for D3WTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTL.kraken2.bracken_C.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l C -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of classes in sample: 5 \n",
      "\t  >> Number of classes with reads > threshold: 2 \n",
      "\t  >> Number of classes with reads < threshold: 3 \n",
      "    >>> Total reads in sample: 5238\n",
      "\t  >> Total reads kept at classes level (reads > threshold): 5214\n",
      "\t  >> Total reads discarded (classes reads < threshold): 14\n",
      "\t  >> Reads distributed: 10\n",
      "\t  >> Reads not distributed (eg. no classes above threshold): 0\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTL.kraken2.bracken_C.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken C level analysis completed for D3WTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTL.kraken2.bracken_O.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l O -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of orders in sample: 8 \n",
      "\t  >> Number of orders with reads > threshold: 2 \n",
      "\t  >> Number of orders with reads < threshold: 6 \n",
      "    >>> Total reads in sample: 5238\n",
      "\t  >> Total reads kept at orders level (reads > threshold): 5206\n",
      "\t  >> Total reads discarded (orders reads < threshold): 20\n",
      "\t  >> Reads distributed: 10\n",
      "\t  >> Reads not distributed (eg. no orders above threshold): 2\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTL.kraken2.bracken_O.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken O level analysis completed for D3WTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTL.kraken2.bracken_F.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l F -t 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of families in sample: 9 \n",
      "\t  >> Number of families with reads > threshold: 2 \n",
      "\t  >> Number of families with reads < threshold: 7 \n",
      "    >>> Total reads in sample: 5238\n",
      "\t  >> Total reads kept at families level (reads > threshold): 5196\n",
      "\t  >> Total reads discarded (families reads < threshold): 26\n",
      "\t  >> Reads distributed: 10\n",
      "\t  >> Reads not distributed (eg. no families above threshold): 6\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTL.kraken2.bracken_F.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken F level analysis completed for D3WTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTL.kraken2.bracken_G.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l G -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of genuses in sample: 9 \n",
      "\t  >> Number of genuses with reads > threshold: 2 \n",
      "\t  >> Number of genuses with reads < threshold: 7 \n",
      "    >>> Total reads in sample: 5238\n",
      "\t  >> Total reads kept at genuses level (reads > threshold): 5196\n",
      "\t  >> Total reads discarded (genuses reads < threshold): 22\n",
      "\t  >> Reads distributed: 10\n",
      "\t  >> Reads not distributed (eg. no genuses above threshold): 10\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTL.kraken2.bracken_G.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken G level analysis completed for D3WTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTL.kraken2.bracken_S.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l S -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of species in sample: 11 \n",
      "\t  >> Number of species with reads > threshold: 3 \n",
      "\t  >> Number of species with reads < threshold: 8 \n",
      "    >>> Total reads in sample: 5238\n",
      "\t  >> Total reads kept at species level (reads > threshold): 3850\n",
      "\t  >> Total reads discarded (species reads < threshold): 20\n",
      "\t  >> Reads distributed: 1356\n",
      "\t  >> Reads not distributed (eg. no species above threshold): 12\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTL.kraken2.bracken_S.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken S level analysis completed for D3WTL.kraken2\n",
      "Processing Kraken report for Bracken: D3WTU.kraken2.report (Base name: D3WTU.kraken2)\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTU.kraken2.bracken_P.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l P -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of phylums in sample: 4 \n",
      "\t  >> Number of phylums with reads > threshold: 2 \n",
      "\t  >> Number of phylums with reads < threshold: 2 \n",
      "    >>> Total reads in sample: 7862\n",
      "\t  >> Total reads kept at phylums level (reads > threshold): 7854\n",
      "\t  >> Total reads discarded (phylums reads < threshold): 4\n",
      "\t  >> Reads distributed: 4\n",
      "\t  >> Reads not distributed (eg. no phylums above threshold): 0\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTU.kraken2.bracken_P.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken P level analysis completed for D3WTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTU.kraken2.bracken_C.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l C -t 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of classes in sample: 7 \n",
      "\t  >> Number of classes with reads > threshold: 2 \n",
      "\t  >> Number of classes with reads < threshold: 5 \n",
      "    >>> Total reads in sample: 7862\n",
      "\t  >> Total reads kept at classes level (reads > threshold): 7848\n",
      "\t  >> Total reads discarded (classes reads < threshold): 10\n",
      "\t  >> Reads distributed: 4\n",
      "\t  >> Reads not distributed (eg. no classes above threshold): 0\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTU.kraken2.bracken_C.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken C level analysis completed for D3WTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTU.kraken2.bracken_O.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l O -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of orders in sample: 9 \n",
      "\t  >> Number of orders with reads > threshold: 1 \n",
      "\t  >> Number of orders with reads < threshold: 8 \n",
      "    >>> Total reads in sample: 7862\n",
      "\t  >> Total reads kept at orders level (reads > threshold): 7834\n",
      "\t  >> Total reads discarded (orders reads < threshold): 22\n",
      "\t  >> Reads distributed: 0\n",
      "\t  >> Reads not distributed (eg. no orders above threshold): 6\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTU.kraken2.bracken_O.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken O level analysis completed for D3WTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTU.kraken2.bracken_F.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l F -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of families in sample: 9 \n",
      "\t  >> Number of families with reads > threshold: 1 \n",
      "\t  >> Number of families with reads < threshold: 8 \n",
      "    >>> Total reads in sample: 7862\n",
      "\t  >> Total reads kept at families level (reads > threshold): 7832\n",
      "\t  >> Total reads discarded (families reads < threshold): 20\n",
      "\t  >> Reads distributed: 0\n",
      "\t  >> Reads not distributed (eg. no families above threshold): 10\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTU.kraken2.bracken_F.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken F level analysis completed for D3WTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTU.kraken2.bracken_G.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l G -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:42:59\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of genuses in sample: 9 \n",
      "\t  >> Number of genuses with reads > threshold: 1 \n",
      "\t  >> Number of genuses with reads < threshold: 8 \n",
      "    >>> Total reads in sample: 7862\n",
      "\t  >> Total reads kept at genuses level (reads > threshold): 7832\n",
      "\t  >> Total reads discarded (genuses reads < threshold): 16\n",
      "\t  >> Reads distributed: 0\n",
      "\t  >> Reads not distributed (eg. no genuses above threshold): 14\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTU.kraken2.bracken_G.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:42:59\n",
      "  Bracken complete.\n",
      "Bracken G level analysis completed for D3WTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTU.kraken2.bracken_S.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l S -t 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D3WTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of species in sample: 8 \n",
      "\t  >> Number of species with reads > threshold: 2 \n",
      "\t  >> Number of species with reads < threshold: 6 \n",
      "    >>> Total reads in sample: 7862\n",
      "\t  >> Total reads kept at species level (reads > threshold): 5800\n",
      "\t  >> Total reads discarded (species reads < threshold): 12\n",
      "\t  >> Reads distributed: 2032\n",
      "\t  >> Reads not distributed (eg. no species above threshold): 18\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D3WTU.kraken2.bracken_S.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken S level analysis completed for D3WTU.kraken2\n",
      "Processing Kraken report for Bracken: D6NTL.kraken2.report (Base name: D6NTL.kraken2)\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTL.kraken2.bracken_P.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l P -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of phylums in sample: 11 \n",
      "\t  >> Number of phylums with reads > threshold: 8 \n",
      "\t  >> Number of phylums with reads < threshold: 3 \n",
      "    >>> Total reads in sample: 40964\n",
      "\t  >> Total reads kept at phylums level (reads > threshold): 40726\n",
      "\t  >> Total reads discarded (phylums reads < threshold): 10\n",
      "\t  >> Reads distributed: 226\n",
      "\t  >> Reads not distributed (eg. no phylums above threshold): 0\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTL.kraken2.bracken_P.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken P level analysis completed for D6NTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTL.kraken2.bracken_C.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l C -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of classes in sample: 21 \n",
      "\t  >> Number of classes with reads > threshold: 13 \n",
      "\t  >> Number of classes with reads < threshold: 8 \n",
      "    >>> Total reads in sample: 40964\n",
      "\t  >> Total reads kept at classes level (reads > threshold): 40546\n",
      "\t  >> Total reads discarded (classes reads < threshold): 24\n",
      "\t  >> Reads distributed: 390\n",
      "\t  >> Reads not distributed (eg. no classes above threshold): 2\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTL.kraken2.bracken_C.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken C level analysis completed for D6NTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTL.kraken2.bracken_O.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l O -t 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of orders in sample: 44 \n",
      "\t  >> Number of orders with reads > threshold: 24 \n",
      "\t  >> Number of orders with reads < threshold: 20 \n",
      "    >>> Total reads in sample: 40964\n",
      "\t  >> Total reads kept at orders level (reads > threshold): 40112\n",
      "\t  >> Total reads discarded (orders reads < threshold): 54\n",
      "\t  >> Reads distributed: 790\n",
      "\t  >> Reads not distributed (eg. no orders above threshold): 6\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTL.kraken2.bracken_O.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken O level analysis completed for D6NTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTL.kraken2.bracken_F.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l F -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of families in sample: 73 \n",
      "\t  >> Number of families with reads > threshold: 26 \n",
      "\t  >> Number of families with reads < threshold: 47 \n",
      "    >>> Total reads in sample: 40964\n",
      "\t  >> Total reads kept at families level (reads > threshold): 39344\n",
      "\t  >> Total reads discarded (families reads < threshold): 158\n",
      "\t  >> Reads distributed: 1416\n",
      "\t  >> Reads not distributed (eg. no families above threshold): 44\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTL.kraken2.bracken_F.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken F level analysis completed for D6NTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTL.kraken2.bracken_G.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l G -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of genuses in sample: 108 \n",
      "\t  >> Number of genuses with reads > threshold: 25 \n",
      "\t  >> Number of genuses with reads < threshold: 83 \n",
      "    >>> Total reads in sample: 40964\n",
      "\t  >> Total reads kept at genuses level (reads > threshold): 38516\n",
      "\t  >> Total reads discarded (genuses reads < threshold): 264\n",
      "\t  >> Reads distributed: 2082\n",
      "\t  >> Reads not distributed (eg. no genuses above threshold): 100\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTL.kraken2.bracken_G.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken G level analysis completed for D6NTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTL.kraken2.bracken_S.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l S -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of species in sample: 122 \n",
      "\t  >> Number of species with reads > threshold: 25 \n",
      "\t  >> Number of species with reads < threshold: 97 \n",
      "    >>> Total reads in sample: 40964\n",
      "\t  >> Total reads kept at species level (reads > threshold): 37340\n",
      "\t  >> Total reads discarded (species reads < threshold): 310\n",
      "\t  >> Reads distributed: 3010\n",
      "\t  >> Reads not distributed (eg. no species above threshold): 302\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTL.kraken2.bracken_S.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken S level analysis completed for D6NTL.kraken2\n",
      "Processing Kraken report for Bracken: D6NTU.kraken2.report (Base name: D6NTU.kraken2)\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTU.kraken2.bracken_P.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l P -t 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of phylums in sample: 14 \n",
      "\t  >> Number of phylums with reads > threshold: 6 \n",
      "\t  >> Number of phylums with reads < threshold: 8 \n",
      "    >>> Total reads in sample: 17298\n",
      "\t  >> Total reads kept at phylums level (reads > threshold): 17118\n",
      "\t  >> Total reads discarded (phylums reads < threshold): 26\n",
      "\t  >> Reads distributed: 152\n",
      "\t  >> Reads not distributed (eg. no phylums above threshold): 0\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTU.kraken2.bracken_P.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken P level analysis completed for D6NTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTU.kraken2.bracken_C.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l C -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of classes in sample: 24 \n",
      "\t  >> Number of classes with reads > threshold: 10 \n",
      "\t  >> Number of classes with reads < threshold: 14 \n",
      "    >>> Total reads in sample: 17298\n",
      "\t  >> Total reads kept at classes level (reads > threshold): 17034\n",
      "\t  >> Total reads discarded (classes reads < threshold): 50\n",
      "\t  >> Reads distributed: 212\n",
      "\t  >> Reads not distributed (eg. no classes above threshold): 0\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTU.kraken2.bracken_C.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken C level analysis completed for D6NTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTU.kraken2.bracken_O.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l O -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of orders in sample: 43 \n",
      "\t  >> Number of orders with reads > threshold: 20 \n",
      "\t  >> Number of orders with reads < threshold: 23 \n",
      "    >>> Total reads in sample: 17298\n",
      "\t  >> Total reads kept at orders level (reads > threshold): 16856\n",
      "\t  >> Total reads discarded (orders reads < threshold): 84\n",
      "\t  >> Reads distributed: 356\n",
      "\t  >> Reads not distributed (eg. no orders above threshold): 0\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTU.kraken2.bracken_O.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken O level analysis completed for D6NTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTU.kraken2.bracken_F.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l F -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of families in sample: 70 \n",
      "\t  >> Number of families with reads > threshold: 22 \n",
      "\t  >> Number of families with reads < threshold: 48 \n",
      "    >>> Total reads in sample: 17298\n",
      "\t  >> Total reads kept at families level (reads > threshold): 16400\n",
      "\t  >> Total reads discarded (families reads < threshold): 190\n",
      "\t  >> Reads distributed: 684\n",
      "\t  >> Reads not distributed (eg. no families above threshold): 22\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTU.kraken2.bracken_F.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken F level analysis completed for D6NTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTU.kraken2.bracken_G.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l G -t 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of genuses in sample: 106 \n",
      "\t  >> Number of genuses with reads > threshold: 21 \n",
      "\t  >> Number of genuses with reads < threshold: 85 \n",
      "    >>> Total reads in sample: 17298\n",
      "\t  >> Total reads kept at genuses level (reads > threshold): 15940\n",
      "\t  >> Total reads discarded (genuses reads < threshold): 270\n",
      "\t  >> Reads distributed: 982\n",
      "\t  >> Reads not distributed (eg. no genuses above threshold): 104\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTU.kraken2.bracken_G.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken G level analysis completed for D6NTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTU.kraken2.bracken_S.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l S -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6NTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of species in sample: 119 \n",
      "\t  >> Number of species with reads > threshold: 25 \n",
      "\t  >> Number of species with reads < threshold: 94 \n",
      "    >>> Total reads in sample: 17298\n",
      "\t  >> Total reads kept at species level (reads > threshold): 15148\n",
      "\t  >> Total reads discarded (species reads < threshold): 276\n",
      "\t  >> Reads distributed: 1400\n",
      "\t  >> Reads not distributed (eg. no species above threshold): 472\n",
      "\t  >> Unclassified reads: 2\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6NTU.kraken2.bracken_S.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken S level analysis completed for D6NTU.kraken2\n",
      "Processing Kraken report for Bracken: D6WTL.kraken2.report (Base name: D6WTL.kraken2)\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTL.kraken2.bracken_P.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l P -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of phylums in sample: 4 \n",
      "\t  >> Number of phylums with reads > threshold: 2 \n",
      "\t  >> Number of phylums with reads < threshold: 2 \n",
      "    >>> Total reads in sample: 11498\n",
      "\t  >> Total reads kept at phylums level (reads > threshold): 11480\n",
      "\t  >> Total reads discarded (phylums reads < threshold): 10\n",
      "\t  >> Reads distributed: 8\n",
      "\t  >> Reads not distributed (eg. no phylums above threshold): 0\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTL.kraken2.bracken_P.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken P level analysis completed for D6WTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTL.kraken2.bracken_C.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l C -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of classes in sample: 8 \n",
      "\t  >> Number of classes with reads > threshold: 3 \n",
      "\t  >> Number of classes with reads < threshold: 5 \n",
      "    >>> Total reads in sample: 11498\n",
      "\t  >> Total reads kept at classes level (reads > threshold): 11464\n",
      "\t  >> Total reads discarded (classes reads < threshold): 24\n",
      "\t  >> Reads distributed: 8\n",
      "\t  >> Reads not distributed (eg. no classes above threshold): 2\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTL.kraken2.bracken_C.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken C level analysis completed for D6WTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTL.kraken2.bracken_O.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l O -t 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM START TIME: 01-03-2025 05:43:00\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of orders in sample: 10 \n",
      "\t  >> Number of orders with reads > threshold: 4 \n",
      "\t  >> Number of orders with reads < threshold: 6 \n",
      "    >>> Total reads in sample: 11498\n",
      "\t  >> Total reads kept at orders level (reads > threshold): 11456\n",
      "\t  >> Total reads discarded (orders reads < threshold): 24\n",
      "\t  >> Reads distributed: 16\n",
      "\t  >> Reads not distributed (eg. no orders above threshold): 2\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTL.kraken2.bracken_O.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:00\n",
      "  Bracken complete.\n",
      "Bracken O level analysis completed for D6WTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTL.kraken2.bracken_F.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l F -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:01\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of families in sample: 13 \n",
      "\t  >> Number of families with reads > threshold: 5 \n",
      "\t  >> Number of families with reads < threshold: 8 \n",
      "    >>> Total reads in sample: 11498\n",
      "\t  >> Total reads kept at families level (reads > threshold): 11446\n",
      "\t  >> Total reads discarded (families reads < threshold): 32\n",
      "\t  >> Reads distributed: 18\n",
      "\t  >> Reads not distributed (eg. no families above threshold): 2\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTL.kraken2.bracken_F.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:01\n",
      "  Bracken complete.\n",
      "Bracken F level analysis completed for D6WTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTL.kraken2.bracken_G.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l G -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:01\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of genuses in sample: 16 \n",
      "\t  >> Number of genuses with reads > threshold: 5 \n",
      "\t  >> Number of genuses with reads < threshold: 11 \n",
      "    >>> Total reads in sample: 11498\n",
      "\t  >> Total reads kept at genuses level (reads > threshold): 11436\n",
      "\t  >> Total reads discarded (genuses reads < threshold): 36\n",
      "\t  >> Reads distributed: 20\n",
      "\t  >> Reads not distributed (eg. no genuses above threshold): 6\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTL.kraken2.bracken_G.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:01\n",
      "  Bracken complete.\n",
      "Bracken G level analysis completed for D6WTL.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTL.kraken2.bracken_S.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l S -t 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM START TIME: 01-03-2025 05:43:01\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTL.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of species in sample: 17 \n",
      "\t  >> Number of species with reads > threshold: 5 \n",
      "\t  >> Number of species with reads < threshold: 12 \n",
      "    >>> Total reads in sample: 11498\n",
      "\t  >> Total reads kept at species level (reads > threshold): 8812\n",
      "\t  >> Total reads discarded (species reads < threshold): 36\n",
      "\t  >> Reads distributed: 2620\n",
      "\t  >> Reads not distributed (eg. no species above threshold): 30\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTL.kraken2.bracken_S.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:01\n",
      "  Bracken complete.\n",
      "Bracken S level analysis completed for D6WTL.kraken2\n",
      "Processing Kraken report for Bracken: D6WTU.kraken2.report (Base name: D6WTU.kraken2)\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTU.kraken2.bracken_P.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l P -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:01\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of phylums in sample: 3 \n",
      "\t  >> Number of phylums with reads > threshold: 2 \n",
      "\t  >> Number of phylums with reads < threshold: 1 \n",
      "    >>> Total reads in sample: 1656\n",
      "\t  >> Total reads kept at phylums level (reads > threshold): 1654\n",
      "\t  >> Total reads discarded (phylums reads < threshold): 2\n",
      "\t  >> Reads distributed: 0\n",
      "\t  >> Reads not distributed (eg. no phylums above threshold): 0\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTU.kraken2.bracken_P.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:01\n",
      "  Bracken complete.\n",
      "Bracken P level analysis completed for D6WTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTU.kraken2.bracken_C.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l C -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:01\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of classes in sample: 4 \n",
      "\t  >> Number of classes with reads > threshold: 1 \n",
      "\t  >> Number of classes with reads < threshold: 3 \n",
      "    >>> Total reads in sample: 1656\n",
      "\t  >> Total reads kept at classes level (reads > threshold): 1644\n",
      "\t  >> Total reads discarded (classes reads < threshold): 12\n",
      "\t  >> Reads distributed: 0\n",
      "\t  >> Reads not distributed (eg. no classes above threshold): 0\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTU.kraken2.bracken_C.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:01\n",
      "  Bracken complete.\n",
      "Bracken C level analysis completed for D6WTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTU.kraken2.bracken_O.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l O -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:01\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of orders in sample: 5 \n",
      "\t  >> Number of orders with reads > threshold: 1 \n",
      "\t  >> Number of orders with reads < threshold: 4 \n",
      "    >>> Total reads in sample: 1656\n",
      "\t  >> Total reads kept at orders level (reads > threshold): 1644\n",
      "\t  >> Total reads discarded (orders reads < threshold): 12\n",
      "\t  >> Reads distributed: 0\n",
      "\t  >> Reads not distributed (eg. no orders above threshold): 0\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTU.kraken2.bracken_O.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:01\n",
      "  Bracken complete.\n",
      "Bracken O level analysis completed for D6WTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTU.kraken2.bracken_F.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l F -t 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM START TIME: 01-03-2025 05:43:01\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of families in sample: 6 \n",
      "\t  >> Number of families with reads > threshold: 1 \n",
      "\t  >> Number of families with reads < threshold: 5 \n",
      "    >>> Total reads in sample: 1656\n",
      "\t  >> Total reads kept at families level (reads > threshold): 1644\n",
      "\t  >> Total reads discarded (families reads < threshold): 12\n",
      "\t  >> Reads distributed: 0\n",
      "\t  >> Reads not distributed (eg. no families above threshold): 0\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTU.kraken2.bracken_F.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:01\n",
      "  Bracken complete.\n",
      "Bracken F level analysis completed for D6WTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTU.kraken2.bracken_G.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l G -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:01\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of genuses in sample: 6 \n",
      "\t  >> Number of genuses with reads > threshold: 1 \n",
      "\t  >> Number of genuses with reads < threshold: 5 \n",
      "    >>> Total reads in sample: 1656\n",
      "\t  >> Total reads kept at genuses level (reads > threshold): 1644\n",
      "\t  >> Total reads discarded (genuses reads < threshold): 12\n",
      "\t  >> Reads distributed: 0\n",
      "\t  >> Reads not distributed (eg. no genuses above threshold): 0\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTU.kraken2.bracken_G.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:01\n",
      "  Bracken complete.\n",
      "Bracken G level analysis completed for D6WTU.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTU.kraken2.bracken_S.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l S -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:01\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/D6WTU.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of species in sample: 6 \n",
      "\t  >> Number of species with reads > threshold: 2 \n",
      "\t  >> Number of species with reads < threshold: 4 \n",
      "    >>> Total reads in sample: 1656\n",
      "\t  >> Total reads kept at species level (reads > threshold): 1228\n",
      "\t  >> Total reads discarded (species reads < threshold): 10\n",
      "\t  >> Reads distributed: 416\n",
      "\t  >> Reads not distributed (eg. no species above threshold): 2\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/D6WTU.kraken2.bracken_S.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:01\n",
      "  Bracken complete.\n",
      "Bracken S level analysis completed for D6WTU.kraken2\n",
      "Processing Kraken report for Bracken: DoLower.kraken2.report (Base name: DoLower.kraken2)\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/DoLower.kraken2.bracken_P.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l P -t 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM START TIME: 01-03-2025 05:43:01\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of phylums in sample: 2 \n",
      "\t  >> Number of phylums with reads > threshold: 2 \n",
      "\t  >> Number of phylums with reads < threshold: 0 \n",
      "    >>> Total reads in sample: 8140\n",
      "\t  >> Total reads kept at phylums level (reads > threshold): 8126\n",
      "\t  >> Total reads discarded (phylums reads < threshold): 0\n",
      "\t  >> Reads distributed: 14\n",
      "\t  >> Reads not distributed (eg. no phylums above threshold): 0\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/DoLower.kraken2.bracken_P.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:01\n",
      "  Bracken complete.\n",
      "Bracken P level analysis completed for DoLower.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/DoLower.kraken2.bracken_C.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l C -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:01\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of classes in sample: 6 \n",
      "\t  >> Number of classes with reads > threshold: 4 \n",
      "\t  >> Number of classes with reads < threshold: 2 \n",
      "    >>> Total reads in sample: 8140\n",
      "\t  >> Total reads kept at classes level (reads > threshold): 8118\n",
      "\t  >> Total reads discarded (classes reads < threshold): 6\n",
      "\t  >> Reads distributed: 16\n",
      "\t  >> Reads not distributed (eg. no classes above threshold): 0\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/DoLower.kraken2.bracken_C.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:01\n",
      "  Bracken complete.\n",
      "Bracken C level analysis completed for DoLower.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/DoLower.kraken2.bracken_O.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l O -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:01\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of orders in sample: 17 \n",
      "\t  >> Number of orders with reads > threshold: 8 \n",
      "\t  >> Number of orders with reads < threshold: 9 \n",
      "    >>> Total reads in sample: 8140\n",
      "\t  >> Total reads kept at orders level (reads > threshold): 8094\n",
      "\t  >> Total reads discarded (orders reads < threshold): 28\n",
      "\t  >> Reads distributed: 18\n",
      "\t  >> Reads not distributed (eg. no orders above threshold): 0\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/DoLower.kraken2.bracken_O.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:01\n",
      "  Bracken complete.\n",
      "Bracken O level analysis completed for DoLower.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/DoLower.kraken2.bracken_F.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l F -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:01\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of families in sample: 23 \n",
      "\t  >> Number of families with reads > threshold: 7 \n",
      "\t  >> Number of families with reads < threshold: 16 \n",
      "    >>> Total reads in sample: 8140\n",
      "\t  >> Total reads kept at families level (reads > threshold): 8054\n",
      "\t  >> Total reads discarded (families reads < threshold): 56\n",
      "\t  >> Reads distributed: 16\n",
      "\t  >> Reads not distributed (eg. no families above threshold): 14\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/DoLower.kraken2.bracken_F.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:01\n",
      "  Bracken complete.\n",
      "Bracken F level analysis completed for DoLower.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/DoLower.kraken2.bracken_G.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l G -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:01\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of genuses in sample: 25 \n",
      "\t  >> Number of genuses with reads > threshold: 4 \n",
      "\t  >> Number of genuses with reads < threshold: 21 \n",
      "    >>> Total reads in sample: 8140\n",
      "\t  >> Total reads kept at genuses level (reads > threshold): 7976\n",
      "\t  >> Total reads discarded (genuses reads < threshold): 76\n",
      "\t  >> Reads distributed: 16\n",
      "\t  >> Reads not distributed (eg. no genuses above threshold): 72\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/DoLower.kraken2.bracken_G.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:01\n",
      "  Bracken complete.\n",
      "Bracken G level analysis completed for DoLower.kraken2\n",
      " >> Checking for Valid Options...\n",
      " >> Running Bracken \n",
      "      >> python3 /opt/Bracken/src/est_abundance.py -i /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report -o /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/DoLower.kraken2.bracken_S.tsv -k /mnt/localdatabase/k2_unite-allEUK/database1000mers.kmer_distrib -l S -t 10\n",
      "PROGRAM START TIME: 01-03-2025 05:43:01\n",
      "BRACKEN SUMMARY (Kraken report: /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report)\n",
      "    >>> Threshold: 10 \n",
      "    >>> Number of species in sample: 27 \n",
      "\t  >> Number of species with reads > threshold: 4 \n",
      "\t  >> Number of species with reads < threshold: 23 \n",
      "    >>> Total reads in sample: 8140\n",
      "\t  >> Total reads kept at species level (reads > threshold): 6084\n",
      "\t  >> Total reads discarded (species reads < threshold): 80\n",
      "\t  >> Reads distributed: 1892\n",
      "\t  >> Reads not distributed (eg. no species above threshold): 84\n",
      "\t  >> Unclassified reads: 0\n",
      "BRACKEN OUTPUT PRODUCED: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken/DoLower.kraken2.bracken_S.tsv\n",
      "PROGRAM END TIME: 01-03-2025 05:43:01\n",
      "  Bracken complete.\n",
      "Bracken S level analysis completed for DoLower.kraken2\n",
      "All Bracken analysis completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report\n",
      ">> Checking report file: /mnt/Data/2024-000012/Termites_soil/kraken2_output/DoLower.kraken2.report\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import subprocess\n",
    "\n",
    "\n",
    "''' ===== Configuration ===== '''          \n",
    "project_id = \"2024-000012\"\n",
    "taxon_ranks = ['P', 'C', 'O', 'F', 'G', 'S']\n",
    "read_len = \"1000\" #Adjust this depends the type of genes (either 16S rDNA: 1600 or ITS: 1000)as\n",
    "num_workers = 1  # Adjust this number based on your system's capabilities\n",
    "\n",
    "''' File path settings '''\n",
    "base_path =  f\"/mnt/Data/{project_id}\"\n",
    "kraken_output_dir = os.path.join(base_path, \"Termites_soil/kraken2_output\")\n",
    "output_dir = os.path.join(base_path, \"Termites_soil/kraken2_output/bracken\")\n",
    "bracken_db = \"/mnt/localdatabase/k2_unite-allEUK/\"\n",
    "\n",
    "\n",
    "''' ===== Functions ===== ''' \n",
    "def bracken_exists(base_name):\n",
    "    missing_levels = []\n",
    "    for level in taxon_ranks:\n",
    "        output_file = os.path.join(output_dir, f\"{base_name}.bracken_{level}.tsv\")\n",
    "        if not os.path.exists(output_file):\n",
    "            missing_levels.append(level)\n",
    "    return missing_levels\n",
    "\n",
    "def process_kraken_report_for_bracken(kraken_report_file):\n",
    "    base_name = os.path.splitext(kraken_report_file)[0]\n",
    "    print(f\"Processing Kraken report for Bracken: {kraken_report_file} (Base name: {base_name})\")\n",
    "\n",
    "    # Skip processing if already processed\n",
    "    if not bracken_exists(base_name):\n",
    "        print(f\"Skipping {base_name}, all Bracken analyses already completed.\")\n",
    "        return\n",
    "\n",
    "    # Bracken analysis for different taxonomic levels\n",
    "    for level in bracken_exists(base_name):  # Genus, Family, Species\n",
    "        bracken_cmd = [\n",
    "            \"bracken\", \"-d\", bracken_db,\n",
    "            \"-i\", os.path.join(kraken_output_dir, kraken_report_file),\n",
    "            \"-o\", os.path.join(output_dir, f\"{base_name}.bracken_{level}.tsv\"),\n",
    "            \"-r\", read_len, \"-l\", level\n",
    "        ]\n",
    "        try:\n",
    "            subprocess.run(bracken_cmd, check=True)\n",
    "            print(f\"Bracken {level} level analysis completed for {base_name}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error during Bracken {level} analysis for {base_name}: {e}\")\n",
    "\n",
    "\n",
    "''' ===== Workflow ===== '''\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if there are Kraken2 reports in the directory\n",
    "    kraken_reports = [f for f in os.listdir(kraken_output_dir) if f.endswith(\".kraken2.report\")]\n",
    "    \n",
    "    if not kraken_reports:\n",
    "        print(f\"No Kraken2 reports found in {kraken_output_dir}.\")\n",
    "    else:\n",
    "        print(f\"Found {len(kraken_reports)} Kraken2 reports. Processing Bracken...\")\n",
    "\n",
    "    # Using ThreadPoolExecutor with a specified number of workers\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        executor.map(process_kraken_report_for_bracken, kraken_reports)\n",
    "\n",
    "    print(\"All Bracken analysis completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b8ddf-f809-4d95-86f1-d0ac367905ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103188cb-f1cc-4bf5-b933-e3fbb38993cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''backup'''\n",
    "import os\n",
    "import concurrent.futures\n",
    "import subprocess\n",
    "\n",
    "\n",
    "''' ===== Configuration ===== '''\n",
    "project_id = \"2024-000012\"\n",
    "taxon_ranks = ['P', 'C', 'O', 'F', 'G', 'S'] # Add taxonomic ranks that you need for analysis\n",
    "num_workers = 1  # Adjust this number based on your system's capabilities\n",
    "\n",
    "''' File path settings '''\n",
    "base_path =  f\"/mnt/Data/{project_id}\"  # Base directory containing the csv files\n",
    "input_dir = os.path.join(base_path, \"7_kraken_filtered\")\n",
    "output_dir = os.path.join(base_path, \"8_bracken\")\n",
    "bracken_db = \"/mnt/localdatabase/k2_refmicrobiome/\"\n",
    "\n",
    "\n",
    "''' ===== Functions ===== ''' \n",
    "def bracken_exists(base_name):\n",
    "    output_path = Path(output_dir)\n",
    "    pattern = f\"{base_name}.bracken_*.tsv\"\n",
    "    return any(base_path.glob(pattern))\n",
    "    \n",
    "def process_kraken_report_for_bracken(kraken_report_file):\n",
    "    base_name = os.path.splitext(kraken_report_file)[0]\n",
    "    print(f\"Processing Kraken report for Bracken: {kraken_report_file} (Base name: {base_name})\")\n",
    "\n",
    "    existing_levels = set(\n",
    "        file.stem.split(f\"{base_name}.bracken_\")[-1]\n",
    "        for file in Path(output_dir).glob(f\"{base_name}.bracken_*.tsv\")\n",
    "    )\n",
    "\n",
    "    if not existing_levels:\n",
    "        print(f\"No existing Bracken levels detected for {base_name}. Skipping analysis.\")\n",
    "        return\n",
    "    print(f\"Detected Bracken levels for {base_name}: {', '.join(existing_levels)}.\")\n",
    "\n",
    "    for level in existing_levels:\n",
    "        bracken_cmd = [\n",
    "            \"bracken\", \"-d\", bracken_db,\n",
    "            \"-i\", os.path.join(input_dir, kraken_report_file),\n",
    "            \"-o\", os.path.join(output_dir, f\"{base_name}.bracken_{level}.tsv\"),\n",
    "            \"-r\", \"300\", \"-l\", level\n",
    "        ]\n",
    "        try:\n",
    "            subprocess.run(bracken_cmd, check=True)\n",
    "            print(f\"Bracken {level} level analysis completed for {base_name}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error during Bracken {level} analysis for {base_name}: {e}\")\n",
    "            \n",
    "\n",
    "''' ===== Workflow ===== '''\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    kraken_reports = [f for f in os.listdir(input_dir) if f.endswith(\".kraken2.report\")]\n",
    "\n",
    "    if not kraken_reports:\n",
    "        print(f\"No Kraken2 reports found in {input_dir}.\")\n",
    "    else:\n",
    "        print(f\"Found {len(kraken_reports)} Kraken2 reports. Processing Bracken...\")\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        executor.map(process_kraken_report_for_bracken, kraken_reports)\n",
    "\n",
    "    print(\"All Bracken analysis completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00241cf1-5313-4f97-822a-a065eb031b94",
   "metadata": {},
   "source": [
    "# 3-4. Bracken merge\n",
    "##### merge results of the samples for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc7302f-100c-40e8-9cc1-cac235113687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input folder: /mnt/Data/2024-000012/Termites_soil/kraken2_output/bracken\n",
      "Output folder: /mnt/Data/2024-000012/Termites_soil/kraken2_output/mergedBracken\n",
      "Folder identifier: bracken\n",
      "Detected taxonomic levels: {'G', 'F', 'P', 'C', 'S', 'O'}\n",
      "Processing taxonomic level: G\n",
      "Combined file for level G saved to: /mnt/Data/2024-000012/Termites_soil/kraken2_output/mergedBracken/combined_bracken_G.tsv\n",
      "Processing taxonomic level: F\n",
      "Combined file for level F saved to: /mnt/Data/2024-000012/Termites_soil/kraken2_output/mergedBracken/combined_bracken_F.tsv\n",
      "Processing taxonomic level: P\n",
      "Combined file for level P saved to: /mnt/Data/2024-000012/Termites_soil/kraken2_output/mergedBracken/combined_bracken_P.tsv\n",
      "Processing taxonomic level: C\n",
      "Combined file for level C saved to: /mnt/Data/2024-000012/Termites_soil/kraken2_output/mergedBracken/combined_bracken_C.tsv\n",
      "Processing taxonomic level: S\n",
      "Combined file for level S saved to: /mnt/Data/2024-000012/Termites_soil/kraken2_output/mergedBracken/combined_bracken_S.tsv\n",
      "Processing taxonomic level: O\n",
      "Combined file for level O saved to: /mnt/Data/2024-000012/Termites_soil/kraken2_output/mergedBracken/combined_bracken_O.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "''' ===== Configuration ===== '''\n",
    "project_id = \"2024-000012\"\n",
    "\n",
    "''' File path settings '''\n",
    "base_path = f\"/mnt/Data/{project_id}\"  # Base directory containing the csv files\n",
    "input_dir = os.path.join(base_path, \"Termites_soil/kraken2_output/bracken\")\n",
    "output_dir = os.path.join(base_path, \"Termites_soil/kraken2_output/mergedBracken\")\n",
    "\n",
    "\n",
    "''' ===== Functions ===== '''\n",
    "def combine_taxonomic_levels(input_dir, output_dir):\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Extract identifier from input folder (e.g., 'D2' or 'D1')\n",
    "    folder_identifier = input_path.stem.split('_')[-1]  # Extracts 'D2' or 'D1'\n",
    "\n",
    "    # Debugging: Print the absolute path and identifier to check if it's correct\n",
    "    print(f\"Input folder: {input_path.absolute()}\")\n",
    "    print(f\"Output folder: {output_path.absolute()}\")\n",
    "    print(f\"Folder identifier: {folder_identifier}\")\n",
    "\n",
    "    # Attempt to find all .tsv files that match the naming convention\n",
    "    files = list(input_path.glob('*bracken_*.tsv'))\n",
    "\n",
    "    if not files:\n",
    "        print(\"No files found. Check your input folder path and file naming convention.\")\n",
    "        return\n",
    "\n",
    "    # Dynamically detect levels from filenames\n",
    "    detected_levels = {file.stem.split('_')[-1] for file in files}  # Extract levels like 'S', 'G', 'F'\n",
    "    print(f\"Detected taxonomic levels: {detected_levels}\")\n",
    "\n",
    "    # Combine files for each taxonomic level\n",
    "    for level in detected_levels:\n",
    "        print(f\"Processing taxonomic level: {level}\")\n",
    "        file_paths = [file for file in files if file.stem.endswith(f\"_{level}\")]\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "        for file_path in file_paths:\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "            sample_name = file_path.stem.split('.')[0]  # Extracts sample name, e.g., '5985'\n",
    "            \n",
    "            # Check if 'name' exists in the file\n",
    "            if 'name' not in df.columns:\n",
    "                print(f\"Warning: 'name' column missing in file {file_path}\")\n",
    "                continue\n",
    "\n",
    "            # Create the Max column with NaN initially, place it in the fourth column position\n",
    "            df = df[['name', 'taxonomy_lvl', 'taxonomy_id', 'new_est_reads']].rename(columns={\n",
    "                'taxonomy_lvl': 'taxRank',\n",
    "                'taxonomy_id': 'taxID',\n",
    "                'new_est_reads': sample_name\n",
    "            })\n",
    "            \n",
    "            # Add 'Max' column with NaN\n",
    "            df.insert(3, 'Max', pd.NA)\n",
    "\n",
    "            if combined_df.empty:\n",
    "                combined_df = df\n",
    "            else:\n",
    "                combined_df = pd.merge(combined_df, df, on=['name', 'taxRank', 'taxID', 'Max'], how='outer')\n",
    "\n",
    "        if not combined_df.empty:\n",
    "            # Fill NA values for the sample columns with 0\n",
    "            for col in combined_df.columns:\n",
    "                if col not in ['name', 'taxRank', 'taxID', 'Max']:\n",
    "                    combined_df[col] = combined_df[col].fillna(0)\n",
    "\n",
    "            # Convert sample columns to numeric after merging\n",
    "            sample_cols = combined_df.columns[4:]  # Get all sample columns after 'Max'\n",
    "            combined_df[sample_cols] = combined_df[sample_cols].astype(float)\n",
    "\n",
    "            # Recalculate the 'Max' column across all sample columns\n",
    "            combined_df['Max'] = combined_df[sample_cols].max(axis=1)\n",
    "\n",
    "            # Sort the DataFrame by 'name' in alphabetical order\n",
    "            combined_df = combined_df.sort_values(by='name')\n",
    "\n",
    "            # Save the combined DataFrame\n",
    "            output_file_path = output_path / f'combined_{folder_identifier}_{level}.tsv'\n",
    "            combined_df.to_csv(output_file_path, index=False, sep='\\t')\n",
    "            print(f\"Combined file for level {level} saved to: {output_file_path}\")\n",
    "        else:\n",
    "            print(f\"No valid data for level {level}\")\n",
    "\n",
    "''' ===== Workflow ===== '''\n",
    "combine_taxonomic_levels(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c1d73-115f-44ba-a3f9-efc871ba7f1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. Microbiome Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5305e39-3fb9-4632-bd6c-3c1e1ffbca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.manifold import MDS\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import datetime\n",
    "\n",
    "########### Configuration ###########\n",
    "\n",
    "# Read the data file\n",
    "file_path = '/mnt/Data/Taiwan_soil_microbiome/seminar'\n",
    "data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "pdf_name = '/mnt/Data/Taiwan_soil_microbiome/seminar/results/Taiwan_Soil_Microbiome_16S.pdf'\n",
    "svg_folder = '/mnt/Data/Taiwan_soil_microbiome/seminar/results/svg_files'  # Update this with your desired directory\n",
    "output_choice = 'SVG' # Set the output choice (default as 'SVG')\n",
    "\n",
    "\"\"\"Calculates diversity indices: Richness, Shannon, and Simpson.\"\"\"\n",
    "def calculate_diversity_indices(data):\n",
    "    richness = data.apply(lambda x: (x > 0).sum(), axis=0)\n",
    "    shannon = data.apply(lambda x: entropy(x[x > 0]), axis=0)\n",
    "    simpson = data.apply(lambda x: 1 - sum((i/sum(x))**2 for i in x[x > 0]), axis=0)\n",
    "    return pd.DataFrame({'Richness': richness, 'Shannon': shannon, 'Simpson': simpson})\n",
    "\n",
    "\"\"\"Plots bar charts of the three alpha diversity indices.\"\"\"\n",
    "def plot_alpha_diversity(alpha_diversity):\n",
    "    colors = sns.color_palette('viridis', n_colors=3)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    for ax, (index, color) in zip(axes, zip(alpha_diversity.columns, colors)):\n",
    "        alpha_diversity[index].plot(kind='bar', ax=ax, color=color)\n",
    "        ax.set_title(f'{index} (Alpha Diversity)')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.set_xlabel('Soil Samples')\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    plt.close(fig)\n",
    "    print('Alpha diversity... Done...')\n",
    "    return fig\n",
    "\n",
    "def plot_heatmaps(samples_data, normalized_data, data):\n",
    "    fig1 = plt.figure(figsize=(10, 8))\n",
    "    bray_curtis_dissimilarity = squareform(pdist(samples_data.T, metric='braycurtis'))\n",
    "    \n",
    "    sns.heatmap(bray_curtis_dissimilarity, cmap=\"YlGnBu\", \n",
    "                xticklabels=samples_data.columns, \n",
    "                yticklabels=samples_data.columns)\n",
    "    plt.title('Bray-Curtis Dissimilarity Between Soil Samples')\n",
    "    plt.xlabel('Sample ID')\n",
    "    plt.ylabel('Sample ID')\n",
    "    plt.close(fig1)\n",
    "    print('Bray-Curtis Dissimilarity... Done...')\n",
    "\n",
    "    fig2 = plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(normalized_data.set_index(data['name']), cmap=\"YlGnBu\")\n",
    "    plt.title('Microbial Abundance Across Different Soil Samples')\n",
    "    plt.close(fig2)\n",
    "    print('Heatmap... Done...')\n",
    "    \n",
    "    return fig1, fig2\n",
    "\n",
    "def plot_pcoa(samples_data):\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    bray_curtis_dissimilarity = pdist(samples_data.T, metric='braycurtis')\n",
    "    mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "    pcoa_result = mds.fit_transform(squareform(bray_curtis_dissimilarity))\n",
    "    pcoa_df_temp = pd.DataFrame(data=pcoa_result, columns=['PCoA 1', 'PCoA 2'])\n",
    "    pcoa_df_temp['Soil Sample'] = samples_data.T.index\n",
    "\n",
    "    pcoa_df_temp['Treatment'] = pcoa_df_temp['Soil Sample'].apply(lambda x: x.split('_')[0] if '_' in x else x)\n",
    "    pcoa_df_temp['Replicate'] = pcoa_df_temp['Soil Sample'].apply(lambda x: x.split('_')[1] if '_' in x else 'NA')\n",
    "\n",
    "    ax = sns.scatterplot(x='PCoA 1', y='PCoA 2', hue='Replicate', style='Treatment',\n",
    "                         data=pcoa_df_temp, s=200, palette='viridis')\n",
    "\n",
    "    for line in range(0, pcoa_df_temp.shape[0]):\n",
    "         ax.text(pcoa_df_temp[\"PCoA 1\"][line]+0.02, pcoa_df_temp[\"PCoA 2\"][line],\n",
    "                 pcoa_df_temp[\"Soil Sample\"][line], horizontalalignment='left',\n",
    "                 size='medium', color='black', weight='semibold')\n",
    "\n",
    "    plt.title('PCoA of Microbial Composition of Soil Samples', fontsize=16)\n",
    "    plt.close(fig)\n",
    "    print('PCoA... Done...')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def save_plots(output_choice=output_choice, svg_folder=svg_folder):\n",
    "    # Calculate alpha diversity indices\n",
    "    samples_data = data.drop(columns=['name', 'taxRank', 'taxID', 'Max'])\n",
    "    normalized_data = samples_data.div(samples_data.sum(axis=1), axis=0)\n",
    "    alpha_diversity = calculate_diversity_indices(normalized_data)\n",
    "    print(alpha_diversity)\n",
    "\n",
    "    # Generate plots\n",
    "    fig_alpha = plot_alpha_diversity(alpha_diversity)\n",
    "    fig_heatmap1, fig_heatmap2 = plot_heatmaps(samples_data, normalized_data, data)\n",
    "    fig_pcoa = plot_pcoa(samples_data)\n",
    "\n",
    "    if output_choice == 'PDF':\n",
    "        # Save all figures to a single PDF\n",
    "        with PdfPages(pdf_name) as pdf:\n",
    "            # Add alpha diversity table\n",
    "            fig, ax = plt.subplots(figsize=(12, 4))\n",
    "            ax.axis('off')\n",
    "            table = ax.table(cellText=alpha_diversity.values, colLabels=alpha_diversity.columns, rowLabels=alpha_diversity.index, cellLoc='center', loc='center')\n",
    "            table.auto_set_font_size(False)\n",
    "            table.set_fontsize(10)\n",
    "            table.scale(1.2, 1.2)\n",
    "            plt.title('Alpha Diversity Indices Table', fontsize=16)\n",
    "            pdf.savefig(fig, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            pdf.savefig(fig_alpha, bbox_inches='tight')\n",
    "            pdf.savefig(fig_heatmap1, bbox_inches='tight')\n",
    "            pdf.savefig(fig_heatmap2, bbox_inches='tight')\n",
    "            pdf.savefig(fig_pcoa, bbox_inches='tight')\n",
    "\n",
    "            # PDF Metadata\n",
    "            d = pdf.infodict()\n",
    "            d['Title'] = 'Soil Microbiome Analysis Results'\n",
    "            d['Author'] = 'Your Name'\n",
    "            d['Subject'] = 'Microbiome Analysis'\n",
    "            d['Keywords'] = 'PCoA, Alpha Diversity, Beta Diversity, Microbiome'\n",
    "            d['CreationDate'] = datetime.datetime.today()\n",
    "            \n",
    "            print(\"PDF saved successfully.\")\n",
    "    \n",
    "    elif output_choice == 'SVG':\n",
    "        # Save each figure as a separate SVG file\n",
    "        fig_alpha.savefig(f'{svg_folder}alpha_diversity.svg', format='svg')\n",
    "        fig_heatmap1.savefig(f'{svg_folder}heatmap_braycurtis.svg', format='svg')\n",
    "        fig_heatmap2.savefig(f'{svg_folder}heatmap_abundance.svg', format='svg')\n",
    "        fig_pcoa.savefig(f'{svg_folder}pcoa_plot.svg', format='svg')\n",
    "\n",
    "        print(\"SVG files saved successfully.\")\n",
    "\n",
    "# Save all figures to a single PDF file\n",
    "with PdfPages(pdf_name) as pdf:\n",
    "    # Adding diversity indices table to the pdf (publishable format)\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=alpha_diversity.values, colLabels=alpha_diversity.columns, rowLabels=alpha_diversity.index, cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.2)  # Adjust table size\n",
    "    plt.title('Alpha Diversity Indices Table', fontsize=16)\n",
    "    pdf.savefig(fig, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    pdf.savefig(fig_alpha, bbox_inches='tight')\n",
    "    pdf.savefig(fig_heatmap1, bbox_inches='tight')\n",
    "    pdf.savefig(fig_heatmap2, bbox_inches='tight')\n",
    "    pdf.savefig(fig_pcoa, bbox_inches='tight')\n",
    "\n",
    "    # Metadata for the PDF\n",
    "    d = pdf.infodict()\n",
    "    d['Title'] = 'Soil Microbiome Analysis Results'\n",
    "    d['Author'] = 'Your Name'\n",
    "    d['Subject'] = 'Microbiome Analysis'\n",
    "    d['Keywords'] = 'PCoA, Alpha Diversity, Beta Diversity, Microbiome'\n",
    "    d['CreationDate'] = datetime.datetime.today()\n",
    "\n",
    "# Save the plots based on the output choice\n",
    "save_plots(output_choice)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rpy2)",
   "language": "python",
   "name": "python3_rpy2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
