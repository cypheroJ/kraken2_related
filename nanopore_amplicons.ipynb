{"cells":[{"cell_type":"markdown","id":"4563fa16-3ee5-4e25-b470-32d88b7e9223","metadata":{"id":"4563fa16-3ee5-4e25-b470-32d88b7e9223"},"source":["# Nanopore amplicon processing & annotation"]},{"cell_type":"markdown","id":"0b9a67b1-caa0-4cee-9a58-fba222879115","metadata":{"id":"0b9a67b1-caa0-4cee-9a58-fba222879115"},"source":["# 1. dorado Basecalling"]},{"cell_type":"markdown","id":"b53759f3-21b9-4543-b6b3-488cd9d47545","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"b53759f3-21b9-4543-b6b3-488cd9d47545"},"source":["#### Download Dorado basecaller\n","- if necessary"]},{"cell_type":"code","execution_count":null,"id":"6c42f6d5-92bd-43cb-90f7-7fa9a2e1259d","metadata":{"id":"6c42f6d5-92bd-43cb-90f7-7fa9a2e1259d"},"outputs":[],"source":["!wget https://cdn.oxfordnanoportal.com/software/analysis/dorado-0.8.1-linux-x64.tar.gz -O /mnt/NanoporeRawData/dorado.tar.gz\n","!tar -xvf /mnt/NanoporeRawData/dorado.tar.gz"]},{"cell_type":"markdown","id":"bca14b77-2860-44a2-8d8c-3053e2b44933","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"bca14b77-2860-44a2-8d8c-3053e2b44933"},"source":["#### Fast5 Files Conversion\n","- if necessary"]},{"cell_type":"code","execution_count":null,"id":"db204a46-7fac-4420-b393-902118431096","metadata":{"id":"db204a46-7fac-4420-b393-902118431096"},"outputs":[],"source":["import os\n","import subprocess\n","\n","# Define paths\n","raw_data_path = \"/mnt/NanoporeRawData/\"\n","fast5_path = os.path.join(raw_data_path, \"fast5\")\n","pod5_path = os.path.join(raw_data_path, \"pod5\")\n","\n","# Function to check if the number of files match and none are empty\n","def check_pod5_files(fast5_path, pod5_path):\n","    fast5_files = [f for f in os.listdir(fast5_path) if f.endswith('.fast5')]\n","    pod5_files = [f for f in os.listdir(pod5_path) if f.endswith('.pod5')]\n","\n","    # Check if counts match and ensure no empty files\n","    if len(fast5_files) != len(pod5_files):\n","        return False\n","\n","    for pod5_file in pod5_files:\n","        if os.path.getsize(os.path.join(pod5_path, pod5_file)) == 0:  # Check if the file is empty\n","            return False\n","\n","    return True\n","\n","# Check if only the fast5 directory exists, and convert FAST5 to POD5\n","if os.path.exists(fast5_path) and not os.path.exists(pod5_path):\n","    print(\"FAST5 directory detected, converting to POD5...\")\n","\n","    # Install pod5 package if not already installed\n","    subprocess.run([\"pip\", \"install\", \"pod5\"], check=True)\n","\n","    # Ensure the pod5 directory exists\n","    os.makedirs(pod5_path, exist_ok=True)\n","\n","    # Convert FAST5 to POD5, using -o for output and -O for one-to-one mapping\n","    subprocess.run([\n","        \"pod5\", \"convert\", \"fast5\",\n","        \"-o\", pod5_path,  # Output directory for POD5 files\n","        \"-O\", fast5_path,  # Parent directory for input files\n","        fast5_path  # Input path\n","    ], check=True)\n","\n","# Rerun conversion if checks fail\n","if not check_pod5_files(fast5_path, pod5_path):\n","    print(\"File count mismatch or empty POD5 files detected. Re-running conversion...\")\n","    subprocess.run([\n","        \"pod5\", \"convert\", \"fast5\",\n","        \"-o\", pod5_path,\n","        \"-O\", fast5_path,\n","        fast5_path\n","    ], check=True)\n","\n","print(\"POD5 conversion completed.\")"]},{"cell_type":"markdown","id":"230e4bff-54ac-4272-a776-6b6768a8db5f","metadata":{"id":"230e4bff-54ac-4272-a776-6b6768a8db5f"},"source":["## 1-1. Pod5 Files Basecalling\n","- basecall\n","- convert BAM to fastq\n","- generate NanoPlot"]},{"cell_type":"code","execution_count":null,"id":"03796505-4a57-4c3a-9396-c022c2e1f4b9","metadata":{"id":"03796505-4a57-4c3a-9396-c022c2e1f4b9"},"outputs":[],"source":["import os\n","import subprocess\n","\n","# Define paths\n","raw_data_path = \"/mnt/NanoporeRawData/pod5\"\n","basecall_data_path = \"/mnt/Data/1_dorado\"\n","\n","# Ensure the output directory exists\n","os.makedirs(basecall_data_path, exist_ok=True)\n","\n","# Run dorado basecaller and write BAM output directly\n","print(\"Running dorado basecaller...\")\n","bam_file_path = os.path.join(basecall_data_path, \"all.bam\")\n","with open(bam_file_path, \"w\") as bam_file:\n","    subprocess.run([\n","        \"dorado\", \"basecaller\", \"sup\", raw_data_path\n","    ], stdout=bam_file, check=True)\n","\n","# Convert BAM to FASTQ using bedtools\n","print(\"Converting BAM to FASTQ...\")\n","subprocess.run([\n","    \"bedtools\", \"bamtofastq\",\n","    \"-i\", bam_file_path,\n","    \"-fq\", os.path.join(basecall_data_path, \"all.fastq\")\n","], check=True)\n","\n","# Generate NanoPlot\n","print(\"Generating NanoPlot...\")\n","subprocess.run([\n","    \"NanoPlot\",\n","    \"--fastq\", os.path.join(basecall_data_path, \"all.fastq\"),\n","    \"-o\", basecall_data_path\n","], check=True)\n","\n","print(\"Dorado basecalling and analysis completed.\")\n"]},{"cell_type":"markdown","id":"8f958b9f-007d-41fa-8d81-06454c037817","metadata":{"id":"8f958b9f-007d-41fa-8d81-06454c037817"},"source":["# 2. NanoACT Demultiplexing & Processing"]},{"cell_type":"markdown","id":"b23a65f7-6bc9-4dc7-9e83-0e5fc7c6005f","metadata":{"id":"b23a65f7-6bc9-4dc7-9e83-0e5fc7c6005f"},"source":["##### Rename SampleID\n","- if only 1 gene fragment"]},{"cell_type":"code","execution_count":null,"id":"8861667d-11d7-4c6a-8e88-ad0b95bcb177","metadata":{"id":"8861667d-11d7-4c6a-8e88-ad0b95bcb177"},"outputs":[],"source":["import os\n","import csv\n","\n","\n","''' ===== Configuration ===== '''\n","project_name = \"\"  # Set this to your project name\n","sample_type = \"DNA samples\"  # Specify the sample type for filtering\n","column_for_replace = \"Sample number\" #or Name or Morph description\n","replace_name = True  # Set True to replace SampleID with Reference ID from reference.csv\n","\n","''' File path settings '''\n","base_dir = f\"/mnt/Data/{project_name}\"  # Base directory containing the csv files\n","barcode_ID_file = os.path.join(base_dir, f\"{project_name}.csv\")\n","reference_file = os.path.join(base_dir, \"reference.csv\")\n","output_file = os.path.join(base_dir, f\"{project_name}_modf.csv\")\n","\n","\n","''' ===== Workflow ===== '''\n","# Initialize containers for data\n","barcode_IDs = []\n","reference_data = {}\n","\n","# Load reference data if the file exists\n","if replace_name and os.path.exists(reference_file):\n","    # Read the reference file and extract relevant columns\n","    with open(reference_file, 'r') as ref_file:\n","        reader = csv.DictReader(ref_file)  # Assuming CSV file\n","        for row in reader:\n","            # Filter by the specified sample type\n","            if row['Sample type'] == sample_type:\n","                reference_data[row['PCR ID']] = row[column_for_replace]\n","    print(f\"Loaded reference data for sample type: {sample_type}.\")\n","else:\n","    print(\"reference.csv not found. Skipping sample replacement step.\")\n","\n","# Read the barcode_ID file\n","with open(barcode_ID_file, 'r') as barcode_file:\n","    reader = csv.DictReader(barcode_file)\n","    barcode_header = reader.fieldnames  # Save the header\n","    for row in reader:\n","        sample_id = row['SampleID']  # Original SampleID from the barcode file\n","        # Replace SampleID if reference data is available and matches\n","        if reference_data and sample_id in reference_data:\n","            row['SampleID'] = reference_data[sample_id]  # Replace with Reference ID\n","        barcode_IDs.append(row)  # Add modified row to the list\n","\n","# Prepare output data\n","output_data = barcode_IDs  # Append modified barcode data\n","\n","# Write to the new CSV file\n","with open(output_file, 'w', newline='') as output_csv:\n","    writer = csv.DictWriter(output_csv, fieldnames=barcode_header)\n","    writer.writeheader()\n","    writer.writerows(output_data)\n","\n","print(f\"File written to: {output_file}\")"]},{"cell_type":"markdown","id":"ba0ee255-af7e-4b94-9e4f-0e86ddb783e7","metadata":{"id":"ba0ee255-af7e-4b94-9e4f-0e86ddb783e7"},"source":["##### Rename SampleID\n","- if > 1 gene fragment with same SampleID"]},{"cell_type":"code","execution_count":null,"id":"6af7282f-4ac9-4ba3-8828-9357ffdbe98b","metadata":{"id":"6af7282f-4ac9-4ba3-8828-9357ffdbe98b"},"outputs":[],"source":["import os\n","import csv\n","\n","\n","''' ===== Configuration ===== '''\n","project_name = \"\"  # Set this to your project name\n","sample_type = \"DNA samples\"  # Specify the sample type for filtering\n","column_for_replace = \"Sample number\"  # Column from reference file to replace SampleID\n","replace_name = True  # Set True to replace SampleID with Reference ID from reference.csv\n","\n","''' File path settings '''\n","base_dir = f\"/mnt/Data/{project_name}\"  # Base directory containing the csv files\n","barcode_ID_file = os.path.join(base_dir, f\"{project_name}.csv\")\n","reference_file = os.path.join(base_dir, \"reference.csv\")\n","output_file = os.path.join(base_dir, f\"{project_name}_modf.csv\")\n","\n","\n","''' ===== Workflow ===== '''\n","# Initialize containers for data\n","barcode_IDs = []\n","reference_data = {}\n","\n","trichophyton_genes = {\"SQLE\": \"TrSQLE-F1_H1\", \"EF\": \"EF-DermF_H1\", \"ITS\": \"1389F_H1\"}\n","\n","if replace_name and os.path.exists(reference_file):\n","    with open(reference_file, 'r') as ref_file:\n","        reader = csv.DictReader(ref_file)\n","        for row in reader:\n","            if row['Sample type'] == sample_type:\n","                primer_f = row.get(\"1' PCR Primer_F\", \"\")\n","                matched_gene_id = next((gene for gene, marker in trichophyton_genes.items() if primer_f == marker), None)\n","                modified_sample_id = f\"{row[column_for_replace]}_{matched_gene_id}\" if matched_gene_id else row[column_for_replace]\n","                reference_data[row['PCR ID']] = modified_sample_id\n","    print(f\"Loaded reference data for sample type: {sample_type}.\")\n","elif replace_name:\n","    raise FileNotFoundError(f\"Reference file not found at {reference_file}. Please provide the correct path.\")\n","\n","with open(barcode_ID_file, 'r') as barcode_file:\n","    reader = csv.DictReader(barcode_file)\n","    barcode_header = reader.fieldnames\n","    for row in reader:\n","        sample_id = row['SampleID']\n","        modified_sample_id = reference_data.get(sample_id, sample_id)\n","        row['SampleID'] = modified_sample_id\n","        barcode_IDs.append(row)\n","\n","with open(output_file, 'w', newline='') as output_csv:\n","    writer = csv.DictWriter(output_csv, fieldnames=barcode_header)\n","    writer.writeheader()\n","    writer.writerows(barcode_IDs)\n","\n","print(f\"File written to: {output_file}\")"]},{"cell_type":"markdown","id":"14714e00-330c-4c6f-9da9-b50eb054e078","metadata":{"id":"14714e00-330c-4c6f-9da9-b50eb054e078"},"source":["## 2-1. Load NanoAct"]},{"cell_type":"code","execution_count":null,"id":"1c562dc8-b546-4c84-af72-8bc805bdb434","metadata":{"id":"1c562dc8-b546-4c84-af72-8bc805bdb434"},"outputs":[],"source":["import os\n","\n","working_directory = os.getcwd()\n","\n","# Change to home directory\n","os.chdir(os.path.expanduser(\"~\"))\n","\n","# Check if 'nanoACT' directory exists\n","if not os.path.exists(\"nanoACT\"):\n","    # If not, clone the repository\n","    !git clone https://github.com/Raingel/nanoACT.git\n","    os.chdir(os.path.expanduser(\"~/nanoACT/\"))\n","else:\n","    # If the directory exists, reset local changes and pull the latest updates\n","    os.chdir(os.path.expanduser(\"~/nanoACT/\"))\n","    !git fetch --all > /dev/null 2>&1\n","    !git reset --hard origin/main > /dev/null 2>&1 # Force reset to the latest commit\n","    !git pull > /dev/null 2>&1\n","\n","# Install requirements if necessary\n","\"\"\"\n","!pip install --upgrade pip\n","!pip install -r requirements.txt\n","\"\"\"\n","\n","# Import nanoAct and initialize\n","from nanoact import nanoact\n","dumb = nanoact.NanoAct(TEMP = \"/home/nanoACT/temp/\")\n","\n","# Change back to the original working directory\n","os.chdir(working_directory)\n","\n","# Verify the current working directory\n","print(os.getcwd())"]},{"cell_type":"markdown","id":"6437fcd3-cd5e-4f63-bb8c-9cd6f517289c","metadata":{"id":"6437fcd3-cd5e-4f63-bb8c-9cd6f517289c"},"source":["## 2-2. Processing sequences\n","- Quality filtering\n","- Demultiplexing\n","- Orientation\n","- Trimming artificial reads"]},{"cell_type":"code","execution_count":null,"id":"58ec5098-9e3c-4a52-91d2-c5cf1db9077c","metadata":{"id":"58ec5098-9e3c-4a52-91d2-c5cf1db9077c"},"outputs":[],"source":["import os\n","\n","\n","''' ===== Configuration ===== '''\n","project_name = \"\"  # Set this to your project name\n","renamed = False\n","\n","''' Analysis settings '''\n","input_format = \"fastq\"\n","output_format = \"fastq\" #輸出檔案的格式，預設為 'both'。可以是 fastq 或 fasta。'both' 代表同時輸出 fastq 和 fasta\n","mismatch_ratio_f = 0.1 #FwIndex容許的錯誤率，預設為0.15。例如barcode長度為20bp，則容許0.15*20=3bp的錯誤(edit distance)\n","mismatch_ratio_r = 0.1 #RvAnchor容許的錯誤率，預設為0.15\n","\n","# Quality Filter Configuration\n","QSCORE = 9 #recommended 7-9\n","MIN_LEN = 800 #depends on the length of your reads\n","MAX_LEN = 2000 #depends on the length of your reads\n","\n","# Demultiplexing Configuration\n","expected_length_variation = 0.75 #預期的read長度變異，預設為0.3。例如預期的read長度為300bp，則容許0.3*300=90bp的變異\n","search_range = 150 #搜尋barcode的範圍，預設為150bp。代表搜尋範圍為前150bp和後150bp\n","rvc_rvanchor = False #預設為'False'。'True'則程式執行reverse-complement。\n","\n","# Orientation Correction Configuration\n","orientation_search_range = 500 #搜尋FwPrimer和RvPrimer的範圍，預設為200bp。代表搜尋範圍為前200bp和後200bp。\n","\n","# Trim Reads Configuration\n","fw_offset = 0 #從距離找到的切除位點開始往後切除幾個bp，預設為0，可以是負數。例如fw_offset=-10，則從距離找到的切除位點開始往前切除10個bp\n","rv_offset = 0 #從距離找到的切除位點開始往前切除幾個bp，預設為0，可以是負數。例如rv_offset=-10，則從距離找到的切除位點開始往後切除10個bp\n","discard_no_match = False\n","check_both_directions = True\n","reverse_complement_rv_col = True\n","trimming_search_range = 200\n","\n","# Clustering Configuration (mmseqs_cluster)\n","cluster_min_seq_id = 0.98\n","cluster_mode = 0\n","cov_mode = 0\n","kmer_length = 15\n","kmer_per_seq = 20\n","sensitivity = 8.5\n","min_read_num = 4\n","suppress_output = True #suppress_output=False will output all details of the clustering process. Use it when unknown error occurs.\n","\n","# Consensus Configuration (mafft_consensus)\n","minimal_reads = 2  # minimal_reads for consensus\n","max_reads = -1 #max_reads: 設定最多的序列數量，-1 代表不限制。例如max_reads=100，則只會隨機取100個序列進行排比\n","adjustdirection = False\n","\n","\n","''' ===== Workflow ===== '''\n","def process_data(project_name):\n","    data_base_path = f\"/mnt/Data/{project_name}\"\n","    src_path_dorado = os.path.join(data_base_path, \"1_dorado\")\n","    des_path_nanofilt = os.path.join(data_base_path, \"2_nanofilt\")\n","    des_path_demultiplex = os.path.join(data_base_path, \"3_demultiplex\")\n","    des_path_orientation = os.path.join(data_base_path, \"4_orientation\")  # Orientation output folder\n","    des_path_trimmed = os.path.join(data_base_path, \"5_trimmed\")  # Trimming output folder\n","    des_path_mmseqs = os.path.join(data_base_path, \"6_mmseqs\")\n","    des_path_consensus = os.path.join(data_base_path, \"7_consensus\")\n","    if renamed:\n","        barcode_index_file = os.path.join(data_base_path, f\"{project_name}_modf.csv\")\n","    else:\n","        barcode_index_file = os.path.join(data_base_path, f\"{project_name}.csv\")\n","\n","\n","    # Step 1. Filter by Quality and Length\n","    filtered_fastq = dumb.qualityfilt(\n","        src = os.path.join(src_path_dorado, 'all.fastq'),\n","        des = des_path_nanofilt,\n","        name = 'all_qualityfilt.fastq',\n","        QSCORE = QSCORE,\n","        MIN_LEN = MIN_LEN,\n","        MAX_LEN = MAX_LEN\n","    )\n","\n","    # Step 2. Demultiplexing\n","    demultiplexed = dumb.singlebar(\n","        src = os.path.join(des_path_nanofilt, 'all_qualityfilt.fastq'),\n","        des = des_path_demultiplex,\n","        BARCODE_INDEX_FILE = barcode_index_file,\n","        mismatch_ratio_f = mismatch_ratio_f,\n","        mismatch_ratio_r = mismatch_ratio_r,\n","        expected_length_variation = expected_length_variation,\n","        search_range = search_range,\n","        rvc_rvanchor = rvc_rvanchor,\n","        input_format = input_format,\n","        output_format = output_format\n","    )\n","\n","    # Step 3. Orientation correction\n","    orientation = dumb.orientation(\n","        src = des_path_demultiplex,\n","        des = des_path_orientation,\n","        input_format = input_format,\n","        output_format = output_format,\n","        BARCODE_INDEX_FILE = barcode_index_file,\n","        FwPrimer = \"FwPrimer\",\n","        RvPrimer = \"RvPrimer\",\n","        search_range = orientation_search_range\n","    )\n","\n","    # Step 4. Trim Reads\n","    trimmed = dumb.trim_reads(\n","        src = des_path_orientation,\n","        des = des_path_trimmed,\n","        BARCODE_INDEX_FILE = barcode_index_file,\n","        fw_col = \"FwPrimer\",\n","        rv_col = \"RvPrimer\",\n","        input_format = input_format,\n","        output_format = output_format,\n","        mode = \"table\",\n","        fw_offset = fw_offset,\n","        rv_offset = rv_offset,\n","        mismatch_ratio_f = mismatch_ratio_f,\n","        mismatch_ratio_r = mismatch_ratio_r,\n","        discard_no_match = discard_no_match,\n","        check_both_directions = check_both_directions,\n","        reverse_complement_rv_col = reverse_complement_rv_col,\n","        search_range = trimming_search_range\n","    )\n","\n","    # Step 5. Clustering\n","    clustering = dumb.mmseqs_cluster(\n","        src = des_path_trimmed,\n","        des = des_path_mmseqs,\n","        min_seq_id = cluster_min_seq_id,\n","        cluster_mode = cluster_mode,\n","        cov_mode = cov_mode,\n","        k = kmer_length,\n","        kmer_per_seq = kmer_per_seq,\n","        s = sensitivity,\n","        min_read_num = min_read_num,\n","        suppress_output = suppress_output,\n","        input_format = input_format,\n","        output_format = \"fasta\"\n","    )\n","\n","    # Step 6. Consensus\n","    consensus = dumb.mafft_consensus(\n","        src = des_path_mmseqs,\n","        des = des_path_consensus,\n","        minimal_reads = minimal_reads,\n","        max_reads = max_reads,\n","        adjustdirection = adjustdirection,\n","        input_format = \"fas\"\n","    )\n","\n","    return \"Data processing complete.\"\n","\n","# Call the function with the desired project ID:\n","result = process_data(project_name)\n","print(result)"]},{"cell_type":"markdown","id":"89b54316-1761-4c4b-9303-0d439612bde0","metadata":{"id":"89b54316-1761-4c4b-9303-0d439612bde0"},"source":["# 3. Sequences Processing"]},{"cell_type":"markdown","id":"4762841e-af6b-4853-9c90-b5c08e8b2d42","metadata":{"id":"4762841e-af6b-4853-9c90-b5c08e8b2d42"},"source":["## 3-1. Generate sequences processing summary"]},{"cell_type":"code","execution_count":null,"id":"88d916ec-ade0-44ae-9f73-d6ad59c3f494","metadata":{"id":"88d916ec-ade0-44ae-9f73-d6ad59c3f494"},"outputs":[],"source":["import os\n","import csv\n","from Bio import SeqIO\n","import re\n","from itertools import groupby\n","from operator import itemgetter\n","\n","''' ===== Configuration ===== '''\n","project_name = \"\"\n","\n","'''File Path Settings'''\n","base_dir = f\"/mnt/Data/{project_name}\"\n","input_dir = os.path.join(base_dir, \"7_consensus\")\n","output_csv = os.path.join(base_dir, \"con_fastq_summary.csv\")\n","\n","''' ===== Functions ===== '''\n","def parse_fas_files(input_dir):\n","    parsed_data = []\n","    for filename in os.listdir(input_dir):\n","        if filename.endswith('.fas') and filename.startswith('con_'):  # Check if filename starts with 'con_'\n","            file_path = os.path.join(input_dir, filename)\n","            for record in SeqIO.parse(file_path, \"fasta\"):\n","                fasta_name = record.id.split('_')[0]\n","                seq_length = len(record.seq)\n","                parsed_data.append((fasta_name, filename, seq_length))\n","    return parsed_data\n","\n","def extract_read_number(filename):\n","    match = re.search(r'_r(\\d+)', filename)\n","    return int(match.group(1)) if match else float('inf')  # Use 'inf' for files without read numbers\n","\n","\n","''' ===== Workflow ===== '''\n","parsed_data = parse_fas_files(input_dir)\n","parsed_data.sort(key=lambda x: extract_read_number(x[1]), reverse = True)  # Sort by read number from filename\n","with open(output_csv, 'w', newline='') as csvfile:\n","    csv_writer = csv.writer(csvfile)\n","    csv_writer.writerow(['FASTA_name', 'Filename', 'Sequence_Length'])\n","    csv_writer.writerows(parsed_data)\n","\n","print(f\"Parsed {len(parsed_data)} records. Output saved to {output_csv}\")"]},{"cell_type":"markdown","id":"46b0f19c-b711-44ee-bedf-61b9cbe345e9","metadata":{"id":"46b0f19c-b711-44ee-bedf-61b9cbe345e9"},"source":["## 3-2. Select the highest read files"]},{"cell_type":"code","execution_count":null,"id":"633f5b05-80de-4512-ba2f-e5785d96477d","metadata":{"id":"633f5b05-80de-4512-ba2f-e5785d96477d"},"outputs":[],"source":["import os\n","import shutil\n","import re\n","import time\n","import csv\n","from collections import defaultdict\n","from threading import Thread\n","from Bio import SeqIO\n","\n","\n","''' ===== Configuration ===== '''\n","project_name = \"\"\n","\n","'''File Path Settings'''\n","base_dir = f\"/mnt/Data/{project_name}\"\n","input_dir = os.path.join(base_dir, \"7_consensus\")\n","output_dir = os.path.join(base_dir, \"8_highest_read_per_sample\")\n","output_csv = os.path.join(base_dir, \"highest_fasta_summary.csv\")\n","\n","\n","''' ===== Spinner Function ===== '''\n","def spinner():\n","    symbols = ['|', '/', '-', '\\\\']\n","    idx = 0\n","    while not stop_spinner:\n","        print(f\"\\rProcessing files... {symbols[idx % len(symbols)]}\", end='', flush=True)\n","        idx += 1\n","        time.sleep(0.1)\n","\n","\n","''' ===== Functions ===== '''\n","def extract_highest_read_files(input_dir, output_dir):\n","    global stop_spinner\n","    stop_spinner = False\n","\n","    # Start the spinner in a separate thread\n","    spinner_thread = Thread(target=spinner)\n","    spinner_thread.start()\n","\n","    try:\n","        # Check if the output directory exists, if not, create it\n","        if not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        # Regex to extract sample name, optional gene ID, cluster number, and read number\n","        pattern = re.compile(\n","            r'con_([^_]+)(?:_([^_]+))?_cluster_(\\d+)_r(\\d+)\\.fas', re.IGNORECASE\n","        )\n","\n","        # Dictionary to store the highest read file for each sample\n","        highest_read_files = defaultdict(lambda: ('', 0))\n","\n","        # Scan through all the files in the input directory\n","        for file in os.listdir(input_dir):\n","            if file.endswith('.fas'):\n","                match = pattern.match(file)\n","                if match:\n","                    sample_name, gene_id, cluster_number, read_number = match.groups()\n","                    key = (sample_name, gene_id)  # Use both sample name and gene ID as the key\n","                    read_number = int(read_number)\n","                    if read_number > highest_read_files[key][1]:\n","                        highest_read_files[key] = (file, read_number)\n","\n","        # Copy the highest read files to the output directory\n","        copied_files_count = 0\n","        for file, _ in highest_read_files.values():\n","            src = os.path.join(input_dir, file)\n","            dest = os.path.join(output_dir, file)\n","            shutil.copyfile(src, dest)\n","            copied_files_count += 1\n","\n","        # Stop the spinner\n","        stop_spinner = True\n","        spinner_thread.join()\n","\n","        # Clear the spinner and print a final message\n","        print(f\"\\rCopied {copied_files_count} highest read files to {output_dir}\".ljust(80))\n","    except Exception as e:\n","        stop_spinner = True\n","        spinner_thread.join()\n","        print(f\"\\rError occurred: {e}\".ljust(80))\n","\n","def parse_fas_files(input_dir):\n","    parsed_data = []\n","    for filename in os.listdir(input_dir):\n","        if filename.endswith('.fas') and filename.startswith('con_'):  # Check if filename starts with 'con_'\n","            file_path = os.path.join(input_dir, filename)\n","            for record in SeqIO.parse(file_path, \"fasta\"):\n","                fasta_name = record.id.split('_')[0]\n","                seq_length = len(record.seq)\n","                parsed_data.append((fasta_name, filename, seq_length))\n","    return parsed_data\n","\n","def extract_read_number(filename):\n","    match = re.search(r'_r(\\d+)', filename)\n","    return int(match.group(1)) if match else float('inf')  # Use 'inf' for files without read numbers\n","\n","''' ===== Workflow ===== '''\n","if __name__ == \"__main__\":\n","    stop_spinner = False\n","    extract_highest_read_files(input_dir, output_dir)\n","    parsed_data = parse_fas_files(output_dir)  # Parse from the output directory\n","    with open(output_csv, 'w', newline='') as csvfile:\n","        csv_writer = csv.writer(csvfile)\n","        csv_writer.writerow(['FASTA_name', 'Filename', 'Sequence_Length'])\n","        csv_writer.writerows(parsed_data)\n","\n","    print(f\"Parsed {len(parsed_data)} records. Output saved to {output_csv}\")\n"]},{"cell_type":"markdown","id":"65b79b57-4b58-48df-8338-0092ec2693d0","metadata":{"id":"65b79b57-4b58-48df-8338-0092ec2693d0"},"source":["# 4. BLAST ID for the highest read files\n","- NCBI BLAST\n","- Local BLAST"]},{"cell_type":"code","execution_count":null,"id":"544f30f5-75ab-4142-aff4-4f885aa613cd","metadata":{"id":"544f30f5-75ab-4142-aff4-4f885aa613cd"},"outputs":[],"source":["import os\n","\n","\n","''' ===== Configuration ===== '''\n","project_name = \"\"\n","output_csv = \"blast_ncbi.csv\"\n","\n","'''File Path Settings'''\n","base_dir = f\"/mnt/Data/{project_name}\"\n","input_dir = os.path.join(base_dir, \"8_highest_read_per_sample\")\n","output_dir = base_dir\n","\n","''' ===== Workflow ===== '''\n","dumb.blast_2 (src = input_dir, #Input: 一個資料夾，資料夾中包含以SampleID為檔名的fasta檔案，例如 SampleID.fasta\n","              des = output_dir, #Output: 一個資料夾，程式會在該資料夾中輸出一個csv檔案，檔案\n","              name = output_csv, #name: 輸出檔案的檔名，預設為blast.csv。\n","              funguild = True, #funguild: 是否進行funguild的分析，預設為True。\n","              startswith = \"con_\", #startswith: 輸入fasta檔案的檔名所具有的開頭，預設為con_。\n","              input_format = \"fas\", #input_format: 輸入fasta檔案的格式，預設為fasta, 若為fas則為副檔名為fas的fasta檔案。\n","              query_range = (None, None), #`query_range`: 代表該序列要用來送去blast的區間。一般建議不要超過500 bp。過長除了會造成blast伺服器負擔過重外，由於blast的排序同時考慮coverage及similarity。過長\n","                                #的query將會導致高coverage但低similarity的hit排在前面，排擠掉中等coverage但高similarity的hit\n","                                #輸入值為tuple，範例： 假設序列為 AAATTTCCC\n","                                #query_range=(None,None)則代表完全不裁切\n","                                #query_range=(0,None)也代表完全不裁切\n","                                #query_range=(0,-1) 則代表從第0個位置(第1個 bp，程式上習慣從0開始計數)開始，到(不包含)倒數最後一個，實際query為 AAATTTCC\n","                                #query_range=(2,5) 則代表從第2個開始，到(不包含)第5個，實際query為 ATT\n","                                #query_range=(3,-3) 則代表從第3個開始，到(不包含)倒數第3個，實際query為 TTT\n","              batch = 10 #`batch`: This parameter is an integer indicating the number of sequences to blast at a time.\n","                      #The input sequences are divided into batches of size `batch`, and each batch is blasted separately.\n","                      #This is done to avoid overloading the NCBI BLAST server with too many requests at once.\n","             )\n","\n","#將consensus序列進行blast，並生成一個csv檔，內包含每個序列的blast結果\n","#Input: A folder containing fasta files named in the specified format\n","#Format: con_{sampleID}_cluster_{number}_r{reads_number}\n","#e.g.: con_2523_cluster_1_r499\n","#Output: A csv file named {name} is saved in the {des} folder\n","# `funguild`: This parameter is a boolean value that indicates whether to perform a Funguild search or not.\n","#             Funguild is a web-based annotation tool that allows users to predict the ecological functions of fungal communities based on their taxonomic composition.\n","# `startswith`: This parameter is a string indicating the prefix that the input fasta file names should start with. It is used to filter out files that do not match the given prefix.\n","# `query_range`: 代表該序列要用來送去blast的區間。一般建議不要超過500 bp。過長除了會造成blast伺服器負擔過重外，由於blast的排序同時考慮coverage及similarity。過長\n","#                的query將會導致高coverage但低similarity的hit排在前面，排擠掉中等coverage但高similarity的hit\n","#                輸入值為tuple，範例： 假設序列為 AAATTTCCC\n","#                query_range=(None,None)則代表完全不裁切\n","#                query_range=(0,None)也代表完全不裁切\n","#                query_range=(0,-1) 則代表從第0個位置(第1個 bp，程式上習慣從0開始計數)開始，到(不包含)倒數最後一個，實際query為 AAATTTCC\n","#                query_range=(2,5) 則代表從第2個開始，到(不包含)第5個，實際query為 ATT\n","#                query_range=(3,-3) 則代表從第3個開始，到(不包含)倒數第3個，實際query為 TTT\n","# `batch`: This parameter is an integer indicating the number of sequences to blast at a time.\n","#          The input sequences are divided into batches of size `batch`, and each batch is blasted separately.\n","#          This is done to avoid overloading the NCBI BLAST server with too many requests at once.\n"]},{"cell_type":"code","execution_count":null,"id":"b6b15304-a791-4897-9337-0fb1750ec237","metadata":{"id":"b6b15304-a791-4897-9337-0fb1750ec237"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}