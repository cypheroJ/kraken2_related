{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01032aec-317c-4a2b-bb48-8d2e22fd7ec3",
   "metadata": {},
   "source": [
    "# Nanopore MLST Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41774e4b-53a9-4b9a-8c7d-90ba91a7b141",
   "metadata": {},
   "source": [
    "# 1. dorado Basecalling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accdcacb-c633-4491-b88c-986f70a4d243",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Download Dorado basecaller\n",
    "- if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3684414-0b38-4da4-ac01-d2ee24ea3566",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71673,
     "status": "ok",
     "timestamp": 1690673768489,
     "user": {
      "displayName": "Yin-Tse Huang",
      "userId": "00471228198785597773"
     },
     "user_tz": -480
    },
    "id": "tgWBvko6uEtD",
    "outputId": "19c89325-da8a-44a5-b60a-4c42411879ce"
   },
   "outputs": [],
   "source": [
    "!wget https://cdn.oxfordnanoportal.com/software/analysis/dorado-0.9.0-linux-x64.tar.gz -O /mnt/NanoporeRawData/dorado.tar.gz\n",
    "!tar -xvf /mnt/NanoporeRawData/dorado.tar.gz -C /mnt/NanoporeRawData/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fbad22-0d7f-447c-973c-5d8b68fcf144",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Fast5 Files Conversion\n",
    "- if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce212263-953c-4354-a704-36f6251f1fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAST5 directory detected, converting to POD5...\n",
      "Requirement already satisfied: pod5 in /opt/miniconda3/lib/python3.12/site-packages (0.3.15)\n",
      "Requirement already satisfied: lib-pod5==0.3.15 in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (0.3.15)\n",
      "Requirement already satisfied: iso8601 in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (2.1.0)\n",
      "Requirement already satisfied: more-itertools in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (10.5.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (1.26.4)\n",
      "Requirement already satisfied: pytz in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (2024.1)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (23.2)\n",
      "Requirement already satisfied: polars~=0.19 in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (0.20.31)\n",
      "Requirement already satisfied: vbz-h5py-plugin in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (4.66.2)\n",
      "Requirement already satisfied: pyarrow~=16.1.0 in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (16.1.0)\n",
      "Requirement already satisfied: h5py~=3.11.0 in /opt/miniconda3/lib/python3.12/site-packages (from pod5) (3.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting 536 Fast5s: 100%|##########| 2142460/2142460 [4:05:15<00:00, 145.59Reads/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POD5 conversion completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Define paths\n",
    "raw_data_path = \"/mnt/NanoporeRawData/2023-000029/\"\n",
    "fast5_path = os.path.join(raw_data_path, \"fast5\")\n",
    "pod5_path = os.path.join(raw_data_path, \"pod5\")\n",
    "\n",
    "# Function to check if the number of files match and none are empty\n",
    "def check_pod5_files(fast5_path, pod5_path):\n",
    "    fast5_files = [f for f in os.listdir(fast5_path) if f.endswith('.fast5')]\n",
    "    pod5_files = [f for f in os.listdir(pod5_path) if f.endswith('.pod5')]\n",
    "\n",
    "    # Check if counts match and ensure no empty files\n",
    "    if len(fast5_files) != len(pod5_files):\n",
    "        return False\n",
    "\n",
    "    for pod5_file in pod5_files:\n",
    "        if os.path.getsize(os.path.join(pod5_path, pod5_file)) == 0:  # Check if the file is empty\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Check if only the fast5 directory exists, and convert FAST5 to POD5\n",
    "if os.path.exists(fast5_path) and not os.path.exists(pod5_path):\n",
    "    print(\"FAST5 directory detected, converting to POD5...\")\n",
    "\n",
    "    # Install pod5 package if not already installed\n",
    "    subprocess.run([\"pip\", \"install\", \"pod5\"], check=True)\n",
    "\n",
    "    # Ensure the pod5 directory exists\n",
    "    os.makedirs(pod5_path, exist_ok=True)\n",
    "\n",
    "    # Convert FAST5 to POD5, using -o for output and -O for one-to-one mapping\n",
    "    subprocess.run([\n",
    "        \"pod5\", \"convert\", \"fast5\", \n",
    "        \"-o\", pod5_path,  # Output directory for POD5 files\n",
    "        \"-O\", fast5_path,  # Parent directory for input files\n",
    "        fast5_path  # Input path\n",
    "    ], check=True)\n",
    "\n",
    "# Rerun conversion if checks fail\n",
    "if not check_pod5_files(fast5_path, pod5_path):\n",
    "    print(\"File count mismatch or empty POD5 files detected. Re-running conversion...\")\n",
    "    subprocess.run([\n",
    "        \"pod5\", \"convert\", \"fast5\", \n",
    "        \"-o\", pod5_path, \n",
    "        \"-O\", fast5_path,  \n",
    "        fast5_path  \n",
    "    ], check=True)\n",
    "\n",
    "print(\"POD5 conversion completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755dd342-7367-4e3c-808b-68f51bdba9a3",
   "metadata": {},
   "source": [
    "## 1-1. Pod5 Files Basecalling\n",
    "- basecall\n",
    "- convert BAM to fastq\n",
    "- generate NanoPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450b6471-c339-4bc4-9f93-2d3d79f6a9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Dorado binary: /mnt/NanoporeRawData/dorado-0.9.0-linux-x64/bin/dorado\n",
      "Running dorado basecaller...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-30 10:45:26.903] [info] Running: \"basecaller\" \"sup\" \"/mnt/NanoporeRawData/2024-000023/pod5\"\n",
      "[2024-12-30 10:45:32.723] [info]  - downloading dna_r10.4.1_e8.2_400bps_sup@v5.0.0 with httplib\n",
      "[2024-12-30 10:46:36.456] [info] > Creating basecall pipeline\n",
      "[2024-12-30 10:46:46.593] [info] Calculating optimized batch size for GPU \"NVIDIA GeForce RTX 4090\" and model /mnt/Data/2024-000023/.temp_dorado_model-c0e550a761c0f790/dna_r10.4.1_e8.2_400bps_sup@v5.0.0. Full benchmarking will run for this device, which may take some time.\n",
      "[2024-12-30 10:47:03.368] [info] cuda:0 using chunk size 11520, batch size 128\n",
      "[2024-12-30 10:47:03.856] [info] cuda:0 using chunk size 5760, batch size 128\n",
      "[2024-12-30 13:45:46.330] [info] > Finished in (ms): 10721982\n",
      "[2024-12-30 13:45:46.330] [info] > Simplex reads basecalled: 2688138\n",
      "[2024-12-30 13:45:46.330] [info] > Simplex reads filtered: 29\n",
      "[2024-12-30 13:45:46.330] [info] > Basecalled @ Samples/s: 2.841616e+06\n",
      "[2024-12-30 13:45:47.924] [info] > Finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting BAM to FASTQ...\n",
      "Generating NanoPlot...\n",
      "Dorado basecalling and analysis completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from packaging import version\n",
    "\n",
    "\n",
    "''' ===== Configuration ===== '''\n",
    "dorado_base_path = \"/mnt/NanoporeRawData\"\n",
    "raw_data_path = \"/mnt/NanoporeRawData/2024-000023/pod5\"\n",
    "basecall_data_path = \"/mnt/Data/2024-000023/1_dorado\"\n",
    "\n",
    "\n",
    "''' ===== Workflow ===== '''\n",
    "def get_latest_dorado_path(base_path):\n",
    "    dorado_versions = [\n",
    "        os.path.join(base_path, d) for d in os.listdir(base_path)\n",
    "        if d.startswith(\"dorado-\") and os.path.isdir(os.path.join(base_path, d))\n",
    "    ]\n",
    "    dorado_versions = sorted(\n",
    "        dorado_versions, \n",
    "        key=lambda d: version.parse(d.split('-')[1].split('-')[0]),\n",
    "        reverse=True\n",
    "    )\n",
    "    if dorado_versions:\n",
    "        return os.path.join(dorado_versions[0], \"bin\", \"dorado\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No Dorado versions found in the specified base path.\")\n",
    "\n",
    "try:\n",
    "    dorado_path = get_latest_dorado_path(dorado_base_path)\n",
    "    print(f\"Using Dorado binary: {dorado_path}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    exit(1)\n",
    "\n",
    "os.makedirs(basecall_data_path, exist_ok=True)\n",
    "\n",
    "print(\"Running dorado basecaller...\")\n",
    "bam_file_path = os.path.join(basecall_data_path, \"all.bam\")\n",
    "with open(bam_file_path, \"w\") as bam_file:\n",
    "    subprocess.run([\n",
    "        dorado_path, \"basecaller\", \"sup\", raw_data_path\n",
    "    ], stdout=bam_file, check=True)\n",
    "\n",
    "print(\"Converting BAM to FASTQ...\")\n",
    "subprocess.run([\n",
    "    \"bedtools\", \"bamtofastq\", \n",
    "    \"-i\", bam_file_path,\n",
    "    \"-fq\", os.path.join(basecall_data_path, \"all.fastq\")\n",
    "], check=True)\n",
    "\n",
    "print(\"Generating NanoPlot...\")\n",
    "subprocess.run([\n",
    "    \"NanoPlot\", \n",
    "    \"--fastq\", os.path.join(basecall_data_path, \"all.fastq\"),\n",
    "    \"-o\", basecall_data_path\n",
    "], check=True)\n",
    "\n",
    "print(\"Dorado basecalling and analysis completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd13ad-060f-48b6-857c-03b3c578676c",
   "metadata": {},
   "source": [
    "# 2. NanoACT Demultiplexing & Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e37efe0-5c86-422d-b188-bfac42153441",
   "metadata": {},
   "source": [
    "## 2-1. Load NanoAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd289e45-3f5d-48fe-accf-e4b420883648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/Data/2024-000023\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "working_directory = os.getcwd()\n",
    "\n",
    "# Change to home directory\n",
    "os.chdir(os.path.expanduser(\"~\"))\n",
    "\n",
    "# Check if 'nanoACT' directory exists\n",
    "if not os.path.exists(\"nanoACT\"):\n",
    "    # If not, clone the repository\n",
    "    !git clone https://github.com/Raingel/nanoACT.git\n",
    "    os.chdir(os.path.expanduser(\"~/nanoACT/\"))\n",
    "else:\n",
    "    # If the directory exists, reset local changes and pull the latest updates\n",
    "    os.chdir(os.path.expanduser(\"~/nanoACT/\"))\n",
    "    !git fetch --all > /dev/null 2>&1\n",
    "    !git reset --hard origin/main > /dev/null 2>&1 # Force reset to the latest commit\n",
    "    !git pull > /dev/null 2>&1\n",
    "\n",
    "# Install requirements if necessary\n",
    "\"\"\"\n",
    "!pip install --upgrade pip\n",
    "!pip install -r requirements.txt\n",
    "\"\"\"\n",
    "\n",
    "# Import nanoAct and initialize\n",
    "from nanoact import nanoact\n",
    "dumb = nanoact.NanoAct(TEMP = \"/home/huanglabserver/nanoACT/temp/\")\n",
    "\n",
    "# Change back to the original working directory\n",
    "os.chdir(working_directory)\n",
    "\n",
    "# Verify the current working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b440f-0ebd-4405-81dd-f3de5864da07",
   "metadata": {},
   "source": [
    "## 2-2. Creating Barcoded_primers List\n",
    "- Select target taxon\n",
    "- Extract Sample ID and PCR ID\n",
    "- Read Barcode file\n",
    "- Read MLST primers file\n",
    "- Rename PCR ID with real Sample ID\n",
    "- Create Barcoded-primers file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362ffe3e-230c-451e-af12-ce5a03f46341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded reference data.\n",
      "File written to: /mnt/Data/2024-000023/2024-000023_barcoded_MLST.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "''' ===== Configuration ===== '''\n",
    "project_id = \"2024-000023\"  # Set this to your project ID\n",
    "target_taxon = \"Candida\"\n",
    "replace_name = True # Set True to replace PCR ID with your own sample name according to reference.csv\n",
    "\n",
    "''' File path settings '''\n",
    "base_dir =  f\"/mnt/Data/{project_id}\"  # Base directory containing the csv files\n",
    "barcode_ID_file = os.path.join(base_dir, f\"{project_id}.csv\")\n",
    "reference_file = os.path.join(base_dir, \"reference.csv\")\n",
    "MLST_primers_file = os.path.join(base_dir, \"mlst_primers.csv\")\n",
    "output_file = os.path.join(base_dir, f\"{project_id}_barcoded_MLST.csv\")\n",
    "\n",
    "\n",
    "''' ===== Workflow  ===== '''\n",
    "barcode_IDs = []\n",
    "MLST_primers = []\n",
    "reference_data = {}\n",
    "\n",
    "candida_genes = {\"AAT1a\", \"ACC1\", \"ADP1\", \"MPIb\", \"SYA1\", \"VPS13\", \"ZWF1b\"}\n",
    "scedosporium_genes = {\"ACT-1_4\", \"fRPB2-5f_7cR\", \"SOD2F3_R3\", \"Bt2a_2b\", \"1389_ITS4ngs\", \"Sau-CYP\"}\n",
    "trichophyton_genes = {\"TrSQLE\", \"EF-Derm\", \"1389_ITS4ngs\"}\n",
    "\n",
    "if target_taxon.lower() == \"candida\":\n",
    "    target_genes = candida_genes\n",
    "elif target_taxon.lower() == \"scedosporium\":\n",
    "    target_genes = scedosporium_genes\n",
    "elif target_taxon.lower() == \"trichophyton\":\n",
    "    target_genes = trichophyton_genes\n",
    "else:\n",
    "    raise ValueError(f\"Unknown target_taxon: {target_taxon}\")\n",
    "    \n",
    "if replace_name and os.path.exists(reference_file):\n",
    "    with open(reference_file, 'r') as ref_file:\n",
    "        reader = csv.DictReader(ref_file)  # Assuming CSV file\n",
    "        for row in reader:\n",
    "            sample = row['Sample'].split('_')[1]  # Extract part after '_'\n",
    "            reference_data[row['PCR ID']] = sample  # Store mapping of PCR ID to sample\n",
    "    print(\"Loaded reference data.\")\n",
    "else:\n",
    "    print(\"reference.csv not found. Skipping sample replacement step.\")\n",
    "\n",
    "with open(barcode_ID_file, 'r') as barcode_file:\n",
    "    reader = csv.DictReader(barcode_file)\n",
    "    for row in reader:\n",
    "        pcr_id = row['SampleID']  # Get PCR ID from barcode file\n",
    "        if reference_data and pcr_id in reference_data:\n",
    "            row['SampleID'] = reference_data[pcr_id]  # Replace SampleID with Sample from reference_data\n",
    "        barcode_IDs.append(row)  # Add modified row to list\n",
    "\n",
    "with open(MLST_primers_file, 'r') as primers_file:\n",
    "    reader = csv.reader(primers_file)\n",
    "    mlst_data = list(reader)\n",
    "    MLST_primers_header = mlst_data[0]  # Extract the header\n",
    "    MLST_primers = [row for row in mlst_data[1:] if row[0] in target_genes]  # Filter for target genes\n",
    "\n",
    "output_data = [['SampleID', 'FwIndex'] + MLST_primers_header[2:]]  # New header\n",
    "\n",
    "num_primers = len(MLST_primers)\n",
    "for i, barcode_info in enumerate(barcode_IDs):\n",
    "    barcode_ID = barcode_info['SampleID']\n",
    "    fw_index = barcode_info['FwIndex']\n",
    "    for j in range(num_primers):\n",
    "        primer_index = (i * num_primers + j) % num_primers  # Cycling through primers for uniqueness\n",
    "        primer_row = MLST_primers[primer_index]\n",
    "        output_data.append([f'{barcode_ID}_{primer_row[0]}', fw_index] + primer_row[2:])\n",
    "\n",
    "with open(output_file, 'w', newline='') as output_csv:\n",
    "    writer = csv.writer(output_csv)\n",
    "    writer.writerows(output_data)\n",
    "\n",
    "print(f\"File written to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82a2022-e38e-48a4-89b7-35c8241d3300",
   "metadata": {},
   "source": [
    "## 2-3. Processing Sequences\n",
    "- Quality filtering\n",
    "- Double Demultiplexing\n",
    "- Orientation \n",
    "- Trimming artificial reads\n",
    "- Clustering using mmseqs2\n",
    "- Consensus using MAFFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b97de2-0a33-4e12-9950-7739ef75042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "''' ===== Configuration ===== '''\n",
    "project_id = \"2024-000023\"  # Set this to your project ID\n",
    "\n",
    "\n",
    "''' Analysis settings '''\n",
    "input_format = \"fastq\"\n",
    "output_format = \"both\" #輸出檔案的格式，預設為 'both'。可以是 fastq 或 fasta。'both' 代表同時輸出 fastq 和 fasta\n",
    "mismatch_ratio_f = 0.1 #FwIndex容許的錯誤率，預設為0.15。例如barcode長度為20bp，則容許0.15*20=3bp的錯誤(edit distance)\n",
    "mismatch_ratio_r = 0.1 #RvAnchor容許的錯誤率，預設為0.15\n",
    "\n",
    "\n",
    "# Quality Filter Configuration\n",
    "QSCORE = 9 #recommended 7-9\n",
    "MIN_LEN = 400 #depends on the length of your reads\n",
    "MAX_LEN = 850 #depends on the length of your reads\n",
    "\n",
    "# Demultiplexing Configuration\n",
    "expected_length_variation = 0.3 #預期的read長度變異，預設為0.3。例如預期的read長度為300bp，則容許0.3*300=90bp的變異\n",
    "search_range = 150 #搜尋barcode的範圍，預設為150bp。代表搜尋範圍為前150bp和後150bp\n",
    "rvc_rvanchor = True #預設為'False'。'True'則程式執行reverse-complement。\n",
    "\n",
    "# Orientation Correction Configuration\n",
    "orientation_search_range = 500 #搜尋FwPrimer和RvPrimer的範圍，預設為200bp。代表搜尋範圍為前200bp和後200bp。\n",
    "\n",
    "# Trim Reads Configuration\n",
    "fw_offset = 0 #從距離找到的切除位點開始往後切除幾個bp，預設為0，可以是負數。例如fw_offset=-10，則從距離找到的切除位點開始往前切除10個bp\n",
    "rv_offset = 0 #從距離找到的切除位點開始往前切除幾個bp，預設為0，可以是負數。例如rv_offset=-10，則從距離找到的切除位點開始往後切除10個bp\n",
    "discard_no_match = False\n",
    "check_both_directions = True\n",
    "reverse_complement_rv_col = True\n",
    "trimming_search_range = 200\n",
    "\n",
    "# Clustering Configuration (mmseqs_cluster)\n",
    "cluster_min_seq_id = 0.98\n",
    "cluster_mode = 0\n",
    "cov_mode = 0\n",
    "kmer_length = 15\n",
    "kmer_per_seq = 20\n",
    "sensitivity = 8.5\n",
    "min_read_num = 3\n",
    "suppress_output = True #suppress_output=False will output all details of the clustering process. Use it when unknown error occurs.\n",
    "\n",
    "# Consensus Configuration (mafft_consensus)\n",
    "minimal_reads = 2  # minimal_reads for consensus\n",
    "max_reads = -1 #max_reads: 設定最多的序列數量，-1 代表不限制。例如max_reads=100，則只會隨機取100個序列進行排比\n",
    "adjustdirection = False\n",
    "\n",
    "\n",
    "''' ===== Workflow ===== '''\n",
    "def process_data(project_id):\n",
    "    data_base_path = f\"/mnt/Data/{project_id}\"\n",
    "    src_path_dorado = os.path.join(data_base_path, \"1_dorado/\")\n",
    "    des_path_nanofilt = os.path.join(data_base_path, \"2_nanofilt/\")\n",
    "    des_path_demultiplex = os.path.join(data_base_path, \"3_demultiplex/\")\n",
    "    des_path_orientation = os.path.join(data_base_path, \"4_orientation/\")  # Orientation output folder\n",
    "    des_path_trimmed = os.path.join(data_base_path, \"5_trimmed/\")  # Trimming output folder\n",
    "    des_path_mmseqs = os.path.join(data_base_path, \"6_mmseqs/\")  # Clustering output folder\n",
    "    des_path_consensus = os.path.join(data_base_path, \"7_consensus/\")  # Consensus output folder\n",
    "    barcode_index_file = os.path.join(data_base_path, f\"{project_id}_barcoded_MLST.csv\")\n",
    "\n",
    "\n",
    "    # Step 1. Filter by Quality and Length\n",
    "    filtered_fastq = dumb.qualityfilt(\n",
    "        src = os.path.join(src_path_dorado, 'all.fastq'),\n",
    "        des = des_path_nanofilt,\n",
    "        name = 'all_qualityfilt.fastq',\n",
    "        QSCORE = QSCORE,\n",
    "        MIN_LEN = MIN_LEN,\n",
    "        MAX_LEN = MAX_LEN\n",
    "    )\n",
    "    \n",
    "    '''\n",
    "    1. 使用 `fastq_reader` 函數處理原始定序檔案，每次讀取四行代表一個定序 read。\n",
    "    2. 進行兩次解多工（demultiplexing）：\n",
    "       1. 第一次基於 FwIndex 將 reads 分群。\n",
    "       2. 第二次基於引子（primers）進行進一步的分群。\n",
    "       3. 如果識別到唯一的 Sample ID，則將 read 分配到對應 Sample 的輸出檔案中。\n",
    "       4. 如果條碼被截斷，則將 read 附加到 \"TRUNCATED\" 字典中。\n",
    "       5. 如果識別出多個條碼，則將 read 附加到 \"MULTIPLE\" 字典中。\n",
    "       6. 如果未識別條碼，則將 read 附加到 \"UNKNOWN\" 字典中。\n",
    "       7. 最終根據條碼將已解多工的 reads 輸出到不同檔案。\n",
    "    '''\n",
    "    # Step 2. Demultiplexing\n",
    "    demultiplexed = dumb.double_demultiplex(\n",
    "        src = os.path.join(des_path_nanofilt, 'all_qualityfilt.fastq'),\n",
    "        des = des_path_demultiplex,\n",
    "        BARCODE_INDEX_FILE = barcode_index_file,\n",
    "        primers = ['FwPrimer', 'RvPrimer'],\n",
    "        mismatch_ratio_f = mismatch_ratio_f,\n",
    "        mismatch_ratio_r = mismatch_ratio_r,\n",
    "        expected_length_variation = expected_length_variation,\n",
    "        search_range = search_range,\n",
    "        rvc_rvanchor = rvc_rvanchor,\n",
    "        input_format = input_format,\n",
    "        output_format = output_format\n",
    "    )\n",
    "    \n",
    "    # Step 3. Orientation correction\n",
    "    orientation = dumb.orientation(\n",
    "        src = des_path_demultiplex,\n",
    "        des = des_path_orientation,\n",
    "        input_format = input_format,\n",
    "        output_format = output_format,\n",
    "        BARCODE_INDEX_FILE = barcode_index_file,\n",
    "        FwPrimer = \"FwPrimer\",\n",
    "        RvPrimer = \"RvPrimer\",\n",
    "        search_range = orientation_search_range\n",
    "    )\n",
    "    \n",
    "    # Step 4. Trim Reads\n",
    "    trimmed = dumb.trim_reads(\n",
    "        src = des_path_orientation,\n",
    "        des = des_path_trimmed,\n",
    "        BARCODE_INDEX_FILE = barcode_index_file,\n",
    "        fw_col = \"FwPrimer\",\n",
    "        rv_col = \"RvPrimer\",\n",
    "        input_format = input_format,\n",
    "        output_format = output_format,\n",
    "        mode = \"table\",\n",
    "        fw_offset = fw_offset,\n",
    "        rv_offset = rv_offset,\n",
    "        mismatch_ratio_f = mismatch_ratio_f,\n",
    "        mismatch_ratio_r = mismatch_ratio_r,\n",
    "        discard_no_match = discard_no_match,\n",
    "        check_both_directions = check_both_directions,\n",
    "        reverse_complement_rv_col = reverse_complement_rv_col,\n",
    "        search_range = trimming_search_range\n",
    "    )\n",
    "\n",
    "    # Step 5. mmseqs Clustering\n",
    "    cluster = dumb.mmseqs_cluster(\n",
    "        src = des_path_trimmed,\n",
    "        des = des_path_mmseqs,\n",
    "        min_seq_id = cluster_min_seq_id,\n",
    "        cluster_mode = cluster_mode,\n",
    "        cov_mode = cov_mode,\n",
    "        k = kmer_length,\n",
    "        kmer_per_seq = kmer_per_seq,\n",
    "        s = sensitivity,\n",
    "        min_read_num = min_read_num,\n",
    "        input_format = input_format,\n",
    "        output_format = output_format,\n",
    "        suppress_output = suppress_output\n",
    "    )\n",
    "\n",
    "    # Step 6. Consensus\n",
    "    consensus = dumb.mafft_consensus(\n",
    "        src = des_path_mmseqs,\n",
    "        des = des_path_consensus,\n",
    "        minimal_reads = minimal_reads,\n",
    "        input_format = \"fas\",\n",
    "        max_reads = max_reads,\n",
    "        adjustdirection = adjustdirection\n",
    "    )\n",
    "\n",
    "    return \"Data processing complete.\"\n",
    "\n",
    "# Call the function with the desired project ID:\n",
    "result = process_data(project_id)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edce03af-fb0a-4a92-a3fa-76e85c516d0a",
   "metadata": {},
   "source": [
    "# 3. Sequence Files Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc11a4a-3bac-477f-8143-e8090d1aba1e",
   "metadata": {},
   "source": [
    "## 3-1. Check unique sequences\n",
    "- Set exact_length = True for protein-coding genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "411ec2b1-1279-4915-97c9-6d62bbd319e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded gene IDs for 35 samples.\n",
      "Cleared output directory: /mnt/Data/2024-000023/8_grouped_by_gene\n",
      "\n",
      "Processing complete...\n",
      "======== Summary report =======\n",
      "GeneID: AAT1a\n",
      "  Filtering Passed: 786/3014 files (26.08%)\n",
      "  Unique Sequences: 2838\n",
      "  Reads Passed: 130901/407402 (32.13%)\n",
      "  Files Copied: 786 file(s) to /mnt/Data/2024-000023/8_grouped_by_gene/AAT1a.\n",
      "--------------------------------\n",
      "GeneID: ACC1\n",
      "  Filtering Passed: 30/54 files (55.56%)\n",
      "  Unique Sequences: 51\n",
      "  Reads Passed: 2184/2738 (79.77%)\n",
      "  Files Copied: 30 file(s) to /mnt/Data/2024-000023/8_grouped_by_gene/ACC1.\n",
      "--------------------------------\n",
      "GeneID: ADP1\n",
      "  Filtering Passed: 455/1857 files (24.50%)\n",
      "  Unique Sequences: 1712\n",
      "  Reads Passed: 117039/266442 (43.93%)\n",
      "  Files Copied: 455 file(s) to /mnt/Data/2024-000023/8_grouped_by_gene/ADP1.\n",
      "--------------------------------\n",
      "GeneID: MPIb\n",
      "  Filtering Passed: 571/2558 files (22.32%)\n",
      "  Unique Sequences: 2422\n",
      "  Reads Passed: 293973/523750 (56.13%)\n",
      "  Files Copied: 571 file(s) to /mnt/Data/2024-000023/8_grouped_by_gene/MPIb.\n",
      "--------------------------------\n",
      "GeneID: SYA1\n",
      "  Filtering Passed: 394/1964 files (20.06%)\n",
      "  Unique Sequences: 1875\n",
      "  Reads Passed: 173936/358038 (48.58%)\n",
      "  Files Copied: 394 file(s) to /mnt/Data/2024-000023/8_grouped_by_gene/SYA1.\n",
      "--------------------------------\n",
      "GeneID: VPS13\n",
      "  Filtering Passed: 258/1282 files (20.12%)\n",
      "  Unique Sequences: 1156\n",
      "  Reads Passed: 151883/324142 (46.86%)\n",
      "  Files Copied: 258 file(s) to /mnt/Data/2024-000023/8_grouped_by_gene/VPS13.\n",
      "--------------------------------\n",
      "GeneID: ZWF1b\n",
      "  Filtering Passed: 194/749 files (25.90%)\n",
      "  Unique Sequences: 691\n",
      "  Reads Passed: 47577/108035 (44.04%)\n",
      "  Files Copied: 194 file(s) to /mnt/Data/2024-000023/8_grouped_by_gene/ZWF1b.\n",
      "--------------------------------\n",
      "\n",
      "CSV file written to /mnt/Data/2024-000023/qc_step1.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from Bio import SeqIO\n",
    "\n",
    "''' ===== Configuration ===== '''\n",
    "project_id = \"2024-000023\"  # Set this to your project ID\n",
    "exact_length = True  # Set to True to apply QC length filtering, False to skip this step\n",
    "length_filtering_option = \"!=\"  # Options: \"<\" or \"!=\", default is \"!=\"\n",
    "\n",
    "''' File path settings '''\n",
    "base_dir = f\"/mnt/Data/{project_id}\"  # Base directory containing the csv files\n",
    "input_directory = os.path.join(base_dir, \"7_consensus\")\n",
    "output_directory = os.path.join(base_dir, \"8_grouped_by_gene\")\n",
    "barcode_csv_path = os.path.join(base_dir, f\"{project_id}_barcoded_MLST.csv\")\n",
    "output_csv_path = os.path.join(base_dir, \"qc_step1.csv\")  # Output CSV file path\n",
    "\n",
    "''' ===== Workflow ===== '''\n",
    "def load_gene_ids_from_barcode(barcode_csv_path):\n",
    "    gene_ids_by_sample = defaultdict(lambda: {'gene_ids': [], 'Note': {}})\n",
    "    with open(barcode_csv_path, mode='r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            sample_id, gene_id = row['SampleID'].split('_')\n",
    "            gene_ids_by_sample[sample_id]['gene_ids'].append(gene_id)\n",
    "            gene_ids_by_sample[sample_id]['Note'][gene_id] = int(row['Note'])\n",
    "    print(f\"Loaded gene IDs for {len(gene_ids_by_sample)} samples.\")\n",
    "    return gene_ids_by_sample\n",
    "\n",
    "def process_sequences(input_directory, output_directory, gene_ids_by_sample, output_csv_path):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        print(f\"Created output directory: {output_directory}\")\n",
    "    shutil.rmtree(output_directory)\n",
    "    print(f\"Cleared output directory: {output_directory}\")\n",
    "    \n",
    "    pattern = re.compile(r'con_([A-Za-z0-9-]+)_([A-Za-z0-9]+)_cluster_(\\d+)_r(\\d+)\\.(fas|fasta)')\n",
    "    gene_summary = defaultdict(lambda: {\n",
    "        'TotalSequences': 0,\n",
    "        'UniqueSequences': 0,\n",
    "        'LengthFilter': 0,\n",
    "        'PassedQC': 0,\n",
    "        'TotalReads': 0\n",
    "    })\n",
    "    sample_rows = []  # For storing rows per sample and gene ID\n",
    "    for sample_id, data in gene_ids_by_sample.items():\n",
    "        gene_ids = data['gene_ids']\n",
    "        exact_lengths = data['Note']\n",
    "        for gene_id in gene_ids:\n",
    "            gene_dir = os.path.join(output_directory, gene_id)\n",
    "            os.makedirs(gene_dir, exist_ok=True)\n",
    "            \n",
    "            unique_sequences = set()\n",
    "            unique_sequences_pre_qc = set()\n",
    "            sequences_checked, sequences_passed_qc, qc_read, total_reads = 0, 0, 0, 0\n",
    "            \n",
    "            for fasta_file in os.listdir(input_directory):\n",
    "                match = pattern.match(fasta_file)\n",
    "                if not match or match.group(1) != sample_id or match.group(2) != gene_id:\n",
    "                    continue  # Skip files not matching current sample and gene ID\n",
    "                    \n",
    "                read_count = int(match.group(4))\n",
    "                total_reads += read_count\n",
    "                fasta_path = os.path.join(input_directory, fasta_file)\n",
    "                sequences_to_write = []\n",
    "                \n",
    "                for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "                    sequences_checked += 1\n",
    "                    sequence = str(record.seq)\n",
    "                    if sequence not in unique_sequences_pre_qc:\n",
    "                        unique_sequences_pre_qc.add(sequence)\n",
    "                    if sequence not in unique_sequences:\n",
    "                        if exact_length:\n",
    "                            if length_filtering_option == \"!=\" and len(sequence) != exact_lengths[gene_id]:\n",
    "                                continue\n",
    "                            elif length_filtering_option == \"<\" and len(sequence) >= exact_lengths[gene_id]:\n",
    "                                continue\n",
    "                        unique_sequences.add(sequence)\n",
    "                        sequences_to_write.append(record)\n",
    "                        sequences_passed_qc += 1\n",
    "                        qc_read += read_count  # Increment qc_read only for sequences passing QC\n",
    "                        \n",
    "                if sequences_to_write:\n",
    "                    dest_path = os.path.join(gene_dir, fasta_file)\n",
    "                    with open(dest_path, \"w\") as out_fasta:\n",
    "                        SeqIO.write(sequences_to_write, out_fasta, \"fasta\")\n",
    "\n",
    "            gene_summary[gene_id]['TotalSequences'] += sequences_checked\n",
    "            gene_summary[gene_id]['UniqueSequences'] += len(unique_sequences_pre_qc)\n",
    "            gene_summary[gene_id]['LengthFilter'] += len(unique_sequences)\n",
    "            gene_summary[gene_id]['PassedQC'] += sequences_passed_qc\n",
    "            gene_summary[gene_id]['TotalReads'] += total_reads\n",
    "\n",
    "            if sequences_checked > 0:\n",
    "                qc = f\"'{sequences_passed_qc}/{sequences_checked}\"\n",
    "                passed_percentage = (sequences_passed_qc / sequences_checked) * 100\n",
    "                read_passed = f\"'{qc_read}/{total_reads}\"\n",
    "                read_percentage = (qc_read / total_reads) * 100\n",
    "            else:\n",
    "                qc = \"\"\n",
    "                passed_percentage = \"\"\n",
    "                read_passed = \"\"\n",
    "                read_percentage = \"\"\n",
    "\n",
    "            sample_rows.append({\n",
    "                'SampleID': sample_id,\n",
    "                'gene_id': gene_id,\n",
    "                'qc': qc,\n",
    "                'qc_passed_%': f\"{passed_percentage:.2f}\" if passed_percentage else \"\",\n",
    "                'read_passed': read_passed,\n",
    "                'read_passed_%': f\"{read_percentage:.2f}\" if read_percentage else \"\"\n",
    "            })\n",
    "\n",
    "    # Print summary report\n",
    "    print(\"\\nProcessing complete...\")\n",
    "    print(\"======== Summary report =======\")\n",
    "    for gene_id, stats in gene_summary.items():\n",
    "        total_sequences = stats['TotalSequences']\n",
    "        unique_sequences = stats['UniqueSequences']\n",
    "        passed_qc = stats['PassedQC']\n",
    "        total_reads = stats['TotalReads']\n",
    "        qc_read = sum(int(re.sub(r\"[^0-9]\", \"\", row['read_passed'].split('/')[0])) for row in sample_rows if row['gene_id'] == gene_id and row['read_passed'])\n",
    "        passed_percentage = (passed_qc / total_sequences) * 100 if total_sequences > 0 else 0\n",
    "        read_percentage = (qc_read / total_reads) * 100 if total_reads > 0 else 0\n",
    "         \n",
    "        print(f\"GeneID: {gene_id}\")\n",
    "        print(f\"  Filtering Passed: {passed_qc}/{total_sequences} files ({passed_percentage:.2f}%)\")\n",
    "        print(f\"  Unique Sequences: {unique_sequences}\")\n",
    "        print(f\"  Reads Passed: {qc_read}/{total_reads} ({read_percentage:.2f}%)\")\n",
    "        print(f\"  Files Copied: {passed_qc} file(s) to {output_directory}/{gene_id}.\")\n",
    "        print(\"-\" * 32)\n",
    "\n",
    "    # Write the sample-specific report to CSV\n",
    "    with open(output_csv_path, mode='w', newline='') as csvfile:\n",
    "        fieldnames = ['SampleID', 'gene_id', 'qc', 'qc_passed_%', 'read_passed', 'read_passed_%']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(sample_rows)\n",
    "    print(f\"\\nCSV file written to {output_csv_path}.\")\n",
    "\n",
    "# Load gene IDs and exact length information\n",
    "gene_ids_by_sample = load_gene_ids_from_barcode(barcode_csv_path)\n",
    "\n",
    "# Run the modified function with the loaded gene IDs and exact length QC\n",
    "process_sequences(input_directory, output_directory, gene_ids_by_sample, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa1a475-0b11-4395-ac45-61095c1645f3",
   "metadata": {},
   "source": [
    "## 3-2. Translation Filtering\n",
    "- Only for protein-coding genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e4cd3-4810-4456-aa7b-e1e55c77efe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No values in 'qc' and 'read_passed' columns for 105-07 AAT1a: ''0/1' and ''0/4'\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 105-12 ACC1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 105-55 ACC1: ''0/1' and ''0/4'\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 102-22 AAT1a: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 102-22 ACC1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 102-22 ADP1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 102-22 MPIb: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 102-22 SYA1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 102-22 VPS13: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 102-22 ZWF1b: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-01 AAT1a: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-01 ACC1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-01 ADP1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-01 MPIb: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-01 SYA1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-01 VPS13: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-01 ZWF1b: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-35 ACC1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-35 ADP1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-35 SYA1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-35 ZWF1b: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-64 AAT1a: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-64 ACC1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-64 ADP1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-64 MPIb: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-64 SYA1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-64 VPS13: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-64 ZWF1b: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 108-35 AAT1a: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 108-35 ACC1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 108-35 ADP1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 108-35 MPIb: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 108-35 SYA1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 108-35 VPS13: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 108-35 ZWF1b: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 109-01 AAT1a: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 109-01 VPS13: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 109-02 AAT1a: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 109-02 VPS13: '' and ''\n",
      "\n",
      "Loaded gene data for 245 samples.\n",
      "Missing gene data: 39.\n",
      "\n",
      "Cleared output directory: /mnt/Data/2024-000023/8_1_translation_filter\n"
     ]
    }
   ],
   "source": [
    "'''latest'''\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "# ===== Configuration =====\n",
    "project_id = \"2024-000023\"  # Set this to your project ID\n",
    "\n",
    "# File path settings\n",
    "base_dir = f\"/mnt/Data/{project_id}\"\n",
    "input_directory = os.path.join(base_dir, \"8_grouped_by_gene\")\n",
    "output_directory = os.path.join(base_dir, \"8_1_translation_filter\")\n",
    "qc_csv_path = os.path.join(base_dir, \"qc_step1.csv\")\n",
    "output_csv_path = os.path.join(base_dir, \"qc_step2.csv\")\n",
    "\n",
    "# ===== Workflow =====\n",
    "def load_gene_ids_from_prev_qc(qc_csv_path):\n",
    "    gene_data = {}\n",
    "    gene_reads = defaultdict(lambda: defaultdict(int))\n",
    "    gene_total_reads = defaultdict(int)\n",
    "    warning_count = 0\n",
    "    \n",
    "    with open(qc_csv_path, mode='r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            sample_id = row['SampleID']\n",
    "            gene_id = row['gene_id']\n",
    "            qc = row['qc']\n",
    "            reads = row['read_passed']\n",
    "            total_files, total_reads = 0, 0\n",
    "            \n",
    "            if '/' in qc and '/' in reads:\n",
    "                try:\n",
    "                    total_files = int(qc.split('/')[1])\n",
    "                    total_reads = int(reads.split('/')[1])\n",
    "                    if int(qc.split('/')[0][1]) == 0 or int(reads.split('/')[0][1]) == 0:\n",
    "                        print(f\"Warning: No values in 'qc' and 'read_passed' columns for {sample_id} {gene_id}: '{qc}' and '{reads}'\")\n",
    "                        warning_count += 1\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Invalid values in 'qc' and 'read_passed' columns for {sample_id} {gene_id}: '{qc}' and '{reads}'\")\n",
    "            else:\n",
    "                print(f\"Warning: No values in 'qc' and 'read_passed' columns for {sample_id} {gene_id}: '{qc}' and '{reads}'\")\n",
    "                warning_count += 1\n",
    "            \n",
    "            gene_data[(sample_id, gene_id)] = total_files\n",
    "            gene_reads[sample_id][gene_id] = total_reads\n",
    "            gene_total_reads[gene_id] += total_reads  # Add integer value for total_reads\n",
    "    \n",
    "    print(f\"\\nLoaded gene data for {len(gene_data)} samples.\")\n",
    "    print(f\"Missing gene data: {warning_count}.\\n\")\n",
    "    return gene_data, gene_reads, gene_total_reads\n",
    "\n",
    "\n",
    "\n",
    "def translate_and_filter(input_directory, output_directory, gene_data, gene_reads, gene_total_reads):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        print(f\"Created output directory: {output_directory}\")\n",
    "    shutil.rmtree(output_directory)\n",
    "    print(f\"Cleared output directory: {output_directory}\")\n",
    "\n",
    "    pattern = re.compile(r'con_([A-Za-z0-9-]+)_([A-Za-z0-9]+)_cluster_(\\d+)_r(\\d+)\\.(fas|fasta)')\n",
    "    gene_files = defaultdict(list)\n",
    "    gene_total_files = defaultdict(int)\n",
    "    gene_pass_count = defaultdict(int)\n",
    "\n",
    "    for gene_id_dir in os.listdir(input_directory):\n",
    "        gene_path = os.path.join(input_directory, gene_id_dir)\n",
    "        if os.path.isdir(gene_path):\n",
    "            for file in os.listdir(gene_path):\n",
    "                if file.endswith(('.fas', '.fasta')) and file.startswith('con'):\n",
    "                    match = pattern.match(file)\n",
    "                    if match:\n",
    "                        sample_id, gene_id, cluster_num, read_num, ext = match.groups()\n",
    "                        read_count = int(read_num)\n",
    "                        gene_files[(sample_id, gene_id)].append((os.path.join(gene_id_dir, file), read_count))\n",
    "                    else:\n",
    "                        print(f\"File {file} in {gene_id_dir} does not match pattern.\")\n",
    "\n",
    "    def translate_sequence(seq):\n",
    "        translations = []\n",
    "        seq_rc = seq.reverse_complement()\n",
    "        \n",
    "        def trim_sequence(sequence):\n",
    "            remainder = len(sequence) % 3\n",
    "            if remainder != 0:\n",
    "                sequence = sequence[:len(sequence) - remainder]\n",
    "            return sequence\n",
    "        \n",
    "        for frame in range(3):\n",
    "            forward_seq = trim_sequence(seq[frame:])\n",
    "            translations.append(forward_seq.translate())\n",
    "        for frame in range(3):\n",
    "            reverse_seq = trim_sequence(seq_rc[frame:])\n",
    "            translations.append(reverse_seq.translate())\n",
    "        \n",
    "        return translations\n",
    "\n",
    "    csv_rows = []\n",
    "\n",
    "    for (sample_id, gene_id), total_files in gene_data.items():\n",
    "        files = gene_files.get((sample_id, gene_id), [])\n",
    "        gene_total_files[gene_id] += total_files\n",
    "        gene_read = gene_reads[sample_id].get(gene_id, 0)  # Use specific SampleID and gene_id pair\n",
    "        passed_file_count, qc_read = 0, 0\n",
    "\n",
    "        if files:\n",
    "            gene_dir = os.path.join(output_directory, gene_id)\n",
    "            if not os.path.exists(gene_dir):\n",
    "                os.makedirs(gene_dir)\n",
    "\n",
    "            for filepath, read_count in files:\n",
    "                full_path = os.path.join(input_directory, filepath)\n",
    "                with open(full_path, 'r') as handle:\n",
    "                    for record in SeqIO.parse(handle, 'fasta'):\n",
    "                        translations = translate_sequence(record.seq)\n",
    "                        frames_without_stop_codons = [translation for translation in translations if '*' not in translation]\n",
    "                        if frames_without_stop_codons:\n",
    "                            qc_read += read_count\n",
    "                            dest_file = os.path.join(gene_dir, os.path.basename(filepath))\n",
    "                            shutil.copy(full_path, dest_file)\n",
    "                            passed_file_count += 1\n",
    "\n",
    "        gene_pass_count[gene_id] += passed_file_count\n",
    "        if total_files > 0 and gene_read > 0:\n",
    "            qc = f\"'{passed_file_count}/{total_files}\"\n",
    "            passed_percentage = (passed_file_count / total_files) * 100\n",
    "            read_passed = f\"'{qc_read}/{gene_read}\"\n",
    "            read_percentage = (qc_read / gene_read) * 100\n",
    "            csv_rows.append({\n",
    "                'SampleID': sample_id,\n",
    "                'gene_id': gene_id,\n",
    "                'qc': qc,\n",
    "                'qc_passed_%': f\"{passed_percentage:.2f}\",\n",
    "                'read_passed': read_passed,\n",
    "                'read_passed_%': f\"{read_percentage:.2f}\"\n",
    "            })\n",
    "        else:\n",
    "            csv_rows.append({\n",
    "                'SampleID': sample_id,\n",
    "                'gene_id': gene_id,\n",
    "                'qc': '',\n",
    "                'qc_passed_%': '',\n",
    "                'read_passed': '',\n",
    "                'read_passed_%': ''\n",
    "            })\n",
    "    \n",
    "    print(\"\\nProcessing complete...\")\n",
    "    print(\"======== Summary report =======\")\n",
    "    for gene_id in gene_pass_count:\n",
    "        total_files = gene_total_files[gene_id]\n",
    "        passed_file_count = gene_pass_count[gene_id]\n",
    "        qc_read = sum(int(re.sub(r\"[^0-9]\", \"\", row['read_passed'].split('/')[0])) for row in csv_rows if row['gene_id'] == gene_id and row['read_passed'])\n",
    "        total_reads = sum(gene_reads[sample_id][gene_id] for sample_id in gene_reads if gene_id in gene_reads[sample_id])\n",
    "        read_percentage = (qc_read / total_files) * 100 if total_files > 0 else 0\n",
    "        print(f\"GeneID: {gene_id}\")\n",
    "        print(f\"  Filtering Passed: {passed_file_count}/{total_files} files ({(passed_file_count / total_files) * 100:.2f}%)\")\n",
    "        print(f\"  Reads Passed: {qc_read}/{total_reads} ({read_percentage:.2f}%)\")\n",
    "        print(f\"  Files Copied: {passed_file_count} file(s) to {output_directory}/{gene_id}.\")\n",
    "        print(\"-\" * 32)\n",
    "\n",
    "    with open(output_csv_path, mode='w', newline='') as csv_file:\n",
    "        fieldnames = ['SampleID', 'gene_id', 'qc', 'qc_passed_%', 'read_passed', 'read_passed_%']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_rows)\n",
    "    print(f\"\\nCSV file written to {output_csv_path}.\")\n",
    "\n",
    "# Load gene_id information from the barcode file\n",
    "gene_data, gene_reads, gene_total_reads = load_gene_ids_from_prev_qc(qc_csv_path)\n",
    "\n",
    "# Run the modified function with the loaded gene data\n",
    "translate_and_filter(input_directory, output_directory, gene_data, gene_reads, gene_total_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9da1011b-524f-4491-ac42-1e66104e1262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "reads = \"'0/4\"\n",
    "read = reads.split('/')[0][1]\n",
    "print(read)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162567c7-2172-4fb1-b87d-1b30a56059f1",
   "metadata": {},
   "source": [
    "## 3-3. Extract Top Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83d6dbf6-28cd-4df2-b38b-29e183fb0835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No values in 'qc' and 'read_passed' columns for 105-12 ACC1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 102-22 AAT1a: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 102-22 ACC1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 102-22 ADP1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 102-22 MPIb: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 102-22 SYA1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 102-22 VPS13: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 102-22 ZWF1b: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-01 AAT1a: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-01 ACC1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-01 ADP1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-01 MPIb: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-01 SYA1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-01 VPS13: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-01 ZWF1b: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-35 ACC1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-35 ADP1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-35 SYA1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-35 ZWF1b: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-64 AAT1a: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-64 ACC1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-64 ADP1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-64 MPIb: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-64 SYA1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-64 VPS13: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 103-64 ZWF1b: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 108-35 AAT1a: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 108-35 ACC1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 108-35 ADP1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 108-35 MPIb: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 108-35 SYA1: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 108-35 VPS13: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 108-35 ZWF1b: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 109-01 AAT1a: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 109-01 VPS13: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 109-02 AAT1a: '' and ''\n",
      "Warning: No values in 'qc' and 'read_passed' columns for 109-02 VPS13: '' and ''\n",
      "\n",
      "Loaded gene data for 245 samples.\n",
      "Created output directory: /mnt/Data/2024-000023/9_final_result\n",
      "Cleared output directory: /mnt/Data/2024-000023/9_final_result\n",
      "\n",
      "Processing complete...\n",
      "======== Summary Report ========\n",
      "GeneID: AAT1a\n",
      "  Filtering Passed: 93/3014 files (3.09%)\n",
      "  Reads Passed: 120899/407402 (29.68%)\n",
      "  Files Copied: 93 file(s) to /mnt/Data/2024-000023/9_final_result/AAT1a.\n",
      "--------------------------------\n",
      "GeneID: ACC1\n",
      "  Filtering Passed: 30/54 files (55.56%)\n",
      "  Reads Passed: 2184/2738 (79.77%)\n",
      "  Files Copied: 30 file(s) to /mnt/Data/2024-000023/9_final_result/ACC1.\n",
      "--------------------------------\n",
      "GeneID: ADP1\n",
      "  Filtering Passed: 104/1857 files (5.60%)\n",
      "  Reads Passed: 110951/266442 (41.64%)\n",
      "  Files Copied: 104 file(s) to /mnt/Data/2024-000023/9_final_result/ADP1.\n",
      "--------------------------------\n",
      "GeneID: MPIb\n",
      "  Filtering Passed: 108/2558 files (4.22%)\n",
      "  Reads Passed: 287662/523750 (54.92%)\n",
      "  Files Copied: 108 file(s) to /mnt/Data/2024-000023/9_final_result/MPIb.\n",
      "--------------------------------\n",
      "GeneID: SYA1\n",
      "  Filtering Passed: 103/1964 files (5.24%)\n",
      "  Reads Passed: 170216/358038 (47.54%)\n",
      "  Files Copied: 103 file(s) to /mnt/Data/2024-000023/9_final_result/SYA1.\n",
      "--------------------------------\n",
      "GeneID: VPS13\n",
      "  Filtering Passed: 83/1282 files (6.47%)\n",
      "  Reads Passed: 147029/324142 (45.36%)\n",
      "  Files Copied: 83 file(s) to /mnt/Data/2024-000023/9_final_result/VPS13.\n",
      "--------------------------------\n",
      "GeneID: ZWF1b\n",
      "  Filtering Passed: 80/749 files (10.68%)\n",
      "  Reads Passed: 46381/108035 (42.93%)\n",
      "  Files Copied: 80 file(s) to /mnt/Data/2024-000023/9_final_result/ZWF1b.\n",
      "--------------------------------\n",
      "\n",
      "CSV file written to /mnt/Data/2024-000023/2024-000023_sequencing_report.csv.\n"
     ]
    }
   ],
   "source": [
    "'''latest'''\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# ===== Configuration =====\n",
    "project_id = \"2024-000023\"  # Set this to your project ID\n",
    "top_files_no = 4  # Set the top numbers that you wish to extract\n",
    "\n",
    "# File path settings\n",
    "base_dir = f\"/mnt/Data/{project_id}\"  # Base directory containing the csv files\n",
    "input_directory = os.path.join(base_dir, \"8_1_translation_filter\")\n",
    "output_directory = os.path.join(base_dir, \"9_final_result\")\n",
    "qc_csv_path = os.path.join(base_dir, \"qc_step2.csv\")\n",
    "output_csv_path = os.path.join(base_dir, f\"{project_id}_sequencing_report.csv\")\n",
    "\n",
    "# ===== Workflow =====\n",
    "def load_gene_ids_from_prev_qc(qc_csv_path):\n",
    "    gene_data = {}\n",
    "    gene_reads = defaultdict(lambda: defaultdict(int))\n",
    "    gene_total_reads = defaultdict(int)\n",
    "    warning_count = 0\n",
    "    \n",
    "    with open(qc_csv_path, mode='r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            sample_id = row['SampleID']\n",
    "            gene_id = row['gene_id']\n",
    "            qc = row['qc']\n",
    "            reads = row['read_passed']\n",
    "            total_files, total_reads = 0, 0\n",
    "            \n",
    "            if '/' in qc and '/' in reads:\n",
    "                try:\n",
    "                    total_files = int(qc.split('/')[1])\n",
    "                    total_reads = int(reads.split('/')[1])\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Invalid values in 'qc' and 'read_passed' columns for {sample_id} {gene_id}: '{qc}' and '{reads}'\")\n",
    "            else:\n",
    "                print(f\"Warning: No values in 'qc' and 'read_passed' columns for {sample_id} {gene_id}: '{qc}' and '{reads}'\")\n",
    "                warning_count += 1\n",
    "            \n",
    "            gene_data[(sample_id, gene_id)] = total_files\n",
    "            gene_reads[sample_id][gene_id] = total_reads\n",
    "            gene_total_reads[gene_id] += total_reads  # Add integer value for total_reads\n",
    "    \n",
    "    print(f\"\\nLoaded gene data for {len(gene_data)} samples.\")\n",
    "    print(f\"Missing gene data: {warning_count}.\\n\")\n",
    "    return gene_data, gene_reads, gene_total_reads\n",
    "\n",
    "def extract_top_reads_files(input_directory, output_directory, gene_data, gene_reads, gene_total_reads):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        print(f\"Created output directory: {output_directory}\")\n",
    "    shutil.rmtree(output_directory)\n",
    "    print(f\"Cleared output directory: {output_directory}\")\n",
    "\n",
    "    pattern = re.compile(r'con_([A-Za-z0-9-]+)_([A-Za-z0-9]+)_cluster_(\\d+)_r(\\d+)\\.(fas|fasta)')\n",
    "    gene_files = defaultdict(list)\n",
    "    gene_total_files = defaultdict(int)\n",
    "    gene_pass_count = defaultdict(int)\n",
    "\n",
    "    for gene_id_dir in os.listdir(input_directory):\n",
    "        gene_path = os.path.join(input_directory, gene_id_dir)\n",
    "        if os.path.isdir(gene_path):\n",
    "            for file in os.listdir(gene_path):\n",
    "                if file.endswith(('.fas', '.fasta')) and file.startswith('con'):\n",
    "                    match = pattern.match(file)\n",
    "                    if match:\n",
    "                        sample_id, gene_id, cluster_num, read_num, ext = match.groups()\n",
    "                        read_count = int(read_num)\n",
    "                        gene_files[(sample_id, gene_id)].append((os.path.join(gene_id_dir, file), read_count))\n",
    "                    else:\n",
    "                        print(f\"File {file} in {gene_id_dir} does not match pattern.\")\n",
    "\n",
    "    csv_rows = []\n",
    "    for (sample_id, gene_id), total_files in gene_data.items():\n",
    "        files = gene_files.get((sample_id, gene_id), [])\n",
    "        gene_total_files[gene_id] += total_files\n",
    "        gene_read = gene_reads[sample_id][gene_id]  # Use specific SampleID and gene_id pair\n",
    "        passed_file_count, qc_read = 0, 0\n",
    "        \n",
    "        if files:\n",
    "            gene_dir = os.path.join(output_directory, gene_id)\n",
    "            if not os.path.exists(gene_dir):\n",
    "                os.makedirs(gene_dir)\n",
    "                \n",
    "        if gene_files[(sample_id, gene_id)]:\n",
    "            files = sorted(gene_files[(sample_id, gene_id)], key=lambda x: x[1], reverse=True)[:top_files_no]\n",
    "            for file, read_count in files:\n",
    "                src_path = os.path.join(input_directory, file)\n",
    "                dest_path = os.path.join(gene_dir, os.path.basename(file))\n",
    "                if os.path.exists(src_path):\n",
    "                    shutil.copy(src_path, dest_path)\n",
    "                    passed_file_count += 1\n",
    "                    qc_read += read_count\n",
    "\n",
    "        gene_pass_count[gene_id] += passed_file_count\n",
    "        if total_files > 0 and gene_read > 0:\n",
    "            qc = f\"'{passed_file_count}/{total_files}\"\n",
    "            passed_percentage = (passed_file_count / total_files) * 100\n",
    "            read_passed = f\"'{qc_read}/{gene_read}\"\n",
    "            read_percentage = (qc_read / gene_read) * 100\n",
    "            if qc_read == int(0):\n",
    "                csv_rows.append({\n",
    "                    'SampleID': sample_id,\n",
    "                    'gene_id': gene_id,\n",
    "                    'qc': '',\n",
    "                    'qc_passed_%': '',\n",
    "                    'read_passed': '',\n",
    "                    'read_passed_%': ''\n",
    "                })\n",
    "            else:\n",
    "                csv_rows.append({\n",
    "                    'SampleID': sample_id,\n",
    "                    'gene_id': gene_id,\n",
    "                    'qc': qc,\n",
    "                    'qc_passed_%': f\"{passed_percentage:.2f}\",\n",
    "                    'read_passed': read_passed,\n",
    "                    'read_passed_%': f\"{read_percentage:.2f}\"\n",
    "                })\n",
    "        else:\n",
    "            csv_rows.append({\n",
    "                'SampleID': sample_id,\n",
    "                'gene_id': gene_id,\n",
    "                'qc': '',\n",
    "                'qc_passed_%': '',\n",
    "                'read_passed': '',\n",
    "                'read_passed_%': ''\n",
    "            })\n",
    "            \n",
    "    # Summary report for each gene_id\n",
    "    print(\"\\nProcessing complete...\")\n",
    "    print(\"======== Summary Report ========\")\n",
    "    for gene_id in gene_pass_count:\n",
    "        total_files = gene_total_files[gene_id]\n",
    "        passed_file_count = gene_pass_count[gene_id]\n",
    "        qc_read = sum(int(re.sub(r\"[^0-9]\", \"\", row['read_passed'].split('/')[0])) for row in csv_rows if row['gene_id'] == gene_id and row['read_passed'])\n",
    "        total_reads = gene_total_reads[gene_id]\n",
    "        passed_percentage = (passed_file_count / total_files) * 100 if total_files > 0 else 0\n",
    "        read_percentage = (qc_read / total_reads) * 100 if total_reads > 0 else 0\n",
    "        print(f\"GeneID: {gene_id}\")\n",
    "        print(f\"  Filtering Passed: {passed_file_count}/{total_files} files ({passed_percentage:.2f}%)\")\n",
    "        print(f\"  Reads Passed: {qc_read}/{total_reads} ({read_percentage:.2f}%)\")\n",
    "        print(f\"  Files Copied: {passed_file_count} file(s) to {output_directory}/{gene_id}.\")\n",
    "        print(\"-\" * 32)\n",
    "\n",
    "    # Write the sorted rows to the CSV with the simplified structure\n",
    "    with open(output_csv_path, mode='w', newline='') as csv_file:\n",
    "        fieldnames = ['SampleID', 'gene_id', 'qc', 'qc_passed_%', 'read_passed', 'read_passed_%']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_rows)\n",
    "    print(f\"\\nCSV file written to {output_csv_path}.\")\n",
    "\n",
    "# Load gene_id information from qc_step2.csv\n",
    "gene_data, gene_reads, gene_total_reads = load_gene_ids_from_prev_qc(qc_csv_path)\n",
    "\n",
    "# Run the function with gene_id filtering and QC check\n",
    "extract_top_reads_files(input_directory, output_directory, gene_data, gene_reads, gene_total_reads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
